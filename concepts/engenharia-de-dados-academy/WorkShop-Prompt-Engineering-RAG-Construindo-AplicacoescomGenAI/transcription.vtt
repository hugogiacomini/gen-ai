Pessoas que têm mais experiência, que
estão há muito mais tempo no
mercado, né?
Por que eu digo isso? Porque
a gente tem aqui vários backgrounds.
O Jorge, por exemplo, ele já
foi um dos meus mentores, foi
meu chefe durante muito tempo,
então eu aprendi demais e ainda
continuo aprendendo.
É um monstro da tecnologia, tem...
Nem sei mais se mora no
Brasil ou fora, tem cliente dentro,
tem cliente fora do país.
Trabalha com isso há milhares de
anos.
E esses feedbacks que eu converso
e escuto, isso aqui é muito
importante
para a gente entender que as
coisas têm mudado.
Só que ao mesmo tempo que
têm mudado, isso não quer dizer
que é bom.
E acho que isso é um
ponto importante de vocês entenderem.
A mudança nem sempre quer dizer
que é só mudança positiva.
Me preocupa muito isso do que
o Jorge estava falando e do
que eu também acredito.
Eu não acredito que a gente
tem que ficar somente em teoria,
obviamente,
mas o que eu descobri cada
dia mais é que quanto mais
você sabe de teoria,
quanto mais você entende por debaixo
dos panos,
cada dia mais...
As ferramentas ajudam ainda mais a
ficar mais fácil
e para você aplicar essa teoria
de fato.
Muita coisa abstraída, o que facilitou
bastante.
E é engraçado porque quando a
gente traz esse conteúdo de GNI,
existem, nossa, tantas coisas que podem
ser ditas aqui.
E eu vou tentar fazer...
Eu vou tentar trazer isso muito
mais na perspectiva da engenharia de
dados
para com o GNI do que
machine learning.
Até porque o meu background não
é esse.
Então, existem um bilhão...
De coisas que eu não tenho
entendimento, por exemplo, teórico a fundo
disso
e me levaria anos para entender,
tá?
Então, entendam que nesse treinamento eu
vou trazer uma teoria até um
certo ponto
e depois a gente vai focar
muito mais no entendimento de negócio,
como resolver, melhores práticas, como
endereçar,
quais são os problemas comuns, enfim.
E aí vão ter várias coisas
que se vocês quiserem entrar mais
a fundo ainda,
vocês vão precisar cavar mais, tá?
Por quê?
Porque senão a gente vai ficar
aqui...
E só inteirando em cima...
E aqui a gente está falando
de GNI, a gente está falando
inteiramente de matemática, né?
E estatística, basicamente.
Quando a gente fala de engenharia
de dados, é um pouco diferente.
Você não precisa ficar tanto em
teoria matemática e estatística, né?
Mas o que eu quero trazer
para vocês hoje aqui?
Eu acho que eu quero trazer...
A gente vai mesclar teoria com
prática, obviamente,
mas eu quero trazer algumas coisas
que são importantes no negócio business,
no que os clientes precisam realmente,
como você operar,
operacionaliza, o que está acontecendo fora
do mercado, e assim por diante.
Então, essa visão de vocês entenderem
como está o mercado hoje,
eu acho que é extremamente importante
vocês saberem o que está acontecendo,
né?
E aí, quem quiser adicionar isso,
fique à vontade, tá?
Eu estou acompanhando o chat aqui
sempre.
Então, podem falar aí que a
gente tem pessoas fantásticas para trocar
aí, tá?
Hoje é um dia de troca,
aproveitem isso.
Tem muita gente boa aqui.
Dá para aprender muita coisa, não
somente no conteúdo que vai ser
passado.
Beleza?
Bem, então a gente vai começar
a falar um pouquinho de fundamentos
de GNI, né?
E, de novo, é uma área
extremamente gigantesca que vem de machine
learning,
que vem de inteligência artificial, que
vem de deep learning, que vem
de várias coisas.
Então, a gente vai tentar achar
um balanço entre isso, como eu
falei anteriormente.
Mas antes de falar disso, eu
gosto muito de olhar o mercado,
né?
Eu lembro há 10 anos atrás,
quando...
Eu tentei me aventurar para o
fora do país, há 11 anos
atrás.
A primeira coisa que eu fui
fazer é, pô, beleza, tem um
bocado de coisa para fazer,
mas o que é que isso
está alinhado, de fato, com o
mercado, né?
Então, o que eu comecei a
olhar e o que eu sempre
estou olhando, realmente,
é o que está acontecendo fora.
O que a gente está vendo
aí, assim, tipo, nascer, né?
Assim como pessoas que estão aqui
já viram muitas coisas nascerem, né?
Eu tenho o privilégio hoje de
ver nascer muita coisa em relação
à IA.
E eu vejo muitas brincadeiras também.
Eu vejo também do pessoal brasileiro
preconceituoso zoando,
ah, agora é só botar IA
no currículo,
ou é só botar IA que
você vai ser contratado.
Eu acho que tem muita gente
achando isso engraçado,
mas é a realidade, cara.
E aí você pode perguntar para
outras pessoas que estão aqui,
podem colocar no chat.
Na minha opinião, a IA, finalmente,
né?
Eu acho que ela tomou o
espaço que ela merece.
E, para mim, ela é muito
maior do que a Wave e
da própria internet 2 .0,
na minha humilde opinião.
É, pelo que eu estou vendo,
é como os clientes se comportam,
o que eles precisam.
É o desespero hoje de, finalmente,
né?
A tecnologia conseguir entregar valor na
ponta.
Eu vejo isso acontecendo realmente com
a Gen AI.
Tá? Eu não vi isso acontecendo
em grande escala com o Machine
Learning.
Eu não vi isso acontecendo em
grande escala, sendo mainstream,
realmente como engenharia de dados que
servem propostas diferentes
para clientes específicos com nichos
diferentes.
Mas, cara, quando a gente fala
de Gen AI,
cara, de fato, se você olhar,
inclusive, os papers da IBM,
se você olhar alguns conteúdos da
IBM,
a IBM é muito pioneira, tem
muita coisa foda sobre IA,
sobre Gen AI e assim por
diante.
Então, a gente vai ver que,
realmente, foi a área que explodiu,
tá?
E, realmente, eu não consigo ver
nada diferente disso.
O que eu também consigo ver
no mercado, tá?
É, olhando o LinkedIn, tá?
Aqui eu peguei o LinkedIn, mas,
na real, você pode olhar outras
fontes também de contratação.
Existem esses quatro cargos aqui que
ainda não necessariamente são escritos e
colocados
como cargos, mas que, hoje, estão
começando a ter uma ótima adoção
no mercado.
Principalmente esse cara aqui, AI Engineer,
tá?
A gente tem bastante esse cara,
Prompt Engineer, também.
E a gente tem um pouquinho
de Generative AI Engineer.
E esse cara aqui que eu
venho falando, né?
Já faz um tempinho, alguns meses
atrás, que é o AI Engineer.
Esse cargo, de fato, ele ainda
não existe publicamente,
mas se você começar a pesquisar
em todos os LNMs, se você
for olhar ali
um pouquinho e dar uma olhada
nas vagas de LinkedIn,
você digitar AI Data Engineer ou
Data AI Engineer,
você vai ver, na verdade, que
as vagas, mundialmente, né?
Worldwide, elas possuem diversas
características que
trazem, né?
Remetem a GNI, né?
LLM Ops, GNI, interação com LLMs,
de fato, entender modelos de fundação
e por aí vai, tá?
Então, são quatro vagas diferentes, mas
o que é interessante olhar
é o crescimento que elas possuem,
tá?
E qual é a métrica de
crescimento.
Aqui a gente tá falando da
quantidade de vagas,
aqui a gente tá falando do
valor aproximado,
aqui a gente tá falando de
crescimento nos próximos anos,
e aqui a gente tá falando,
basicamente, da tech stack
e, de fato, do que é
a responsabilidade.
O que é legal ver, entre
todas elas, é que dados estão
no centro, né?
E aí segue aquela premissa de,
cara, o dado precisa estar muito
bem, né?
Organizado, pra que você possa, de
fato, entregar valor.
E, principalmente, quando a gente olha
esse processo de REG, né?
É uma palavra relativamente engraçada.
Toda vez que eu falo REG,
eu me vejo dançando.
Mas a grande questão é, qual
é a interferência?
Qual realmente é a relação disso
pra engenharia de dados
ou pra, realmente, toda a parte
de GNI ou GAI, especificamente?
E a gente vai entender muito
isso.
E por que a gente trouxe
esse workshop, né?
A gente trouxe esse workshop,
porque é um conteúdo relativamente novo
no espectro de engenharia de dados.
É muito incerto o que, realmente,
o engenho de dados vai fazer
ao longo do tempo, né?
E aí, obviamente, a gente vai
ter gente que faz demais,
a gente vai ter gente que
faz mais nichado,
mas, eventualmente, nós teremos atividades que
são relacionadas à IA.
E eu vou trazer alguns exemplos
práticos e alguns exemplos reais, de
fato,
do que está acontecendo no mercado.
Beleza?
Então, só pra trazer...
Vou fazer esse disclaimer aí.
Uma das coisas, obviamente, que vão
ser comuns entre elas aí,
Python, entendimento de negócio, obviamente,
né?
SQL, sempre um ótimo.
Modelos de fundação.
Você vai ver, basicamente, na maioria
deles, vão te pedir RAG.
E, com certeza, Vector Database é
algo que está na lista de
todos aqui.
E vai estar, realmente, tá?
Esse é um cara muito, muito,
muito importante.
A gente vai entender um pouquinho
hoje sobre ele.
E a gente vai se aprofundar
nos próximos workshops.
Nos próximos conteúdos.
Nós teremos mais conteúdo sobre isso.
Beleza?
Antes de eu começar aqui, queria...
Temos aí 208 pessoas em sala.
Eu queria entender um pouquinho, rapidinho
aqui na mensagem,
se vocês já trabalharam com algum
projeto que envolve GNI de alguma
forma.
Tá?
Então, coloca no chat aí pra
mim, sim, sim ou não, não.
Só pra eu ter uma ideia.
Legal.
Então, a gente tem, basicamente, uma
divisão.
Muito sim, muito não.
Eu diria que quase que metade
sim, quase que metade não.
Isso é interessante.
Obviamente que isso abre várias perguntas.
Mas só pra gente ter um
entendimento, de fato,
do que a gente pode fazer
e entender um pouquinho desses conceitos.
Beleza?
Então, eu diria que seria, sei
lá, 60 não, 50, 40 sim.
Mais ou menos, eu diria.
Tá?
E, provavelmente, a galera que falou
não,
não tá querendo entender.
Cara, o que que tá acontecendo
nesse mercado, né?
Isso é algo que vai vingar?
É uma coisa que realmente eu
preciso estar realmente antenado sobre isso?
O que eu devo fazer?
Então, a gente vai discutir um
pouquinho sobre isso também.
Beleza?
E fiquem à vontade pra perguntar,
tá, pessoal?
Enquanto eu passo o conteúdo aqui.
Bem, primeira coisa, a gente fala
de unicórnio.
Muita gente tem muito pavor sobre
essa palavra.
Eu também tenho muito pavor sobre
essa palavra unicórnio.
Até porque cada dia mais parece...
É...
Eu não sei se vocês sentem
isso, mas...
Parece que cada vez mais o
mercado pede generalistas, né?
Ou seja, empurra você a ser
generalista e entender muita coisa.
Mas a realidade fora do país,
na minha vivência,
ela é completamente diferente.
Você sim precisa conversar sobre vários
tópicos e entender, eventualmente.
Mas pessoas realmente confiam em outras
pelas suas especificidades, né?
Então, eu tenho...
Eu tenho vivido bastante isso.
Eu trabalho em grupos de pessoas
que trabalham com isso.
E a gente vê a diferença
disso em projetos realmente, em aplicações
reais.
Eu acredito, continuo acreditando muito que
a especialização, ela é extremamente
importante.
Ah, legal.
As vezes fez ali, ó.
É...
48, 45.
Então, teve um pouquinho mais de
nãos.
É...
Então, a gente vai explorar isso
um pouco.
O que que...
O que que tange?
O que que a gente já
consegue identificar com todas essas viagens
nativas?
E aí, eu uso todas, tá?
Que eu posso pensar aqui.
Então, Manos, Grock,
Cloud, Gemini, ChatshepT.
Eu tenho, basicamente, Perplexity e assim
por diante.
Então, eu uso bastante essas caras
no dia a dia.
Principalmente, pra julgar um ao outro.
LLM is a judge.
Então, eu pergunto coisas entre eles
e é legal, assim,
muito das coisas, eles conversam muito
bem no sentido mais avançado das
coisas.
Então, compartilham muitas coisas em comuns.
E o que a gente consegue
saber no mercado, olhando as vagas,
olhando todo esse ecossistema,
é que existem algumas coisas, já
no espectro de engenharia de dados,
que fazem muito sentido começar a
olhar e realmente estar apto a
entendê -las, né?
Então, por exemplo, coisas como Vector
Databases,
com certeza, absolutamente, é algo extremamente
importante pra você entender
se você quer entrar na era
de GNI.
Um outro cara que, pra mim,
eu diria que é um dos
cornerstones
de toda parte de como você
pode operacionalizar LLMs.
E, obviamente, a gente tem muita
coisa em cima disso.
Que é o Langchain, que é
a família Chain.
Nós temos aqui os modelos de
fundação, obviamente, né?
Cloud, ChatPT e assim por diante.
Nós temos CrewAI, que é um
ótimo cara pra você criar multiagentes.
Nós temos o LangFuse, que, pra
mim, é uma das melhores bibliotecas
que eu já trabalhei em relação
à experiência.
Eu já implementei isso em produção.
Eu tenho isso em produção.
Então, é um cara que funciona
muito bem.
E a gente tem um cara,
também, muito legal, chamado Docling,
que é uma biblioteca Python muito
interessante pra extração de dados.
Que você possa vetorizar e assim
por diante, tá?
Então, o que que acontece?
Essas coisas, a gente já sabe
que estão sendo utilizadas.
E aí, a grande pergunta é,
Luan, como que isso, de fato,
é aplicado no dia a dia,
não só de quem vai trabalhar
com isso, mas como isso, na
verdade, é utilizado?
Então, casos de uso reais, né?
Tem, realmente, pessoas utilizando isso?
Existem clientes que estão utilizando o
DNA?
E, realmente, ou não? É uma
balela.
Muita gente acredita, ainda, quando eu
começo a falar desse tópico, que
é balela.
E aí, eu acho engraçado, porque
muita gente que tá aqui pode
achar engraçado esse tema.
Porque tem gente que trabalha com
IA há muito tempo.
E que já faz muito projeto
com IA.
O Jorge é um deles. Eu
acho, assim, ó...
O Jorge falava... E eu vou
ser muito sincero.
A gente trabalhou junto. O Jorge...
Admiro muito o Jorge, gente.
Eu gosto de falar isso pros
meus mentores.
Eu não tenho muitos mentores, mas...
Os mentores que eu tenho, eu
gosto muito de exaltar.
Porque são partes e componentes de
você.
O Jorge foi uma figura essencial
na minha vida.
Pra que eu me tornasse quem
eu sou hoje.
Mas eu vi o Jorge falando
de IoT.
Caralho, quantos anos, Jorge? Dez anos?
Dez anos, né?
Eu acho que quase, se não
mais.
E aí, eu vi o Jorge
falando de IoT.
Falei, cara, por que o Jorge
tá falando de IoT? Ele é
doido, né, velho?
Só que é aquela coisa, né?
O cara tava muito mais à
frente.
Então, o Jorge fala de IoT
e trabalha com projetos de IoT.
Eu nem sei quantos anos já.
E, assim, com clientes fora do
país.
E o que eu tô querendo
falar é isso.
É que, às vezes, o que
soa louco pra gente, pra outras
pessoas, é totalmente normal.
Por isso que é importante você
estar em ambientes...
Que nem o Jorge falou, cara,
vim aqui pagar um workshop pra
escutar.
E vai ali e escuta um
pouco.
Pra você trazer em suma, pra
que você possa tomar uma decisão.
Eu vejo muitas discussões no LinkedIn.
A galera, né?
Tem um grupo da galera que
é contra.
Eu não sei porquê, mas tem
uma galera que é do contra.
E realmente não entende e simplesmente
é do contra.
E eu vou mostrar pra vocês
que, realmente, existem coisas acontecendo
nesse
espectro.
E eu acho que, realmente, a
base de tudo é a gente
falar um pouquinho de LLMs.
Obviamente que, de novo, eu não
vou entrar em tanto detalhe aqui.
Mas eu quero manter o entendimento
do que é necessário pra gente
no nível negócio.
Pra que vocês possam escolher o
que realmente vai fazer a grande
diferença no estudo de vocês, né?
Então, obviamente que a gente teve
aí, nos últimos três, quatro anos,
uma mudança, né?
Um paradigm shift muito grande em
como nós consumimos conteúdo.
Em como nós, de fato, trabalhamos,
né?
Termos como vibe coding e assim
por diante estão bem na moda.
Existem várias coisas nesse espectro.
Mas uma coisa surpreendente que, de
fato, mudou é...
Eu lembro...
Há uns dois anos atrás, três
anos atrás ainda, quando eu não,
de fato, utilizava LLMs no meu
dia a dia.
Eu pensava, cara, o Google nunca
vai deixar de ser o Google,
né?
Já fazem mais de 23, 24
anos.
É impossível, cara.
E quando o Chat APT foi
lançado, assim...
Cara, assim, ó...
Eu não sei vocês.
Mas quando você usa da forma
correta, a produtividade que você ganha
é uma coisa, assim...
Eu nem sei explicar em relação
à produtividade que você ganha.
Então, é uma coisa absurda o
que esses modelos de fundação fizeram.
Então, cara, não sou uma super
nova de conseguir fazer várias coisas
ao mesmo tempo.
De entender e de gerar novo
conteúdo baseado em predição.
Mas as diferentes facetas que nós
temos e que a gente consegue
fazer, né?
Então, por exemplo, a gente já
consegue entender características marcantes de
cada
um desses modelos.
Por exemplo, não sei vocês, mas
na minha perspectiva, o cloud, ele
sempre se dá muito bem.
Melhor do que os outros.
Pelos meus testes em coding.
Ele é muito bom em coding.
Pelo menos para engenharia e para
o que eu faço, eu vejo
que ele realmente faz melhor do
que os outros.
Quando a gente olha o Gemini...
O Gemini foi uma decepção inicialmente
para mim.
1 .0, 1 .5, Flash e
assim por diante.
Eu achava extremamente triste a Google
ser o que ela é e
não tivesse um modelo de fundação
tão interessante quanto os outros.
Mas depois o 2 .0, 2
.5...
O 2 .5 Pro, enfim, tem
mostrado ser um ótimo cara.
O ChatPT, muito bem para General
Proposers.
O Llama eu usei pouco, então
eu não sei.
DeepSeq eu usei pouco também, então
não posso falar.
Mas sei que muita gente utiliza
e muita gente fala muito bem
dele.
E o Grok.
Deixa eu perguntar aqui.
Alguém aqui...
Eu uso o Grok no meu
dia a dia.
Mas assim...
Alguém tem algum comentário mais, eu
diria, direcionado às qualidades do Grok?
Porque eu leio muito o KB,
eu leio muita coisa sobre ele
ser de fato um modelo totalmente
diferente, principalmente agora no Grok 4.
Mas eu ainda não consegui ver
aonde de fato ele se destaca,
tá?
E aí, de novo, a interpretação
minha de fato.
Eu ainda não consegui sentir essa
diferença de falar, cara, ele é
muito melhor sobre os outros modelos
em relação a isso, né?
Beleza.
Então, geralmente, no final do dia...
Nós temos um prompt, um LLM
e uma resposta, né?
Então, aqui do nosso lado, do
usuário, a gente entra um input.
E aí, concordo com o Jorge,
a gente tem que aprender a
falar português melhor, escrever português
melhor,
inglês melhor, hoje.
Então, a gente imputa isso para
dentro desse modelo de linguagem e
você recebe uma resposta.
E aí começa uma coisa muito
foda, na minha opinião, que é
o seguinte.
Quando você é consumidor, é isso
que você precisa se preocupar.
Em trabalhar com prompts.
Né?
E, sinceramente, cada vez menos, tá?
Quando eu comecei a trabalhar com
Gemini, eu lembro que não dava
para escrever qualquer coisa.
Técnicas como Chain of Thought, como
Field Shot, como algumas outras coisas
realmente faziam diferença.
Cara, hoje...
Assim, desculpa o palavrão, eu fiquei
muito de cara hoje.
Eu peguei um documento PDF e
eu simplesmente falei assim, extrai para
mim.
E, cara...
Eu fiz isso anteriormente e foi
muito ruim.
E eu fiz isso no Grok
e eu fiquei, assim, assustado.
Eu fiz isso no ChatGPT e
eu fiquei assustado.
Eu fiz isso dentro do Cloud
e eu fiquei mais assustado ainda.
Ou seja, a barreira de prompting
cada vez mais tem diminuído.
Entretanto, existem algumas técnicas que você
vai implementar e a gente vai
ver aqui que fazem diferença em
como o modelo faz reasoning,
em como ele pensa, em como,
de fato, ele entende.
Principalmente se é saudável.
Teve uma coisa engraçada que os
caras perguntaram.
Cara, quantas pedras eu posso comer
por dia?
E aí os modelos respondiam, cara,
duas pedras por dia que tem
mineral e assim por diante e
tal.
Então, cada vez mais a ideia
é que eles fiquem inteligentes, principalmente
por essa ideia de reasoning, que
não era disponível em todos os
modelos.
E hoje você pode entender que
basicamente todos fazem muito bem essa
parte de reasoning, que é o
Chain of Thought.
Que é você instruir, você interar,
você entender e pensar de fato
e racionalizar em cima daquela resposta.
Então, de fato, o que a
gente vai ver ao longo do
tempo é cada vez mais essa
barreira de prompting.
Eles entendem muito bem isso, mas
isso não quer dizer que a
gente não possa trazer kills e
trazer coisas kills e coisas que
a gente possa melhorar.
E eu vou trazer alguns exemplos
reais aqui.
Beleza, então na superfície é isso.
Mas aí deixa eu perguntar para
vocês aqui no chat, já que
a gente tem uma galera aqui,
quase 220 pessoas lotadão hoje.
Obrigado a todo mundo que está
aqui.
Quem aqui já implementou uma solução
simples?
Ou já brincou com alguma coisa?
Ou já tem uma API que
não seja trabalhar ali na IDE
ou trabalhar ali no front do
Cloud ou do GROK ou assim
por diante?
Quem aqui fala sim?
Quem já brincou por debaixo dos
panos?
Então deve ter uma galera que
já brincou com isso e eu
acho que o que vocês entenderam
é que é totalmente diferente.
Você precisa se preocupar muito com
o contexto, você precisa se preocupar
com memória, você precisa se preocupar
com token, você precisa se preocupar
com um trilhão de coisas.
E isso é uma das coisas
que começa a trazer para a
gente muitas perguntas.
E aí traz exatamente a gente
a esse momento que a gente
está aqui no workshop.
Beleza.
O que tudo isso tem a
ver?
Como que eu entendo tudo isso?
Como que eu conecto esses pontos
e assim por diante?
Então a gente vai tentar aqui
conectar todos esses pontos.
E aí a gente precisa entender
algumas coisas básicas para que a
gente tenha um entendimento de como
trabalhar.
Qual o objetivo final aqui?
A gente sair com o entendimento
numa superfície de como isso funciona?
Por debaixo das caixinhas ali bem
pouco, né?
Abstraído.
E entender o que que se
refere ao mundo de RAG.
Por que que ele é tão
importante?
Então essa é a grande ideia
e a gente aprender a aplicar
técnicas de prompt engineering para fazer
a augmentação na hora do retrieval.
Então a gente vai entender os
passos do RAG e a gente
vai entender o que que a
gente consegue fazer em cada um
deles e a gente vai abrir
um set de otimizações em cada
um deles, né?
O que que é chunking?
O que que é embedding?
Quais são os modelos que nós
usamos?
Né?
Qual é o trade -off que
a gente vai escolher?
Como é armazenado no vector database?
Né?
Então a gente tem o processo
offline, a gente tem o processo
online.
Como isso funciona?
Então a gente vai tentar entender
tudo isso hoje, tá?
Mas como diz muita gente, o
ChatGPT ou o modelo de fundação
LLM, nada mais nada menos do
que ele é, é que ele
é um lindo autocomplete, né?
Autocomplete maravilhoso, né?
Porque no final das contas o
que ele vai tentar fazer é
predizer a próxima palavra, né?
É de fato prever a próxima
palavra.
E aí essa parte é muito
interessante, principalmente quando a gente
começa
a entender como isso funciona, né?
Na verdade, acho que a palavra
fascinante é melhor aplicada aqui.
Então, basicamente em termos probabilísticos
que
a gente quer entender, por exemplo,
o integrador saiu do restaurante e
foi para.
Então o que a gente vai
entender aqui, por exemplo, é que
na hora que ele vai calcular
a probabilidade, ele vai, nesse caso
aqui, por exemplo, entender dentro do
modelo tridimensional do trade -off.
Que a gente vai ver um
pouquinho ali da parte de embeddings.
É que a característica é que
possivelmente a próxima palavra que vem
é endereço, né?
Então, obviamente o que a gente
entende é o contexto que a
gente aplica, ele é extremamente importante
para a próxima palavra.
E outra coisa também que a
gente entende é que nem sempre
a mesma pergunta vai gerar a
mesma resposta, né?
Então vocês podem ver que vocês
fizeram essas perguntas aqui.
Cara, o entregador saiu, enfim, pediu
para completar.
Possivelmente ele pode mudar.
Ao longo do tempo das respostas.
Porque existem outras coisas que podem
ter uma maior probabilidade ou uma
probabilidade igual.
Dependendo de como você escreve e
do significado da palavra.
Na real é que ele é
muito bem treinado para poder entender
isso.
Então eu tenho um contexto, eu
tenho o cálculo dessa probabilidade, né?
Eu tenho realmente um entendimento de
qual probabilidade vai ser utilizada na
próxima palavra.
E eu vou para a próxima
palavra.
Que, na verdade, a gente não
fala de palavras.
Obviamente que no nosso contexto humano
a gente fala de palavras.
Só que, na verdade, o modelo
entende um conceito chamado token.
E é por isso que esse
cara é tão importante.
Quantos tokens você enviou.
Quantos tokens você recebeu.
O contexto do seu token.
Então se você olhar as APIs,
por exemplo, é tudo feito por
token.
Então a gente vai tentar entender
como isso funciona.
Se eu preciso prever, de fato,
a próxima palavra.
Como que eu posso instruir o
modelo num contexto que seja favorável.
Que seja favorável para que eu
possa, de fato, entender e otimizar
e tunar e esperar algo muito
bom dessa predição.
Então a gente vai tentar entender
um pouquinho como isso funciona.
E, obviamente, essa é uma área
que eu estou atacando 0 ,00001
do ecossistema do que é essa
próxima palavra.
A gente vai entrar um pouquinho
mais em detalhes aqui embaixo.
Mas a ideia é a gente
entender numa superfície que, pelo menos,
a gente não...
Que não sejamos mais leigos em
relação de olhar e falar, cara,
o que ele está fazendo por
debaixo dos panos.
A gente vai entender um pouquinho
como isso aqui funciona.
Ótimo.
Então um token.
Como funciona um token?
Então eu tentei traduzir aqui na
forma mastigadinha para ficar mais fácil
de a gente entender.
Como que funciona o processo desde
a escrita do usuário até, de
fato, essa entrada dele dentro do
modelo de fundação.
Então, por exemplo.
Então, por exemplo.
Se eu escrevo aqui, o cupom
grátis chegou.
A primeira coisa que ele vai
fazer, ele vai tokenizar essas palavras.
Então ele vai quebrar isso aqui
em tokens.
E a gente vai ver um
pouquinho isso aqui.
Então aqui, no caso, ele decidiu,
o modelo decidiu quebrar em 1,
2, 3, 4, 5 tokens.
Esse tokenizer.
E aí eu tenho classificações e
categorias de tokenizers.
Por que eu estou explicando isso?
Porque se...
Se...
E aí eu não acho que
é uma das coisas que você
vai focar.
Mas é importante você saber.
Uma das opções que você pode
tunar.
E aí você pode utilizar a
Lhama Index e algumas outras características.
E alguns outros modelos de inferência
e assim por diante.
Para aplicar técnicas de tokenização para
melhorar isso.
Eu não acredito que seja aonde
você, de pequeno, médio, até grande.
Grandes ambientes vai fazer.
A não ser que você realmente
tenha um caso de uso específico
para fazer isso.
Por exemplo.
Um dos projetos que eu estou
agora.
É um projeto muito interessante.
Que basicamente.
É uma empresa de vendas de
shopping online.
Fora do país.
Muito grande.
Que é Nordstrom.
E aí eles trouxeram.
Contrataram a Piffin.
Para a gente fazer um projeto
com o GNI.
Que vai fazer basicamente o seguinte.
Eles recebem dos fabricantes.
E das pessoas que anunciam dentro
do site deles.
As especificações dos produtos.
E cara.
São milhares.
Centenas de milhares de produtos.
E eles recebem essas especificações.
E muitas das vezes.
A imagem não bate com a
especificação.
Olha só que projeto legal.
Não bate com a especificação.
E existe um time gigantesco.
Para tentar corrigir.
Para tentar ver o width.
O height.
E assim.
E tentar comparar esses números.
Com a imagem que ele recebe.
Dos produtos.
Para com esse meta dado.
Que é mostrado no site.
Que está no banco de dados
relacional.
Qual é o grande projeto aqui?
Em vez de fazer isso de
forma manual.
É utilizar o GNI para fazer
isso.
Então o que a gente vai
fazer?
A gente emprega um modelo de
GNI.
A gente coloca um prompt.
Para poder especificar isso.
A gente já está em 37
tipos de prompt diferentes.
Por categorias diferentes.
Para que ele possa ler.
E comparar esse meta dado.
E ver se faz.
E se faz de fato diferença.
E isso é um projeto.
Que já está entrando em produção
agora.
Então.
Nos próximos.
Entrelaçamentos.
De novos onboarding de produto.
Eles já estão utilizando esse projeto.
Então eles colocam isso dentro do
Google Cloud Storage.
A gente roda uma série de
funções.
De event driven.
Lá no Cloud Run.
Com chamadas em LLM.
Para poder fazer esse cálculo.
E escrever no banco de dados
relacional.
De fato.
O valor que está de acordo.
Com aquela imagem entrelada.
Aquele binário dele.
Então.
Isso já é.
Realidade.
Tá.
E cara.
A gente está falando de um
projeto.
Muito grande.
E.
A gente nunca pensou.
De fato.
Nunca tivemos o problema.
De falar de tokenizer.
Mas é interessante entender.
Que existem categorias.
Dentro deles.
Né.
Então a gente tem um word
base.
Character base.
E sub word base tokenizer.
Na maioria dos.
Dos.
Diria.
LLMs.
Né.
Os flagships.
Eles estão dentro dessa categoria.
De.
Sub word base tokenizer.
E é interessante.
Que por exemplo.
Cada tokenizer.
Cada embedding.
Que você escolher.
Ele vai se comportar.
De uma forma diferente.
Tá.
Não existe um.
One size fits all.
Aqui.
E só por curiosidade.
O chat GPT.
Utiliza.
Atualmente.
Esse cara.
Chamado TikToken.
Então eu deixei.
Aqui.
Caso vocês tenham interesse.
De entender.
Né.
O que que esse cara.
Na verdade.
Não o que ele faz.
Né.
Mas o que de fato.
Né.
O que levou.
Ao chat GPT.
Escolher esse cara.
Ele tem 15 mil estrelas.
É.
Ele é um projeto.
Escrito.
Inicialmente.
Em Python.
Se eu não estou enganado.
Tá.
E tem então.
Algumas coisas legais.
Aqui.
Bem interessantes.
Sobre como ele.
Performa.
Isso.
Tá.
Então.
A gente vai entender.
Na hora que a gente.
For chamar um embedding.
Um modelo de embedding.
Para embedar.
Isso.
A gente vai entender.
Como que esse cara faz.
Então.
O que ele vai fazer.
Ele vai transformar.
Essas informações.
Aqui.
Em tokens.
E aí.
Eles vão gerar.
Esses tokens.
Nada mais.
Nada menos.
Vai ser gerado.
Um.
Número.
Né.
Em cima deles.
O que.
A representação.
Desse.
De cada palavra.
Dessa.
Em relação.
Ao token.
Né.
E não é só isso.
Agora.
A gente.
Vai para a parte.
Mais complexa.
Que é.
Transformar.
Esses números.
Entrelaçar.
Eles.
Em vetor.
E a gente.
Vai entender.
Um pouco.
Mais.
A frente.
O que.
Que.
Esse vetor.
O que.
Ele faz.
Mas.
Na verdade.
Você.
Vai ver.
Que.
Para.
Cada.
Número.
Desse.
Cara.
Aqui.
Ele.
Vai.
Ter.
Um.
Vetor.
3D.
A gente.
Não.
Consegue.
Representar.
Isso.
Em 2D.
A gente.
Vai.
Como.
Que.
Essa.
Representação.
Em 3D.
Ela.
Acontece.
O.
Que.
É.
Legal.
A.
Gente.
Saber.
Disso.
Por.
Exemplo.
Que.
O.
Token.
Ele.
É.
A.
Parte.
Principal.
Do.
Rolê.
É.
Como.
Nós.
Seres.
Humanos.
Interagimos.
Com.
LLM.
Então.
Nesse.
Caso.
É.
Se.
A.
Gente.
Pegar.
Aqui.
Um.
Exemplo.
Simples.
Pegar.
Por.
Exemplo.
De.
Fato.
O.
Cupom.
O.
Cupom.
Grátis.
Chegou.
Né.
Então.
Eu.
Pegar.
O.
Cupom.
O.
A.
Consegue.
Ver.
Que.
Caracteres.
Especiais.
Por.
Exemplo.
É.
Eles.
Consomem.
Um.
Token.
Enquanto.
Palavras.
Normalmente.
Não.
São.
Grupos.
E.
Aí.
Eu.
Lembro.
Do.
Sam Altman.
Falando.
Isso.
E.
Eu.
Ri.
Bastante.
Que.
Ele.
Falou.
Que.
É.
As.
Perguntas.
Da.
Galera.
Falou.
Obrigado.
Adicionando.
Esses.
Pontos.
De.
Interrogações.
Que.
Tava.
Tava.
Custando.
Bastante.
Para.
Para.
Para.
Para.
Infraestrutura.
Dos.
Caras.
Estava.
Derretendo.
Os.
Sistemas.
Deles.
Mas.
Brincadeiras.
A parte.
Aqui.
Quando.
Eu.
Digito.
Esse.
Cara.
Então.
Coupão.
Grátis.
Chegou.
Aqui.
Eu.
Tenho.
Um.
Espaço.
Aqui.
É.
Coupão.
Grátis.
Chegou.
E.
Eu.
Tenho.
Estou.
Quem.
Diz.
Aqui.
Está.
Vendo.
Então.
A.
Representação.
Do.
Que.
Ele.
O.
Fato.
Ele.
É.
Que.
Numericamente.
Falando.
A.
E.
Olha.
Só.
Que.
Interessante.
Aqui.
Eu.
Estou.
Utilizando.
O modelo.
De.
Pt.
4.
Né.
Então.
Posso.
Vim.
Aqui.
Por.
Exemplo.
Utilizar.
O.
3 .5.
E.
Aí.
Eu.
Já.
Mudei.
Estou.
Vendo.
Então.
Os.
Mesmos.
20.
Caracteres.
Utilizam.
7.
Tocantes.
E.
Se.
A.
Gente.
Fora.
O.
Legado.
Os.
Mesmos.
É.
A.
Mesma.
Sequência.
De.
Palavras.
Agora.
Consumem.
10.
Tocantes.
Então.
Obviamente.
Que.
Que.
Gente.
Consegue.
Entender.
Cara.
Será.
Que.
Realmente.
Eu.
Um.
Trabalho.
Muito.
Melhor.
Do.
Que.
O.
Chat.
E.
Pt.
4.
0.
4.
E o.
Modelo.
De.
Ele.
É.
O.
Caso.
Específico.
Sim.
Mas.
Não.
Sei.
Se.
É.
O.
Local.
O.
Pt.
Olhar.
Como.
Primeiros.
Pontos.
De.
Otimização.
Tá.
É.
Então.
Só.
só para vocês entenderem que a
gente consegue ter uma ideia
de tokens versus caracteres, tá?
Então, token não é a mesma
coisa
do que texto. Muita gente acha,
poxa,
eu estou mandando 10 mil
palavras, linhas ali de caracteres
para dentro do meu modelo
e realmente ele está consumindo isso.
Não,
ele vai transformar isso em tokens
que têm representação
numérica e a partir de fato
isso vai ser colocado
e vai ser enviado para esse
cara.
Inclusive, ele fala o seguinte, né?
Ele fala aqui
do TikTok, né? Que é utilizado.
Até se tiver alguma pergunta aqui
interessante
você
me aponta, tá?
Se eu souber...
Se eu não souber, o lado
bom é que eu passo
tudo para o Jorge, porque o
Jorge sabe, então
eu já fico feliz. Agora, se
fudeu, Jorge.
É melhor você sair, porque se
não sair
eu vou falar, cara, eu não
sei. Passa
para o Jorge aí que o
Jorge sabe.
Mas, basicamente, essa é uma parte
complexa de explicar
para leigos, então
eu, inclusive, lembro que
quando eu estava conversando com
o Chatip, ele falou, cara, me
explica no modo
mais burro para Dummies, por favor.
E mesmo ele me explicando em
modelo
dummy, eu ainda não consegui entender
na época,
então exigiu muita coisa. Mas, aqui
é só uma ideia, a gente
vai entender
aonde são os pontos importantes
de que você precisa, de fato,
compreender isso, mas
no final do dia, você
tem modelos de input, ou seja,
aqui você pode pensar em realmente
embedar uma palavra,
você pode embedar um documento, você
pode...
embedar um vídeo, você pode embedar
um
áudio, tá? Então, não somente
pedaços específicos,
você pode embedar um arquivo
como um todo. E aí, o
que que a gente
tem? E por que que a
gente vai ver isso? Poxa, mas
por que que eu estou
vendo isso? Porque você vai ver
que na hora que você for
trabalhar com
vector databases, e quando você se
comunica
com o LLM, você precisa
de um embedding model, tá?
De um embedding model. Então, você
vai precisar
entender isso.
Então, o que que ele vai
fazer? Ele vai
basicamente criar aquela representação
que a gente viu, mas
no modelo tridimensional,
tá? E a gente vai entender
um pouquinho mais
na prática visual
ali do negócio. Então, é basicamente
como se eu estivesse recebendo streams
de
texto, ou de qualquer característica que
fosse,
a gente tem um modelo de
embedding, e ele vai transformar
neste carinha aqui,
nessa transformação vetorial,
e aí vai plotar
isso dentro de um espaço semântico,
e essa é a parte bonita
do negócio,
né? Então, realmente,
o chat APT não entende as
palavras,
ele entende números, e ele entende
probabilidades, ele entende
o espaço semântico disso,
e como as palavras são encaixadas
perto das outras, e com a
probabilidade
delas estarem ligadas,
tá? E isso é muito bonito
em relação quando a gente entende
um pouquinho
de como isso funciona.
Existem vários modelos de embeddings
aqui disponíveis no mercado, tá?
O que eu vejo a maioria
das pessoas utilizando
é, de fato, o OpenAI,
nem sempre. A gente vai entender,
por exemplo,
que o modelo de embedding que
você
embeda o dado dentro do vector
tem que ser o mesmo
modelo em que você consome
do seu vector,
né? E isso é meio que,
assim, de praxe.
Você vai ser obrigado a fazer
isso,
caso contrário, você vai ter
características diferentes, dimensionais,
sendo escritas dentro desse vetor,
que não vai possibilitar com que
você fale a mesma
língua, tá? Então, você precisa,
se você embeda com text embedding
ADA 002, você vai consultar
na hora de fazer esse search
também
com text embedding ADA 002.
Beleza?
Vai reunindo
as perguntas, Matheus? Eu tô vendo
umas perguntas
legais ali.
E aí, reúne que eu paro
daqui a pouco e a gente
faz um check. Então, basicamente,
como que ele entende esse conceito
semântico,
de espaço semântico?
Durante o treinamento, e aí por
isso que isso é
muito caro, GPUs, milhões de máquinas,
e por aí vai,
que a gente já sabe,
ele divide em dois conceitos de
pares positivos e pares negativos,
aonde, provavelmente,
se você colocar good morning,
good morning tem probabilidade
muito grande de estar perto e
atrelado
a hello.
Muitas vezes, quando a gente utiliza
hello,
a gente utiliza good morning. Então,
matematicamente falando, existe uma
proximidade, uma
probabilidade desses
pares estarem pertos um do outro.
Então, imagina que pra cada
possibilidade que você tem,
esses pares são calculados múltiplas
vezes e testados múltiplas vezes,
tá? Assim como também
pares negativos. Então, por exemplo,
good morning provavelmente vai ser
colocado muito longe de
noisy place, porque não existe muita
similaridade em relação a esses dois
caras.
Então, milhões de vezes durante o
treinamento,
ele vai fazer isso com frase,
imagem, áudio,
produto, pergunta, e assim por diante
na web toda. Então, imagina por
que
um modelo realmente de fundação é
extremamente
caro, né, e extremamente
inviável pra empresas de pequeno, médio
e grande porte. Tem que ser
de porte gigante,
de grande preço, realmente.
E uma das formas de você
entender essa
representação pra leigos como eu
e como nós, que não somos
da
área de fato, é você olhar
isso aqui.
Então, por exemplo, como que
é a projeção de embedding? Bem
legal. Então, o que você pode
fazer aqui, por exemplo, é você
colocar good
e aí, o que que você
consegue fazer? Quando
eu seleciono good,
eu tenho aqui
quais são os pontos de
probabilidade entre eles. Não posso entrar
aqui no meu vetor, por exemplo,
e aí eu consigo ver
aonde good
de fato está colocado
no vetor
e aonde ele está perto
do quê? Deixa eu achar você,
amiguinho,
calma aí. Paciência,
gente. Estou tentando achar good aqui.
Você está aqui, não vou perder
você.
...
...
...
...
...
...
...
...
...
...
...
...
Então, você começa a ver a
relação
das palavras e como elas são
colocadas perto justamente
com os pares positivos ou os
pares
negativos. Cara, isso aqui é muito
impressionante.
Um bando de gente louca,
matemática, consegue fazer isso.
Então, é que você consegue ter
uma ideia de como
de fato ele é embedado, um
vetor
ali, colocado isso de uma
forma 3D para que a gente
possa entender.
E aí eu vou deixar o
link aqui
para vocês se divertirem. Tem umas
coisas legais
aqui. Isso aqui é bem foda.
Poxa, mas por que
eu preciso saber disso para o
meu dia, né?
Na verdade, esse conhecimento
para a gente que vai operacionalizar
a nível business, obviamente
que era importante que em algum
momento você
vai precisar entender o conceito como
um todo
e aí vai, obviamente, depender do
nicho
onde você está, enfim, para cada
vez mais
entender isso.
Isso, obviamente,
que quanto mais você entende,
mais fácil dar soluções para você
vacerar, tá? Mas aqui, pelo menos,
você tem uma ideia de como
esses dados são
plotados matematicamente falando,
beleza? E agora, o que
que isso traduz a, realmente,
a característica real do
negócio, né? Por exemplo, casos
de uso reais, né?
E aí, por exemplo,
caso de uso real
e eu vou mostrar um caso
para vocês
reais, real, por exemplo,
é essa extração de documento inteligente.
Por que que eu digo inteligente?
Porque, cara, extração de documento sobre
PDF, imagem, enfim, cara,
ela está aí há muitos anos.
OCR e assim por diante, poxa,
tem gente que está cansada de
trabalhar
com isso, né? Já acontece há
muitos
anos, né? A diferença é que
agora a gente tem facilidades em
relação
a como fazer isso, né? Então,
a gente pode
em vez de ter um modelo
treinado,
ou em vez somente de
entrar no OCR, ou de treinar
um modelo
realmente específico para
entender diferentes modelos de documento,
você já tem isso muito feito
no modelo
de fundação, né? Então,
o que que acontece é que
você pode utilizar
ele ao seu favor para isso,
né?
Então, uma das coisas que você
pode fazer
e é, vamos supor,
e isso realmente é um projeto
que eu vou entrar em detalhes
com vocês, reais,
tá? Em que eu tive
o prazer de implementar fim a
fim,
é, cara, o arquivo
cai dentro de um bucket,
e aí isso vai ser agnóstico,
né?
E aí seja qualquer tipo de
arquivo,
vai receber uma notificação, e aí
obviamente que o Airflow é um
cara muito bom
para você fazer isso, se a
gente estiver pensando de forma
agnóstica. Bem, eu vou
aplicar uma transformação de negócio
em cima deles, ou eu vou
reagir
a esse evento. Obviamente
que se eu estou recebendo, por
exemplo, no meu caso
vários PNGs,
na verdade TIF, e posteriormente
eu tenho que converter esse cara
para PNG,
então eu recebo esse arquivo
em TIF, e às vezes zipado,
eu preciso
de zipar esse cara, transformar ele
para PNG,
e daí sim, eu
posso criar um prompt de extração
que vai me falar o seguinte,
olha,
dado esse documento,
e a gente vai ter técnicas
para fazer isso,
obviamente não simplesmente num projeto de
produção,
você vai falar, cara, extrai os
dados desse
documento, não funciona dessa forma,
então a gente vai passar ali
alguma estrutura, a gente vai passar
uma super
estrutura para ele, a gente vai
determinar
qual é o role dele, qual
é o contexto dele,
qual é a especialização
desse cara, quais são
os pontos positivos, negativos,
as validações que ele vai fazer
dentro
do prompt, então a gente vai
estruturar um prompt
para fazer isso, e daí eu
vou
extrair essas informações,
posteriormente eu vou estruturá -las
ao longo do processo,
e escrever elas dentro de um
database,
e aí pode ser dentro de
um banco de dados relacional,
pode ser dentro de um data
warehousing,
pode ser dentro do seu lake
house,
independentemente, o output
você pode decidir de como você
quer fazer isso,
então hoje você pode utilizar
a extração de documento
de forma inteligente, utilizando prompts,
logo mais aqui,
na segunda parte da nossa aula,
eu explico um caso real,
que hoje, público, em que eu
posso
mostrar para vocês como isso aqui
realmente acontece
na prática, muita gente olha isso
e fala
cara, será que isso realmente é
utilizado, de novo,
será que a gente utiliza isso
no nível
business, né? Sim, nós utilizamos isso
no nível business bastante, tá?
Então esse é um outro cara.
Um outro
ponto muito legal em relação a
isso
é o conceito de LLM
Ops, que para mim
é uma coisa que foi
uma mão na roda,
roda para mim, e eu vou
explicar durante
o processo, por quê? Porque
bem, quando você tem um prompt,
beleza, mas
muitas das vezes você precisa versionar
o prompt,
né? Por exemplo, muitas das vezes
você tem eu como engenheiro de
dados
trabalhando no prompt, e provavelmente
já aconteceu isso de eu chegar
e falar
pô, cara, eu não estou conseguindo
extrair, eu não estou conseguindo
ter uma melhor acurácia desse cara
aqui
para esses tipos de documento,
tem como você dar uma olhada
no meu prompt
e modificar esse meu prompt?
Agora imagina o seguinte, se esse
prompt
ele está tidily coupled na
programação, ou seja,
ah Luan, mas claro, você não
cria código
e
configuração dentro das
mesmas classes e objetos dentro do
pipe,
sim, obviamente, mas quando
você faz um deploy dentro de
um objeto,
por exemplo, como Cloud Run ou
assim por diante,
você roda seu pipeline de CISD,
o seu prompt
ele está onde? Ele está dentro
da sua stack de deployment,
ele está dentro da sua unidade
de deployment,
dentro do seu docker, ele dentro
da sua unidade de container.
Então, imagina o só,
o Paul chega para mim e
fala
bem Luan, eu preciso mudar esse
template,
existem algumas coisas que a gente
pode aplicar de técnicas
aqui. Lá vai o Luan ter
que rodar
o processo de CISD novamente.
E aí, esse prompt
não ficou legal. Bem, a gente
modifica,
bota no git, passa pelo
processo de aprovação, faz o merge,
aprova,
roda o CISD novamente.
Mais 20 minutos, e assim por
diante.
E a gente vai inteirando em
cima disso.
Bem, será que não seria legal
ter um ambiente desacoplado onde eu
posso
trabalhar o prompt
de forma separada?
E não somente isso, imagina
agora que eu não tenho um
prompt só, eu tenho
13, 15, 17 prompts.
Então, eu quero versionar esses prompts,
eu quero saber, por exemplo,
toda a parte de histórico
de modificação que aconteceu, eu quero
ter rastreabilidade deles com as chamadas
do modelo, e assim por diante.
Então, isso se chama LLM Ops.
Então, não está, a gente não
está
falando de ML Ops, que é
todo o processo de fazer realmente
todo o ciclo de vida
de um modelo de Machine Learning,
desde
da camada de interpuração,
perpruçamento, serving,
e assim por diante. A gente
está falando especificamente
dos modelos de fundação,
de como que você faz LLM
Ops.
E a gente vai ver algumas
ferramentas que fazem isso. Para mim,
destaque
de fato aqui é
o LangFuse. Alguém já usou
o LangFuse aqui? Só por curiosidade,
para saber.
Ninguém?
Nossa, é lindo isso aqui.
Vou mostrar para vocês
isso, tentar trazer para vocês
essa informação, é de chorar.
Principalmente quando você tem a necessidade
de fazer isso.
Parece uma coisa
bonita, mas é uma coisa extremamente
necessária.
Eu vou explicar para vocês logo
mais.
Depois,
Reg, de novo,
parece que eu estou dançando, mas
não.
Cara, esse cara aqui,
esse para você,
para mim,
hoje, um dos principais
pontos
de
entrada
em empresas de pequeno, médio
e grande porte, que são chatbots
inteligentes. Então, não é aquele chatbot
que simplesmente era fiel, sei lá,
e você simplesmente lançava as opções,
mas trazer inteligência para o seu
chat.
E tudo isso começa com uma
base
de conhecimento. E essa base de
conhecimento
tem que ser criada. E eu
vou mostrar
algumas coisas na vida real e
vou trazer
algumas lições que vocês vão evitar
e vão economizar muito dinheiro
e muita dor de cabeça.
Eu vou trazer um exemplo real
para vocês,
dois exemplos reais. O primeiro exemplo
que eu vou trazer para vocês,
de Reg, é exatamente
o primeiro sobre, cara,
qualquer empresa enterprise grande
já tem esse tipo de
inteligência. Principalmente você vai ver no
sites, você vai ver em atendimentos,
você vai ver nesse tipo de
processo.
E a gente vai entender como
constrói isso.
Mas para pequenas e médias empresas,
por exemplo, para infoprodutos,
para empresas de médio porte, isso
é utilizado
absurdamente. Eu vou dar um caso
de uma empresa que vocês conhecem,
chamada
Engenharia de Dados Academy, por exemplo.
A Engenharia de Dados Academy é
uma empresa de treinamento.
A gente está chegando esse ano
com faturamento de 5 milhões,
aproximadamente.
Uma grande vitória
para a gente.
Muitas pessoas aqui fazem,
utilizam a formação e assim por
diante.
E, cara,
uma das coisas que, como empresário,
hoje estando com 32 funcionários,
uma das coisas que eu comecei
a sentir como
empresário é a dor de
trazer novas pessoas.
E escutem isso, porque isso vai
funcionar
para todas as empresas. Eu estou
falando que aqui eu estou
vestindo o hatzinho de empresário.
E o Matheus pode me confirmar
isso.
E a gente começou a crescer,
começou a crescer,
e daí a gente começou a
ter um problema muito grande,
que é trazer um time comercial
para vendas.
O time comercial sabe muito bem
como é que é
essa interação.
Consome muito tempo.
A gente tem que passar a
imagem do especialista
para com a venda. A venda
não pode ser
agressiva demais para tirar o espírito
do que realmente você quer entregar.
A gente está falando com pessoas.
É extremamente complexo
fazer isso. E daí
o que a gente começou a
ver é, cara,
a gente precisa trazer uma inteligência
para dentro da empresa. A gente
contratou uma empresa
seis meses atrás
que fazia isso.
A empresa
falhou miseravelmente em entregar
o que a gente, de fato,
precisava.
Mas a grande ideia é, em
vez de, por exemplo,
quando eu estou em picos de
tráfego de venda,
por exemplo, quem aqui já adquiriu
a formação
de Spark e Databricks?
Nós temos hoje mais de
570 pessoas inscritas na formação.
De fato, é o produto nosso,
o nosso flagship.
Então tem muita gente que está
aqui
que, de fato, faz parte da
formação.
E, cara,
um dos problemas que a gente
teve, nós tivemos
uma queda de 30 % de
vendas
abaixo do esperado
por causa de timidez comercial.
O que eu estou explicando essa
dor para vocês?
Porque se vocês não entenderem a
dor,
não faz sentido vocês estarem aqui,
não faz sentido vocês, de fato,
entenderem
porque a gente está fazendo isso,
porque você precisa configurar e chegar
no nível de detalhe
como a gente vai chegar aqui.
Porque a gente precisa partir
de um problema real.
E aí
nós perdemos muita
venda durante esse processo, porque muita
gente
entrava 10 horas à noite, muita
gente perguntava
até a madrugada, tinha pessoas de
outros países
querendo comprar e tinham problemas
e assim por diante. E a
gente não tinha
ninguém para atender nesses tempos.
E quando a gente tinha tráfego
alto,
cara, um time comercial não consegue.
Por exemplo, uma pessoa não consegue
atender 5 pessoas ao mesmo tempo.
Então isso é impossível.
Então, cara,
por mais que você tenha script,
por mais que você tenha técnica.
Então o que a gente começou
a pensar?
Cara, a gente precisa trazer um
chat.
E eu quero
apresentar para vocês aqui em primeira
mão
o nosso chat
de prova de conceito para vocês
darem
uma brincada. O Matheus, Matheus,
coloca aí para a galera brincar
um pouquinho e trazer ao longo
desse dia o que eles acham
desse chat.
A gente está indo para a
versão 2, na versão
3 dele. Eu vou mostrar como
que era o
protótipo antigamente.
E eu vou mostrar para vocês
posteriormente
como a gente está fazendo isso
com o Langchain.
E Langflow
e Langfuse e assim por diante.
Ou seja, isso é
real. Não só isso, como
outros empresários do nosso lado também
trabalham com Reg. Existem várias
empresas que vendem essa solução
e assim por diante. Então eu
vou pedir para vocês
acessarem o que o Matheus está
falando.
Só para vocês brincarem um pouquinho
aí.
Quando vocês tiverem tempo.
Ele está dentro do Telegram. A
gente está fazendo
alguns testes no Telegram.
E aí essa
versão que vocês estão
é uma versão de
POC. Eu não vou
soltar a versão que a gente
utiliza internamente,
obviamente. Mas essa é a versão
de POC. Eu vou mostrar
o quão fácil foi configurá -la
e qual é a grande ideia
por detrás disso.
Mas, cara, de fato
quem aqui já tem essa
visão que o Reg, de fato,
é algo
extremamente
grande para os próximos anos?
Vocês já entenderam isso ou não?
Se vocês não entenderam,
não tem problema nenhum, porque essa
é a grande ideia
do treinamento aqui. É mostrar para
vocês
porque isso aqui realmente é um
grande
diferencial.
Beleza.
Quando nós
iremos, a gente vai ver que
o processo
de criação, mas nós possuímos
duas partes no processo de
Reg. Nós temos a parte
que a gente chama de pré
-processamento,
que é, de fato, capturar o
dado, fazer o chunking, o embedding,
que a gente vai ver isso
em detalhes,
inserir isso dentro do
Vector Database e,
de fato, consumir essas informações
com o chatbots.
E aí utilizando o conceito
online. Então esses caras estão acessando
os dados que estão
armazenados, os vetores estão armazenados, eles
embedem
os pets que estão armazenados dentro
do Vector Database. Isso é um
outro caso
muito utilizado hoje. Eu diria que,
cara,
esse é,
se não,
o caso mais latente real
utilizado no mercado hoje.
Ele está no top 3 definitivamente
de uso de GNI.
Então,
realmente esse
cara é fantástico.
Beleza?
Outro caso muito legal,
Text to
Query Intelligence.
Esse é um cara que a
gente também está brincando.
Um cara que a gente
já está fazendo isso na Pifian,
fora da Pifian,
para um cliente muito grande,
que já está em produção, inclusive.
Esse cara é um cara
mais complexo, mas é um cara
extremamente interessante de fazer.
Então imagina que você tem os
dados ali do seu
Vector Database armazenados,
que você pode utilizar para várias
coisas. Isso é uma
característica que a gente vai entender.
Depois a gente tem um banco
de dados relacional que armazena
essas características específicas.
A gente tem um modelo de
fundação,
e a gente tem prompts
que vão fazer o que? Exatamente
essa pesquisa. Então,
tanto no nível vetor,
para acessar a coleção, quanto no
nível de traduzir o texto
para SQL e traduzir
essa query aqui dentro. Mas não
somente
isso, trazer
entendimento semântico em cima
dessas consultas aqui,
para você gerar relatórios e realmente
respostas interativas para o usuário final.
É como se a gente estivesse
utilizando o Gini ali
da Databricks e algo do tipo.
Então essa é a grande questão.
Isso é muito interessante,
esse é um outro caso muito
latente
no mercado. Isso
desbloqueia muito do
processo de BI, muito do processo
de você
ter que ter um cara que
está ali
no meio fazendo essa tradução.
Você entrega para o usuário uma
linguagem
natural, onde o usuário pode fazer
isso. Eu já tenho vários
empresários, eu já conheço vários
pessoas fora Piffin que já implementam
isso no dia a dia e
que estão vendo
assim, melhorias
gigantescas, por exemplo, leitura de login,
acesso, por exemplo, dentro
de collections de MongoDB
utilizando linguagem
natural, consultar informação de usuário
em tempo real utilizando isso aqui.
Então, cara, isso aqui é uma
outra vertente
também, muito sinistra.
Matheus, coloca
para mim aí num pool,
qual dos três casos
eles gostariam de ver na
prática nos próximos workshops
ou realmente desses aqui?
O primeiro, o segundo ou o
terceiro?
Qual deles
entre todos vocês acham que seria
o mais interessante? Existem outros aqui,
mas esses três aqui são latentes
que a gente está trabalhando, que
eu tenho
conhecimento e que eu trabalho.
Então eu posso falar esses caras.
Calma que o Matheus
vai lançar a pesquisa sobre
isso. E aí
eu queria mostrar para vocês onde
tudo
isso começou. Isso tudo
começa com o N8n,
o nosso processo como um todo.
E aí a gente vai agora
falar um pouquinho de N8n.
E é importante a gente
setar algumas coisas interessantes
aqui. Quem aqui já escutou?
Quem aqui não conhece N8n
fala eu. Só para eu entender
aqui
o quão
vocês irão ficar
tristes.
Muita gente.
Tá. Eu ia fazer uma outra
pergunta, mas acho que não cabe
aqui, que é
cara, muita gente critica o N8n.
E aí de novo, eu acho
que a grande
importância de você entender tudo isso
é aonde se encaixa.
Nesse caso aqui,
o que que o N8n faz?
Ele é um serviço de automação
de workflows
inteligente. Então cara,
pensa que você consegue
operacionalizar, automatizar
basicamente se você está
utilizando stacks modernas e ferramentas
modernas que todos os seres humanos
normalmente
utilizam hoje na vida,
como você consegue fazer.
Tanto para ser JetOps, como SecOps,
como DevOps, como Sales e assim
por diante.
Você consegue fazer, cara, muita coisa
aqui.
E esse cara, ele
realmente é um grande divisor
de águas, não somente para as
empresas,
mas somente em como você consegue
operacionalizar certas coisas. Então você
consegue utilizar agentes, você consegue fazer
um bocado
de coisa aqui. E uma das
coisas que vocês
vão entender quando vocês começarem a
entrar
ainda mais no mundo de GNI,
de Data Engineering, de AI Data
Engineering
e assim por diante, é entender
aonde
cada ferramenta se encaixa perfeitamente.
De novo, o problema
não é o N8n, o problema
é como
as pessoas utilizam o N8n
em casos específicos. Então, por exemplo,
a galera quer fazer, eu já
vi a galera
tentando fazer RAG dentro do N8n.
Então assim,
vai funcionar? Não, porra, não foi
feito para isso. Então assim, é
aquela
questão da galera realmente não
utilizar a ferramenta da melhor
forma correta. Então a gente
vai entender, por exemplo, o que
ele faz
muito bem aqui, que são coisas
legais.
Uma das coisas que eu gosto
bastante,
eu já abro para vocês, vou
abrir para perguntas
já antes de começar aqui,
é essa parte de automações.
Ele já possui mais de
3 .861
workflows automatizados. Então, cara,
desde você criar o seu primeiro
agente,
como você capturar informações, raspar
informação de internet e assim por
diante,
então você pode chegar aqui em
AI, por exemplo,
você consegue ver quais são os
cookies, os cookbooks que você já
tem,
então a integração com o seu
RAG chatbot for companies document
using Google Drive Gemini,
toda a parte de
automação de Gmail.
Cara, aí aqui vai
um mundo de coisas que você
pode fazer.
É muito legal, tem umas coisas
muito legais assim,
de verdade. Por exemplo, uma das
coisas que a gente
fazia aqui, que é besta, por
exemplo,
toda vez que eu tenho
um workshop, toda vez que eu
tenho
um treinamento assim por diante, eu
tenho
que criar essa aula no Zoom.
Então isso era um processo que
a gente demanda,
só para vocês entenderem, no nível
mais empresarial, tá gente? Para
vocês que são contratados ali às
vezes,
não tem essa visão, como é
importante
esse nível de entendimento do business
como sendo.
Então a gente, cara, tinha um
time, na verdade tinha uma pessoa
que fazia essas coisas mais manuais,
eu diria, né? Então era o
time de suporte
ali. Cara, hoje a gente
tem uma agenda no Google, a
gente
registra na agenda,
automaticamente você tem uma automação
que vai, faz, cria essa
sala no Zoom e manda isso
lá
dentro de um local para a
gente no
Discord. Então, cara, essa sala
foi criada com sucesso, por exemplo.
E daí a gente usa esse
cara para distribuir
essa informação. Então parece besta, né?
Mas, cara, você acaba de economizar
5, 10 minutos
a cada criação dessa, num mês
talvez você
economizou 40, 50 minutos
de coisas que você podia fazer
por uma automação.
Então você consegue fazer muita coisa
aqui, tá?
E por que eu estou falando
isso? Obrigado,
Juliane, porque isso é o que
importa
no final do dia, tá?
E eu gosto de trazer essa
visão e
eu vou trazer cada vez mais
para você, porque eu já
realmente falo muito sobre a Pythian,
cara, mas a gente já está
no nível de empresa, obviamente
que é muito longe de ser
grande,
mas a gente já tem dor
de cabeça
de empresa que
opera bem. Então
as empresas precisam
automatizar,
as empresas precisam inovar,
as empresas precisam
trazer inteligência. Por quê?
Porque se você for no seu
gerente hoje,
me fala aí, quem conversa
com o gerente que já
escutou, cara, a gente precisa
automatizar ou a gente precisa trazer
uma...
a gente precisa de um projeto
de GNI ou de
inteligência. Já se é cobrado isso
nas empresas, né? O cliente
já quer. Então, assim,
isso aqui que a gente está
vendo hoje no sábado,
passando o dia todo conversando,
não é... não estou trazendo para
vocês uma
visão fora que,
cara, não, isso já está explodindo
no Brasil. Eu digo hoje, né,
não sei se vocês concordam comigo,
mas
antigamente eu via o seguinte, que
em relação
a tecnologia, o Brasil estava há
10, 15 anos
de
defasagem tecnológica.
Hoje, eu particularmente
não vejo mais isso, trabalhando fora
do país e
trabalhando para os clientes dentro do
país. Eu vejo que
os clientes utilizam as mesmas tecnologias,
eu vejo que os clientes possuem
entendimento técnico ali,
né, bem diferente, enfim,
no nível grande, mas, cara, entendem
o que é que deve ser.
Eu vejo hoje
o maior desafio,
eu diria, no Brasil,
a mentalidade. Então, a mentalidade
para mim é a grande questão
hoje, não só a tecnologia. Hoje
é muito
de fato mainstream, né,
graças a Deus a gente consegue
utilizar isso aqui, cara.
Coisas que grandes empresas utilizam, a
gente
consegue ter acesso a isso. Então,
hoje eu vejo muito
mais a mentalidade, por isso que
é importante você
andar com pessoas que estão
realmente em outra mentalidade, para que
você
possa abrir esse escopo e possa
conquistar sua vaga fora do país
e
cara, fazer o que você tem
vontade.
Tem várias coisas aqui, o que
eu quero mostrar
para vocês hoje é como
a gente fez essa POC
e quanto tempo demorou.
Matheus, como foi essa POC? Conta
a história real dessa POC
que a gente fez há 2,
3 meses atrás.
Como que foi?
Por que que essa POC começou?
Com
indignação de quem? O que que
estava acontecendo?
Mostra para eles a visão real.
É legal a gente,
é legal a gente entender que
nós somos de dados,
a gente tem que ter uma
de negócio. A gente não quer
técnico
agora. Pensa assim, esquece o técnico
nesse momento. Preocupa com a dor
de negócio. A gente estava discutindo
justamente, fazendo avaliação das
linhas da formação, do processo do
academy para gerar novos produtos,
para vocês. E isso, o que
aconteceu?
O que isso gerou?
Gerou justamente
a ideia da gente
precisar, a gente está falando sobre
IAI,
a gente realmente ter algo interno
disso.
A gente começou a discutir, ver
o mercado.
Ainda muita coisa
no Brasil em si, ainda muito
estágio
iniciante. Lá fora
os papos são todos voltados
para IAI, como deixar,
como eu comentei. A gente estava
usando ali o chat GPT,
o Claudio falou isso no começo,
para você augmentar
você como profissional. Só que tem
a parte de augmentar a empresa,
que eu vejo muito na parte
de agência. Então a gente começou
a discutir
como é que isso seria. Eu
falei assim, cara,
como é que vai ser? Como
é que a gente
consegue colocar alguém 24 por 7?
E não só, e um problema
também de, eu preciso
de uma pessoa também que atenda
vocês
na ponta comercial,
que tenha um alto conhecimento técnico.
Porque o que a gente gosta
é de debater técnico nas contas.
Então
tem que ter esse viés do
tipo, o produto
que eu estou te oferecendo, o
pessoal no começo
ele consegue fazer os dois. Então
o Luan está discutindo, gente, como
é que vai fazer,
como é que vai alavancar, pelo
menos de negócio
gente, não esquece
técnico, não se preocupe com o
negócio.
Como é que a gente vai
conseguir alavancar a área,
como é que a gente vai
conseguir entregar
ainda mais resultado, quanto a gente
já sabe,
agora a gente tem que entregar
em outras pontas, e como é
que a gente
vai fazer isso. Então a gente
começou a discutir
intensivamente como é que fazer.
O Luan falou assim, então
vamos fazer uma
POC aqui, vamos fazer um
pequeno, dois aqui,
esse chart, esse aí nasceu em
que,
Luan? Sete horas de desenvolvimento?
Dez horas? É, esse foi em
dez horas. Foi o primeiro
que eu me naveguei
em cima do N8n.
E aí o que aconteceu? Eu
estava já estressado
demais, a gente tem tantas pessoas
e eu não vi isso andando,
eu falei, cara, quer saber?
A empresa não implementou, eu vou
tentar
fazer algo aqui. E aí, deixa
eu
mostrar para vocês a primeira versão
do que eu fiz
aqui, e isso traz a ideia
do que é o conceito de
RAG.
Na verdade não é um conceito
de RAG, mas isso traz um
pouco a ideia de como
ele é. Isso vai trazer o
conceito
de Information Retrieval. E a gente
vai entender um pouquinho isso aqui.
Isso aqui
é o que a maioria dos
experts
vendem hoje. Então os
caras montam isso aqui e falam,
porra,
crie um fluxo
motherfucker, escalável
que não sei quê, tal, tal,
tal, tal, tal.
Gente, isso aqui é uma merda,
desculpa, assim,
isso é trabalho
ceboso.
Isso aqui é, você está criando
suas automações,
está criando uma coisa para uma
empresa de uma pessoa
só, que tem um cachorro e
um
macaco, vai funcionar bem?
Agora, cara, se você quer isso
para a vida
real, não é isso aqui que
você vai fazer.
Não estou dizendo que você vai
excluir o
N8n, não. O N8n faz coisas
maravilhosas
no quesito
automação e no
quesito de processo em como você
vai encadear
isso de forma inteligente. Mas
isso aqui não é eficiente, eu
vou mostrar para vocês.
E por que eu acho foda
mostrar isso aqui para vocês?
Que isso vai explodir a cabeça
de vocês
do por que o RAG é
muito foda,
do por que chatbots é muito
foda,
e do que todo esse processo
é disruptivo.
Eu vou mostrar para vocês isso.
E eu espero que vocês tenham
o Eureka
e possam falar o seu primeiro
foda.
Não, Jorge,
aqui aprendi com você.
Infelizmente,
infelizmente, esse é o problema.
Aqui não tem
caô não, velho. Eu ensino,
a galera não gosta, já tivemos
discussão aqui
e no final, assim,
a verdade tem que ser dita.
E realmente é isso.
Mas basicamente, o que você tem
aqui
dentro do N8n, né?
Então você tem automações.
Então o legal daqui é que
você tem
diversas coisas já off the shelf
para você. Então você tem aqui
a parte de
AI, então você tem templates aqui
que já te ajudam você trazer
esse cara.
Então basicamente esse AI Agent
você pode plugar três características
nele. O modelo,
a memória, a gente vai falar
um pouquinho
disso, e as tools.
As tools é a quantidade de
ferramentas
que ele vai ter para poder
se
basear e dar uma resposta em
cima
disso. Então aqui você já consegue
fazer
muita coisa, pode encadear, você pode
fazer sumarização, você pode fazer análise
de sentimento, classificação,
e aí você tem várias coisas
que você pode
usar com isso também.
A gente tem parte de ações
em app,
então cara, o cara tem um
trilhão de integrações
muito legais, vai fazer umas coisas
muito
fantásticas aqui, sabendo fazer.
Transformação,
então existem transformações que você tem.
E daqui, o que a gente
começa
a ver como engenheiro de dados,
tá?
Então para para pensar em um
automation AI, ou assim
por diante, esses caras tem background
diferente
da gente. A gente precisa construir
coisas
que primeiro funcionem fim a fim,
uma
POC é ideal para isso, e
a gente precisa
operacionalizar isso em terabyte scale,
como por exemplo o Jorge falou.
Você vai usar
o N8n para fazer isso? Obviamente
que não, não é feito para
isso.
Mas por exemplo, aqui você tem
um nó em que você pode
puxar tanto um
JavaScript ou um Python em beta.
Então o que a gente já
começa a entender numa
ferramenta de Canva, numa ferramenta de
low -code, no -code, é que
cara,
nós temos certas limitações para o
que
nós, como engenheiros de dados,
precisamos fazer. Não quer dizer que
a ferramenta não é boa, quer
dizer que
para o que a gente faz,
no nível que a gente faz,
no nível de sofisticação que a
gente precisa
fazer, ela não vai servir para
muitos
esquisitos. Mas, o que
que é legal de vocês verem
aqui?
Eu queria testar esse cara, então
o que que eu fiz?
Bem, eu vou trazer esse agente
para cá,
esse agente ele vai ter
um modelo, então
o que que eu fiz? Eu
conectei com a
credencial, peguei a API lá dentro
da OpenAI,
escolhi o meu modelo e falei
que
o método de resposta dele vai
ser o que?
Texto. Ele vai responder
texto para mim. Beleza. Memória.
Eu vim aqui de forma
bem simplória, ele entrega
para você o N8n, um modelo
chamado de Simple Memory,
aonde você fala o seguinte, olha
só, para cada
seção definida, você salva
a memória contextual
da minha conversa com
esse User ID aqui,
até o tamanho de 5.
Então, se eu tiver 300 usuários
aqui, imagina o tamanho do meu
contexto.
Então, você já começa a imaginar,
porque que isso é
fascinante? Porque a gente começa a
entender o seguinte, qual é
a dor? A dor é o
seguinte,
amigão, se você
tem 10 mil usuários entrando
aqui para poder consumir a sua
janela de contexto, isso vai
te ajudar? Porra, não.
E tem gente vendendo isso
como template, tem gente vendendo isso
caro,
mas existem outras formas de
você fazer isso. Então, você pode
adicionar aqui, por exemplo, em Memory,
você pode
trazer o Harris Cache, você pode
trazer algumas coisas legais aqui para
fazer
como cache de memória que vai
te ajudar.
Mas, de novo, um agente precisa
muito mais do que um simples
Simple Memory para poder
operacionalizar. E as
tools, que é a grande questão.
Eu vou dar o contexto para
este
agente, né?
Então, aqui eu tenho várias coisas
que eu posso
fazer. Eu posso estar plugado com
outro workflow, eu posso chamar
um código que está fazendo
whatever, eu posso ter uma requisição,
eu posso ter um MCP, a
gente vai falar um pouquinho sobre
isso
depois. Eu posso ter Vector Database
aqui. Opa! Então, eu já consigo
entender o seguinte, eu posso conectar
esse agente com o Vector
Database. Interessante. Eu tenho várias outras
ações
e integrações de ferramentas que eu
posso fazer.
Calendário, Google,
Spreadsheet,
ClickUp, cara, enfim.
Você pode fazer uma cacetada, tipo,
você
pode pesquisar o dado no Postgres
ali,
se o cara perguntar e for
relacionado a isso
e assim por diante. Quem aqui
nunca
viu uma criação de agente, diga
eu aqui.
Só para eu ter uma ideia.
A gente está criando um agente
aqui, estou mostrando a criação
de um agente para vocês.
Beleza, vocês vão adorar isso aqui.
Para protótipo.
Então, tem várias coisas, beleza? Então,
eu
crio o contexto. Então, o que
eu tenho aqui?
Eu tenho o modelo, eu tenho
a memória, eu tenho o contexto,
o meu information retrieval,
ou seja, de novo,
teoria, tecnologia.
Qual a teoria aqui? Eu estou
dando para o meu
modelo, o que?
Informações para ele
interar em cima de perguntas. Então,
isso aqui é a base de
conhecimento dele,
ou seja, o Reg.
É uma base de conhecimento que
ele tem.
Tá? Mas, porra, isso aqui não
é um vector
database. Isso aqui é uma coisa
tosca
de fazer. Eu conectei cada
spreadsheet
na mão.
Então, por exemplo, eu tenho integração
com o Google Docs
e, por exemplo, eu tenho um
documento
de objeções da formação.
Então, quando alguém chega lá e
fala, cara,
essa formação que vocês
vendem por cinco mil novecentos e
quarenta e...
cinco mil novecentos e oitenta e
sete, ela é muito cara.
Beleza. Então, qual é a possível
entendimento que eu vou passar para
o meu
contexto? Poxa, eu não
tenho tempo para fazer agora. Beleza.
O que que eu vou
passar? Eu estou mostrando para vocês
internamente isso aqui.
Olha que legal, tá? Da formação
que vocês conhecem.
Já tentei algo parecido e não
funcionou.
A gente falou da metodologia. Então,
beleza. Isso foi desenhado como um
FAQ
de objeção, para quebrar a objeção.
Por exemplo, a gente ensina isso
para o time começar.
Quando você contrata um novo time
de comercial,
você bota lá, você vai treinar
essa equipe,
você vai dar um ramp -up
na hora, você vai passar
esses documentos para ele estudar. Beleza.
Só que agora imagina o seguinte,
se a gente tivesse um agente
que pudesse
fazer isso. Então, vamos dar uma
olhada.
Então, eu vou vir aqui, Telegram
Web.
E eu vou acessar esse cara
chamado
Wesky... Cadê você?
Wesky Lumi. Cadê o seu nome?
Aqui.
Beleza.
Deixa eu fazer melhor. Eu vou
deletar esse chat.
Eu vou chegar aqui.
Eu vou clicar nele pela primeira
vez.
E aí ele fala, olha,
Wesky Lumi é seu assistente inteligente
de informação, de dados.
Use Start, o menu para explorar.
Start. Beleza. Oi.
Sou o Lumi, seu agente. Posso
explicar
explicações sobre formação, tal, tal, tal.
Posso vir no menu aqui e
ver algumas
informações legais, objetivos, data sets,
assim por diante. Eu posso perguntar
para ele. Aqui, olha.
Tenho dúvidas.
Tenho
dúvidas
se essa
formação
é para mim.
Pode me ajudar?
Beleza.
Então, quando eu faço isso, o
que ele vai
fazer, na verdade, né?
Ele tem um gatilho,
uma trigger no Instagram,
desculpa, no Telegram,
perdão, que vai iniciar
esse workflow e baseado nas perguntas
se eu não estou iniciando a
conversa
ou se eu não pedi o
menu, ele vai
passar essa pergunta para o meu
agente. E esse agente vai fazer
a pesquisa e o grounding em
cima disso aqui.
Então, ele respondeu, ó.
Se você está na dúvida se
essa formação
é para você, vamos dar uma
olhada no perfil ideal
do aluno.
Se é engenheiro de dados analista
querendo migrar, ou talvez
tal, tal, tal, está cansado de
tutoriais, quer melhor
empregabilidade, se bate algumas dessas,
a formação é feita para te
transformar em autoridade técnica
nas ferramentas, experiência prática real, ou
seja,
para quem quer entrar e crescer,
tal. Se é
engenheiro de dados analista para você,
se ainda tiver dúvidas, posso ajudar
a esclarecer mais pontos
para você decidir sem medo de
dar collect errado
no investimento no seu tempo.
Aí, olha só que foda isso
aqui!
Olha o que ele me mostrou.
Ponte,
Doc, Perfil.
Olha só que legal! Então, olha
só!
Deixa eu fazer uma coisa. Eu
vou desativar
o meu fluxo. Então, quem estiver
fazendo alguma coisa aí,
eu vou desativar o meu fluxo.
Eu vou deixar ele automático.
Peço que ninguém faça nenhuma pergunta
pergunta por gentileza e daí eu
vou fazer a pergunta para ele
ó alguém já executou uma
pergunta e a gente vai ver
o que que ele vai rodar
não tá esperando aqui ó então
fala o seguinte é
e quais são e quais são
os principais diferenciais da formação Olha
só que legal agora quando eu
der
essa enter ele vai entregar o
pipeline ele vai no modelo vai
buscar se tem memória anterior a
essa conversa e ele vai me
retornar essa informação aqui os principais
diferenciais
da formação da tablix que fazem
ela se destacar no mercado são
E aí coloca essas informações
e agora vou fazer o seguinte
e quem são é E aí
o que que eu tô utilizando
o simple memory ele já tá
na memória que eu vou
fazer eu vou perguntar mas o
que eu quero mostrar aqui deixa
eu ver se eu consigo mostrar
quem são
e quem são os instrutores essa
formação
Oi e aí quando eu executar
esse aqui o que aí ó
olha onde ele foi vocês viram
que legal o
as que nome veio aqui veio
no chat recebeu a pergunta verificou
se tem alguma coisa na memória
depois ele foi no doc instrutores
para me retornar o que a
formação conta com um time de
história de
respeito que realmente vivem respira engenharia
lua moreno Mateus Vitor essa turma
não só conhece
o backstage da engenharia de dados
mas também sabe como passar conhecimento
da prática sem
enrolação perfeito para quem que é
virar referência de verdade doc instrutores
tá vendo Então isso é uma
base de conhecimento só com a
base de conhecimento o cara utilizando
tô quem se lendo ela em
plaintech
text e assim por diante, tá?
Então, qual é a maravilha disso?
Como que eu fiz isso?
Bem, se eu vier aqui no
Ask Lume,
ele me mostra algumas coisas, tá?
Então, eu tenho o conceito de
Prompt Message
e de System Message.
Então, qual é o meu conceito
de Prompt?
Aí, aqui, a gente está falando
o quê?
De Prompt Engineering.
Olha que coisa legal.
Então, ó, você é o Ask
Lume,
o assistente inteligente de formação,
sua missão é responder perguntas de
forma assertiva,
nova técnica, toque de humor leve,
sempre constando os documentos conectados ao
fluxo,
especialmente quando a dúvida fala sobre
módulos, cargo horário, data set, DNA
e tal.
Documentos disponíveis.
Estou passando para ele aqui a
instrução
de quais são os documentos
e o que significa para cada
um deles.
Quais são as regras obrigatórias?
Sempre que possível, consulte os documentos.
Referências à fonte final com fonte.
Nunca comece a resposta com pergunta
ou resposta.
Use negrito e tálico para passar
blocos.
Não adicione links externos.
Estilo.
Aí, aqui eu estou dando o
tom para ele.
A gente vai falar um pouquinho
de tom, tá?
Isso aqui é bem legal.
Tom confiante, técnico, amigável.
Pode usar uma analogia de dados,
humor leve,
como não precisa dar collect no
conhecimento todo.
Fale com quem já virou noite
debugando um cluster
e viu muitos alunos brilharem.
Aqui eu estou dando few short
examples.
Pode.
Pode causar confusão, sim, tá?
Então, se eu fizer isso em
inglês e pedir a tradução,
porque o peso...
Agora a gente entendeu.
A gente entende o quê?
Que o peso semântico da palavra
pode ser colocado em lugares diferentes,
entendeu, Gabriel?
Por causa do embedding.
Então, o significado dele traduzir e
trazer a pesquisa semântica
pode mudar no estilo da linguagem.
Então, por isso que se você
instruiu todo o processo dele,
você vai vetorizar ele dessa forma
e tentar diminuir o contexto desses
caras, tá?
Agora, vou te falar uma coisa
legal.
E os vetores ajudam a diminuir
esse contexto?
Porra, é isso que eu quero
que vocês entendam.
É as eurecas.
É o foda.
Vocês estão entendendo que aqui o
cara está indo consumindo o token
porque eu tenho que ir lá,
ler esses documentos, trago o token.
Mando no contexto, token.
Faço a query, token.
Tudo é token.
Então, isso não é um modelo
escalável porque a gente está exaurindo
tokens.
A gente precisa reduzir e ter
técnicas para isso.
É por isso o reg, cara.
Tá?
Então, aqui, ó.
Seja direto simpático.
E aí a mensagem do usuário
é essa o quê?
É o jasonclean lá que eu
recebi.
Quem é o jasonclean?
Quem são os instrutores dessa mensagem?
Quem é essa informação?
Então, eu passo o contexto e
passo o user message.
Esse é o user message.
E o system message são características
que você pode passar para ele
de comportamentos específicos que vai mandar
no contexto.
Então, antes do system, do user
message ser enviado,
eu vou mandar um system message
de contexto.
Então, você vai lá no SQLume.
Você tem os documentos e tal.
Porque esse é o contexto que
ele sempre vai mandar.
Por quê?
Porque todas as perguntas e respostas
são o quê?
Efêmeras.
Elas morrem.
Ou seja, por que que é
difícil você criar um sistema de
chatbot escalável e inteligente?
Porque você precisa de memória.
É porque no chat EPT você
conversa eternamente, né?
Porque os caras lidam com isso
de uma outra forma.
Eles têm memória de pequeno prazo,
de médio prazo e de curto
prazo.
Eles nunca esquecem nada que você
fala.
O cloud já lida diferente.
Ele estoura tokens e tem que
começar uma nova conversa.
Por quê?
Porque a janela é de contexto.
Então, aqui, toda vez eu tenho
que passar o meu contexto.
Então, qual é o meu contexto?
É o System Message.
Tá?
Então, vocês estão vendo na prática
isso.
Beleza?
E daí eu vou responder as
perguntas.
Então, ele vai entender, ele vai
rotear aqui, ó.
O que que ele fez?
Ele roteou essa conversa pra quê?
Pra eu conseguir perguntar doc -instrutores,
ó.
E aí, o que que ele
fez?
Ele consultou um documento ID.
O conteúdo desse documento, ó lá.
Ele trouxe inteiro pro contexto.
Isso é caro.
Eu tô gastando token.
Tá?
Então, o que a gente começa
a entender é que eu preciso
encontrar formas inteligentes
pra poder recuperar essas informações aqui
e trabalhar com elas em grande
escala.
Beleza?
Mas, eu tenho um modelo que
funciona muito bem.
Tá?
Que responde muito bem.
Então, inclusive, o Sandro até colocou
ali um exemplo, né, que ele
respondeu.
É, posso começar a formação, mas
se no meio eu cair em
um abismo imoral, eu vou
morrer.
Rir na cara do abismo é
um requisito básico pra qualquer engenheiro
de dados que vira
madrugada devolvendo pipeline.
Brincadeiras à parte, a formação é
estudada pra não ser QA, mas
caso você se sinta atolado,
tem suporte, comunidade, se o medo
do abismo é falta de tempo,
quantas que dá de segredo,
é tanto pra começar tranquilo sabendo
que se der ruim, tem time
pra socorrer e te ajudar
a levantar, sacudir a poeira e
seguir em frente, fazendo aquela mágica
no cluster.
Se joga o abismo, pode até
vir, mas aqui ninguém fica caído
no chão.
Foda, né?
Então assim, cara, isso aqui, esse
nível de sofisticação, eu fiz em
10 horas, caralho.
Vocês não acham isso foda, não?
Cara, pra mim, isso é uma
parada disruptiva.
Assim, isso pra mim é uma
parada assim, ó, eu digo como,
vamos lá, como empresário,
eu paguei 26 mil reais pra
uma empresa que não fez nem
metade disso.
É, pra mim, como engenheiro de
dados, isso pra mim é uma
parada fantástica de implementação,
fácil, com POC, com resultado fim
a fim, onde o usuário pode
testar e analisar e falar,
cara, legal.
Eu vou dar essa prova de
conceito.
Curti, hein?
Tem futuro, muito foda.
E, cara, aqui a gente pode
trabalhar tom, aqui a gente pode
trazer tanta coisa que eu
tô mostrando só pra vocês, só
o esqueleto do que tá fazendo.
Vocês tão vendo além?
Quem achou foda aí, fala aí
pra mim, se realmente a visão
vocês tão pegando.
Resultado rápido, né, mano?
É.
Gostaram?
Conseguiram ver?
Poucas pessoas não conseguiram ainda, não?
Qual o custo?
Eu pago 150 reais de...
O N8n, ele é open source,
tá?
Então, você pode baixar ele pra
dentro do seu Docker e assim
por diante, enfim.
Aqui eu tô pagando...
Eu paguei 2 dólares de chat
EPT, paguei 150 reais de negócio.
Acho que é isso, né, Matheus?
150 reais da instância e 2
dólares de chat EPT.
Isso.
De novo, mas é isso que
a gente vai fazer?
Não.
Isso aqui é uma merda.
Isso aqui não funciona em grande
escala.
É.
Não, Jorge, por favor, se você
viesse aqui depois de 15 anos
comigo e você viesse entregando
isso, você fala, caralho, eu ensinei
esse bicho a fazer isso?
Relaxa, eu vou te deixar com
orgulho, fica tranquilo, tá?
Mas isso aqui é o que
a galera vende.
Então, assim, não tem problema nenhum
você vender isso.
Mas entenda aonde se encaixa, o
que faz e assim por diante.
Beleza?
Mas pra mim, na minha visão
como empresário, validar isso aqui, ver
que isso aqui é uma
coisa muito foda pra gente, tá
validado e testado.
Agora a gente vai pro próximo
passo, né?
Matheus, vou dar uma parada aqui
e vou abrir perguntas porque teve
muita gente que perguntou.
Eu acho melhor vocês levantarem a
mão e perguntar e eu vou
pedir só o seguinte.
Seja direto, porque a gente tem
222 pessoas aqui, então eu vou
pegar 15 minutinhos pra
gente parar e perguntar.
Pergunta rápida, não vou delongar muito,
mas levanta a mão pra você
não perder nada
que você quer.
Vai lá, Montragon.
Vamos lá, gente.
Rápido, isso sim.
Não precisa dar bom dia, não
precisa falar nada, só pergunta.
Pode falar.
Opa, desculpa, tava no mundo aqui.
A galera vai sair fritando a
cabeça aí.
Daí eu queria sugerir umas ferramentas
pra galera brincar com RAG aí,
que é o Streamlit
e o Hugging Face.
Tem também o Lhama, né?
Pra galera poder brincar com isso
aí depois do curso.
Funciona bem pra caramba pra poder
testar, fazer RAG, né?
Juntamente com o Python, com o
Langchain aí.
O Streamlit eu usei no meu
TCC e funciona muito bem pra
galera brincar aí depois do curso
e fritar mais o cérebro aí.
Boa, boa, boa dica.
Marcos.
Opa, tudo bom?
Luan, esses JSONs aí de exemplo,
vai estar disponível?
Algum outro exemplo?
E por que que vocês preferiram...
Telegram ao invés do WhatsApp?
Diz que ele é muito mais
usado pelos usuários.
Eu particularmente criei um já com
Webhook usando WhatsApp, né?
Com Evolution, né?
Aquela API lá do Evolution e
funciona muito bem.
É a primeira vez que eu
vejo aqui com Telegram no workflow
de vocês.
Mas eu queria saber se é
só pra fins didáticos que vocês
usaram também.
Ou tem alguma outra...
Porque assim, eu acho que é
muito mais difícil você convencer o
teu cliente.
Ó, usa Telegram, usa Telegram.
Usa todo mundo, já que todo
mundo...
WhatsApp é praticamente...
É...
Se não é com anô, né?
Todos os celulares aí.
Então, assim, eu só queria saber
o porquê, né?
Do Telegram e se o JSON
aí vai estar disponível.
Valeu.
Tá muito bom.
Tá muito bom.
Valeu.
Esse JSON não vai estar disponível.
Mas pode ficar tranquilo que eu
vou mostrar, além disso, muito mais
coisas, né?
Onde é que vocês vão fazer,
o que eu recomendo.
E a ideia do Telegram é
justamente só por fins educativos mesmo.
Pela configuração como um todo, ela
demora três minutos no WhatsApp.
E você tem vários hops que
você vai fazer nível enterprise
pra conseguir botar isso nível business.
Por isso que a gente fez
nível educativo aqui pro Tim testar.
Mas eventualmente, quando isso entrar em
mainstream, não vai estar em Telegram,
porque, de novo, não é uma
ferramenta que todo mundo usa.
É, Lucas?
É o Jonathan.
Jonathan primeiro, peraí.
Deixa eu liberar ele, tá?
Jonathan, pode ir.
Fala, pessoal.
Tudo bem?
Parabéns aí, Luan e Matheus.
Bom, brevemente aí, queria entender o
seguinte.
Ali, né?
Dentro do N8N, você colocou o
contexto dentro do próprio N8N, né?
Daquela caixinha ali.
É possível...
A resposta provavelmente é, né?
Não sei se é possível.
Não sei se a gente vai
conseguir explorar isso dentro do curso.
Como é que tá isso com
uma conta que a gente já
tem de um LLM e já
trazer o contexto,
por exemplo, de um projeto que
a gente criou ou de um
agente que a gente já vem
dando contexto
pra ele e jogar ele aqui
dentro pra não ter que trazer
tudo pra cá, sabe?
Só pra entender se a gente
vai conseguir explorar isso.
Depende de onde tá essa base.
Quando você diz o conceito, você
tá falando onde essa base tá
do agente, o que você
tá se referindo?
Onde essa base tá?
Pode estar no banco de dados
ou pode estar gravado dentro de
um próprio projeto da OpenAI,
por exemplo.
Eu já venho alimentando um agente
lá, então eu trago esse contexto
pra cá sem ter que
escrever aqui.
Pode.
Tranquilo.
Pode.
A gente vai conseguir explorar isso
aqui ou...
Não, porque a gente não vai
usar N8N, mas eu vou falar
disso.
N8N aqui é só pra vocês
verem e tal.
A gente vai navegar num nível
muito mais profundo do que isso,
mas dá pra fazer, Jonas.
Tá?
Dá pra fazer.
Maravilha.
Maravilha.
Obrigado.
Vai lá, Matheus.
Acelera.
Próximo.
Lucas.
Opa.
Jonas, uma pergunta rápida.
A documentação, a formatação, ela interfere
alguma coisa no modelo?
Por exemplo, se tem que ser
uma formatação padrão e se eu
mudar isso o modelo vai ter
dificuldade em entender essa base de
dados?
Formatação do que especificamente?
Do prompt?
Do documento.
Do documento, isso.
Do doc.
Object.
Do doc.
Ah, tem.
Pra caralho.
Pra caralho.
Muito boa pergunta.
Tem.
Eu vou...
A gente vai começar sobre isso.
Tem.
Muita.
Muita diferença, tá?
Bastante.
Rafael.
Opa.
Primeiro, Luan, muito legal o treinamento.
Aqui você deu um case bem
legal do N8N, que é mais
uma prova de conceito pra cliente
externo, uma empresa de cliente externo
que não é escalável, então mais
prova de conceito.
Mas você tem algum case que
seja produtivo pensando em cliente interno,
tipo, dentro da
empresa?
Claro.
Vou mostrar hoje.
Não, não o N8N.
Mas eu vou mostrar pra você
caso enterprise fora do N8N.
E vou te falar também como
você pode usar o N8N em
casos enterprise.
Era essa a minha dúvida, cases
internos mesmo pra botar em produção,
mesmo que seja na
empresa que eles têm, sei lá,
dois, três clientes que eles podem
usar.
Boa.
Tem, tem sim.
Por exemplo, rapidamente pra responder você
aqui, obviamente que você vai ter
que testar
e entender o nível de capacidade
de throughput que você vai entregar,
mas o que você pode
fazer, em vez de fazer esse
processo offline, né, de treinamento de
REG, imagina que em
vez desses documentos eu tenho um
vector database, então você vai alimentar
o ESC, vai tunar
e vai melhorar todo o ciclo
pra poder otimizar a quantidade de
acessos que você tem.
Isso vai poder ser entregue pra
você no nível mais enterprise, mas
nível escala e aí já
não vai ser a ferramenta que
você vai usar pra você operacionalizar
isso, dependendo
do volume.
Então, assim, tudo tem que vai
ser medido.
Ah, Luan, o que que você
vai usar em vez disso?
Cara, você vai ter Leng Flow,
Leng Chain, você vai ter DeFi,
você vai ter algumas
coisas mais específicas pra gente que
vão performar melhor, com características
melhores,
tá?
Mas aí a gente explora um
pouquinho daqui a pouco.
Bom dia, Rafael.
Eu acabei de perguntar, obrigado.
Ah, tá, deixa eu tirar um
pouquinho.
Bruno.
Bruno?
Pessoal, já prepara tirar o áudio,
Desculpa, eu não tirei, eu achei
que tivesse tirado, desculpa.
Vai lá.
Naquela arquitetura que você mostrou, o
Embedded Model, ali o que você
quer dizer é que por
debaixo dos panos você escolhe...
Ah, tá.
A DNA
que você vai usar,
né?
O ChatGPT ou o Cloud, é
isso?
Pelo que eu entendi, ou eu
entendi errado?
É, você escolhe o modelo.
Por exemplo, se você for usar
esse Text Embedding ADA, ele é
do ChatGPT.
Se você for usar o Embedding
English Tower, ele é do Curriri.
Aí tem vários que você pode
escolher.
Eu vou mostrar.
Entendi.
Tá bom, tá bom.
Obrigado.
De nada.
É melhor.
Bom dia, bom dia.
É, a minha dúvida sincera é,
tem uma dúvida que eu não
Eu quero tentar entender como que
eu poderia fazer desse processo todo,
ou com o N8n,
ou com outras plataformas, converter isso
para um projeto SaaS, onde eu
pudesse ter
rentabilidade, da mesma forma que eles
têm com token, a gente poderia
ter um nível de
rentabilidade com o chat.
Como que eu converteria isso em
um projeto SaaS?
Isso é complexo, porque você vai
ter que ter...
Você vai ter vários trade -offs
de custo, né?
Primeiro, como você coloca isso, qual
a plataforma que você vai escolher.
Obviamente que quanto mais baixo no
nível de implementação, mais barato vai
ficar
em relação a custo, valor, mas
mais difícil o nível técnico.
Então, o que eu estou falando
para isso?
Hoje você pode botar dentro de
uma Railway, você pode botar dentro
de um Elastio, você
pode botar dentro de serviços extremamente
gerenciados, que não precisa se preocupar
com a infra, mas você vai
pagar mais caro.
Ou você pode optar, por exemplo,
ter um Kubernetes e fazer o
deployment de tudo isso, você vai
ter um controle muito maior.
E um custo alto também, em
relação a staff.
Modelo.
Cara, modelo, aí já é um
grande problema, aí essa é a
discussão do momento, né?
Ah, porra, eu vou treinar um
modelo, vou fazer um fine -tuning
no modelo, eu vou simplesmente
pegar um modelo, eu vou pegar
uma máquina e vou botar lá
dentro esse modelo, vou usar
o DeepSeq para fazer isso.
Na teoria é muito bonito, na
prática, eu não sei se as
empresas seguem esse modelo.
No modelo SaaS, o que você
pode fazer é entender os serviços.
É o seguinte, ir lá no
Hugging Face, ver quais são os
modelos, os trilhões de modelos
que você tem, e você vai
ter que fazer prompt tuning, você
vai ter que, cara, gastar muito
mais tempo tentando refinar, e você
vai ter que ter máquina para
fazer isso e medir custo.
O que eu faria numa plataforma
SaaS, tá?
Eu abstraria, eu começaria de cima
para baixo, eu começaria com resultado
e otimização
em vez de otimização e resultado.
Primeiro, eu quero saber se eu
rentabilizo ou se eu, pelo menos,
tenho um cliente, depois
eu começo a pensar quais são
os hops que eu vou botar.
Para mim, a otimização vem depois
desse caso, porque o mundo é
muito acelerado e
você quer saber, você quer entregar
valor.
Então eu chamaria uma OpenAI mesmo,
eu tentaria otimizar os meus tokens,
eu tentaria fazer
muita coisa de otimização aqui, por
exemplo, criar um reg, por exemplo,
fazer um prompt
tuning, por exemplo, utilizar modelos mais
leves que consomem menos e que
tem uma boa
inteligência, fazer balanço, por exemplo, um
Gemini 1 .0, 1 .5, que
é bom para certas
coisas.
Ou um modelo mini que vai
ser muito mais barato por token.
Então tem outros lados que você
pode economizar sem ter que pensar
em fazer sua infraestrutura
do zero.
Eu optaria por esse lado primeiro
e aí eu ia otimizando.
Gabriel.
Pode, Gabriel.
Opa, é porque já ativou o
som.
Cara, minha pergunta é bem direta
aqui para a gente ir com
mais tempo.
O processo sempre ele é unilateral,
ou seja, é possível com os
prompts dos usuários lá
na ponta ser retroalimentado o database
feitor, depois de ser treinado e
tudo mais?
Ou seja, quer saber que se
eu posso realmente voltar, que nem
acontece, né, às vezes a
gente acaba respondendo a lucha GPT,
qualquer outra LLM, ele vai aprendendo
individualmente
com o usuário e sendo alimentado,
é possível fazer isso?
É, mas isso é complexo pra
cacete.
Isso é muito complexo de fazer.
Dá para fazer?
Dá.
Mas é um nível de sofisticação.
Nível V10.
Nível V10 da sua, do seu
processo.
Ele não é o V1, V2.
Tem outras preocupações que você vai
ter antes de começar a pensar
em como você vai
retroalimentar, como você vai fazer fine
-tuning, como que você vai dar
tom, como que você
vai dar personalidade específica.
São processos que você vai depois
disso, tá?
Pode seguir.
Mas tem como fazer sim, obviamente.
Perfeito.
Gabriel, pode ir.
O Luan, como que, qual que
seria a melhor técnica?
para realizar a consulta no meu
banco de dados a nível
granular?
Um exemplo, perguntar qual que é
o preço do produto que está
lá na tabela.
Por exemplo, é RAG, é MCP,
qual que seria o mais indicado
aí no caso?
O RAG é uma ferramenta, o
RAG ele é um processo, o
MCP é uma ferramenta, na verdade
aí você
vai entender qual o tipo de
barra.
Qual é o banco que você
está, primeiro.
Se for relacional, você vai ter
a query sobre index textual que
vai utilizar a search, BM25,
ou IDF, ou assim por diante.
Se você for para um vector
database, você vai usar pergunta semântica
ou modelo híbrido,
a gente vai ver isso.
Então aí você vai entender como
que esse cara pesquisa.
Obviamente que se for textual, vector
database, tá?
Ou qualquer banco que te possibilite
vector, SQL possibilita vector, Databricks hoje
tem
vector.
PG, vector, tem vector.
Qualquer banco que possui vector, possui
vector, é vector.
Você vai usar um vector, um
inverted index ou alguma coisa do
tipo para que você possa
fazer pesquisa semântica aí em cima.
A fonte original ali é o
LTP lá da base do cliente,
entendeu?
Conectado direto nele, aí eu tenho
que extrair essa informação dali, vetorizar
e daí fazer
a consulta.
Não, não.
Você pode fazer direto, só que
aí vai ter de novo, os
trade -offs, de novo, vai ser
uma query batendo no banco.
Aí tem várias preocupações que você
vai ter que ter, mas tem
como fazer, text e o
SQL.
Entendi.
Ele vai gerar a query com
base na linguagem natural que eu
passei para ele, que eu perguntei.
Isso, exatamente.
Entendi.
Beleza.
Obrigado.
Bora.
Tá aí então, porque acho que
mudou a...
Mudou a...
Mudou a...
Eu só queria
saber como é
que está o mercado afora aí,
a utilização dessas ferramentas
de automação.
Você está tendo uma adoção forte
lá fora ou outras ferramentas, se
eu puder falar aí
como é que está essa parte
de consultoria?
Boa.
Adoção forte para N8n, mas nichado
em automações mais leves.
Para nível engenharia, para o que
a gente tem que fazer em
nível de sofisticação,
não.
Aí é Langchain, aí é Python,
muito Python, muito Langflow, que para
mim é a mesma coisa,
basicamente.
Mas está mais no espectro de
Python do que no espectro de
uma ferramenta.
A gente tem algumas outras, por
exemplo, DeFi funciona muito bem, a
gente tem outras ferramentas
também de gerenciamento que funcionam muito
bem, mas no espectro de automação
a líder
é o N8n.
A automação, tá?
Se eu pensar de automação.
Então tem muito.
Por exemplo, se você for hoje
na internet buscar AI Automation Engineer,
cara, tem muita
vaga para fazer automação, mas muito,
não é pouca não, tá?
Will, pode ir.
Bom dia.
A minha pergunta, Luan, é assim,
eu gostei muito da ideia de
fazer a base de conhecimento,
né?
E eu queria saber se dá
para usar o N8n em uma
interface web sem precisar usar mecanismo
de mensageria, WhatsApp, Telegram, essas
coisas.
Tem como você testar ele aqui
dentro, sim.
O chat, você tem o chat
e o outro chat, então você
consegue interagir dentro dele aqui
no teste, sem problema nenhum.
Ed, pode ir.
E aí, bom dia, Luan.
Parabéns aí pelo conteúdo, cara, sensacional.
Na verdade, eu tenho uma dúvida.
Eu queria ouvir a sua experiência
dos projetos que você tem participado
fora.
Eu tenho visto pouco se falar
de observabilidade e teste em cima
de todo esse processo, para
garantir, de fato, a continuidade da
acurácia das entregas ali dos modelos.
O que você tem visto?
A gente vai ver alguma coisa
aqui no decorrer do treinamento.
O que você tem a me
dizer sobre esses dois temas, cara?
Tem de dizer que é, de
novo, negligenciado como tudo de governância,
tudo de tudo como
sempre, mas quando você está indo
para fora do país, o nível
de sofisticação é outro.
Então, esses tipos de coisas são
implementados, sim, porque você precisa saber
o que esse
cara está fazendo.
Se o TopKey está retornando, se
você precisa fazer re -ranking, se
você precisa adicionar,
por exemplo, um método como o
Ragas.
Então, tem como você fazer muita
coisa com o Ragas, é uma
biblioteca muito interessante,
que dá para fazer validação.
Para você saber qualidade, fim a
fim, do seu sistema de Rag.
Então, lá fora, sim, mas não
é a preocupação inicial do projeto,
mas eventualmente, sim,
tem.
Eu acredito que no Brasil vai
ser negligenciado novamente.
A galera coloca os documentos, faz
query e vai falar mal, porque
não sabe como funcionar.
E você vai ser o cara
que sabe e ter um diferencial
e vai criar um sistema fodido
de bom, como
tudo.
Vendo do mesmo jeito, nada mudou.
Na verdade, para mim, hoje...
Na minha humilde opinião, quem estudar
fundamento, quem souber realmente o que
está fazendo,
quem se especializar, vai nadar...
Nossa, vai ganhar muito dinheiro, cada
vez mais, porque hoje o mercado
está cada vez
pior.
Então, assim, minha opinião.
E observabilidade?
Você tem alguma ferramenta, algo para
poder indicar que você tem visto?
Cara, eu uso o LangFuse.
Tudo que eu posso com o
LangFuse.
Eu vou mostrar, inclusive, o LangFuse
aqui.
Boa, boa.
Brigadão aí, tá?
Brigadão mesmo.
De nada.
Falou.
Bom dia, pessoal.
Olá.
Me surgiu uma dúvida quando você
estava falando sobre embedding, porque o
que acontece?
Para você criar a base de
documentos ali para o RAG, você
precisa fazer o embed dos documentos.
Só que uma boa prática, quando
você está construindo um agente, é
você ter ali um
modelo de fallback.
Se aquele modelo falhou, você tem
um outro modelo ali para responder.
A minha dúvida é, quando eu
estou criando a minha base de
documentos, eu preciso me
preocupar com o modelo de embedding
adotado para que ele seja compatível
com os dois
modelos que eu vou estar utilizando,
o principal e o fallback?
Perfeito.
Aí você está confundindo só um
pedaço, que eu vou te esclarecer
que eu tinha essa
mesma dúvida.
A gente vai ver aqui no
treinamento.
Quando você embeda um documento, você
vai usar um embedding específico.
Vamos supor que você pegou o
embedding da OpenAI 02, beleza?
Você embedou esse documento.
Ótimo.
Aí, o seu modelo de fallback,
ele é o cara que vai
consultar.
Então, vamos supor que o primeiro
modelo tentou consultar, deu algum problema,
você
tem um fallback.
Esses dois caras, quando eles forem
consultar o seu vector, antes dele
acessar esse vector,
ele vai ter que embedar também.
Ele vai ter que fazer uma
query, ele vai ter que embedar.
Então, na hora que ele embedar,
ele obrigatoriamente vai ter que embedar
com o mesmo modelo que
foi embedado anteriormente.
Só que o modelo embedado, ele
não é locado para, por exemplo,
se você usou um open,
um query, que é open source.
Você vai usar para os dois,
não tem problema nenhum.
Então, o que você tem que
entender é, na hora que você
tiver um modelo de fallback,
esse modelo de fallback pode acessar
o modelo de embedding que você
vai fazer, em 99 % dos
casos vai, mas ele tem que
ser o mesmo.
E se não, o 3D dimensional
de como ele foi gerado vai
estar diferente, você vai ter
problema de similaridade, você vai ter
problema de ranking e por aí
vai.
Então, o próximo, Matheus.
Bom dia, pessoal.
Fala, Luan.
Massa demais o treinamento.
Cara, eu tenho duas perguntas.
Eu sei que talvez não seja
o momento, mas tipo, quando a
gente vive dentro do universo
da Databricks, a gente vê o
MLflow trazendo alguns processos de qualidade
de vida.
Eu não estou vendo o pessoal
usar ainda, porque eu acho que
quem está usando já está
usando e está quieto.
Então...
Eu queria saber, tipo, não sei
se aqui a gente tem, mas
pensando em produtividade,
esse cara, dentro do Databricks, ele
encaixa como um todo, como uma
luva, né?
E outra dúvida também, que eu
sei que hoje em dia a
gente fala muito de chat, inclusive
a própria Databricks está com, tipo,
eu pergunto essas coisas para eles
e parece que está
todo mundo só nesse primeiro rush
para entregar isso bem feito antes
de partir para outras
coisas.
Mas eu queria ouvir se no
backstage, aí fora, você viu projetos
de GNI também voltados
para a engenharia, igual ele falou
de observabilidade, mas, tipo, infraestrutura,
por
exemplo, o Spark
ali dentro do Databricks já tem
uma interação muito boa com o
próprio Delta, a própria
inteligência deles.
Eu sei que o GNI, a
GNI já traz alguma coisa assim,
mas eu fico pensando em, tipo,
algo
um pouco mais parrudo para, tipo,
ajudar um analista de dados a
não explodir o driver,
sabe?
Tipo, você ouviu algo assim, não
precisa ser só o Databricks, tá?
Boa!
Não chegamos nesse estágio ainda, porque
eu acho que esse é o
estágio de agentes,
de agente key.
E eu acho que a gente
não chegou ainda como humanidade ainda
aí, tipo, mainstream.
Eu acho que a gente está
entrando LLM mainstream para a maioria
das empresas, então daqui
três, quatro, cinco anos a gente
vai ter, tipo, LLM para tudo
quanto é canto, mas agente
key já é algo mais cauteloso
que, por exemplo, tem coisas nichadas
acontecendo.
Eu não vejo, pelo menos projetos
que eu pego de cliente, e
agora cada vez mais, porque
eu estou muito no time de
Solutions Architecting.
Então, eu estou fazendo, eu estou
fazendo EAD, eu estou fazendo reunião
com cliente,
basicamente, dois, três clientes semanalmente
sobre
GNI, que é a minha especialidade
lá
na PIFA.
Então, a gente conversa sobre esses
projetos.
Eu não vi nada ainda específico
sobre isso.
Eu vejo mais esses casos que
eu te falei, cara, chat, com
certeza, extração de documento,
com certeza, utilização para suportar times,
automatizar certas coisas, eu vejo muito,
mas ainda não cheguei nesse ponto
de, cara, criar algo que vai,
de fato, desbloquear ou
trabalhar pé -a -pé com o
engenheiro de dados.
Não estou dizendo que não existe,
mas eu ainda não acho que
é um negócio que está
pegando ainda.
Entendeu?
Ainda não é o Hot in
the Press, não.
E a outra pergunta, qual foi?
Você me fez uma outra pergunta.
Qual foi?
MLflow.
É, MLflow.
De, tipo, você acha que ele
encaixa bem na maioria dos casos
também, mesmo fora da
tecnologia?
Até o Linux, até o Ultabricks,
porque isso é algo que eu
não vi muito, eu acho que
deve
ter sido aquilo que eu falei.
Eu não.
Não, não vejo.
Engraçado que, tipo, é open source,
é muito usado, mas, quando eu
pergunto para a galera
o que é que elas usam,
muita gente às vezes nem usa,
tem framework em Python, enfim, e
eu não vejo tipo uma adoção
massiva de todo lugar que eu
vou, o cara fala que usa
MLflow.
Cara, até os cientistas de dados
que trabalham na Piffin comigo, os
caras não falam muito
de MLflow.
Então, não vejo uma utilização, tipo,
abrupta não.
Próximo
Opa,
tudo bem, Luan?
Tudo
Mais um workshop foda aí
Na prateleira dos workshops foda
Minha pergunta é um pouquinho mais
Business, assim
Quando tu vai iniciar um projeto
Com um cliente
Como é que tu faz pra
tu estimar
O produto
Pra estimar
O quanto de custo a empresa
terá
Porque um dos desafios que eu
tenho
Interno aqui na empresa
É às vezes estimar um projeto
de
AI
Com infraestrutura, tokens
Como é que tu faz pra
estimar
Esses custos?
Cara, eu acho que
Eu nunca cheguei nesse nível ainda
Fora de chegar nesse nível
De estimação
De estimativa
Porque
Isso é feito
Em uma outra camada
Do que a minha
Mas o que os clientes querem
ver
Cara, em 70 %
80 % é
Se eu consigo fazer algo fim
a fim
Então eu diria que
Uma das coisas importantes é
Entender, eu acho que pra cada
cliente
Você vai ter que entender especificamente
o cenário dele
Então não existe de novo uma
size fit só
Não existe uma tecnologia só
Eu vou mostrar várias aqui hoje
Mas o que eu acho que
é importante você fazer
É você
Entender o que o seu cliente
quer
E como você pode entregar isso
E se o custo é uma
barreira
O que eu acredito que não
seja
Muita gente fala de custo
Muita gente fala de custo hoje
de NIA
Mas eu quero sentar com uma
pessoa aqui
Que vai me explicar
E falar, cara, eu tentei tudo
o que tem
Gerenciado e não funciona
E aí eu quero entender
O caso dessa pessoa, porque até
hoje eu não entendi
Eu vejo muita gente falando, mas
eu não vejo
O que de fato eu não
consigo
Trazer a visão ainda, o que
não funciona
Então eu não estou
Falando de produção, quando você vai
para a produção
O game é outro, obviamente
Assim como também
A engenharia de dados
Mas eu acho que essa estimação
De você falar Tolkien
Estimar Tolkien
Na minha humilde opinião, ela é
meio difícil
De você conseguir, porque assim
Cara, qual prompt você está injetando?
Você vai fazer prompt hacking? Você
vai fazer prompt tuning?
Beleza, você fez necessário?
Você já testou os modelos?
Qual modelo que menos economiza?
Comparação, por exemplo, eu já vi
gente fazendo
Coisa que é tipo assim, o
cara pega
Uma parada idiota de chat e
o cara
Mete o 4O
Cara, vai ser muito caro, porque
o cara colocou
O 4O, mas cara, você vai
no Hugging Face
Tem vários modelos interessantes
Você tem LLM API, que tem
várias outras coisas
Você tem
Together AI, que faz toda
A transparência de vários modelos
Que você pode usar, então assim,
tem tanta coisa
Que dá para testar para reduzir
esse negócio
De Tolkien, tem como você fazer
chunking
Melhor para reduzir, tem como
Você adicionar embeds diferentes para
Reduzir, você tem 20 técnicas
Diferentes de chunking para fazer
Você tem na hora do retrieval
Você pode fazer, cara
Várias, você pode injetar um agente
Para poder consultar melhor
Então assim, eu acho que
Eu acho que falta profundidade
De conhecimento, na minha opinião
Antes das pessoas falarem economia de
Tolkien
Eu entro nas reuniões e eu
fico vendo, cara, eu quero economizar
Tolkien
Tá, explica o seu cenário
Qual é o seu cenário?
Sabe, tipo, muita gente fala de
custo
Mas eu não consigo ver
O cara ter exaurido
Todas as possibilidades
Que tem para falar, é, realmente
Eu preciso de um small language
model
Aqui, eu preciso realmente
Botar esse cara numa máquina
Eu preciso comprar uma GPU
E treinar esse cara, cara, desculpa
Eu acho, eu acho que
Para chegar até ali
Você tem que tirar todas as
opções
Do seu caminho, e eu tenho
certeza absoluta
Que essa pessoa que está fazendo
isso, ela não tirou
Tá, então assim, você tirar tudo
Depois você me explicar tudo que
você tirou
E a gente conversa nesse nível
custo
Anteriormente eu não me preocuparia com
isso
Como o primeiro ponto da sua
lista, não, tá?
Eu também, eu também, Ayrton
Eu também, eu também
Acredito muito, eu acho que a
galera não tem
Conhecimento interno
Ou, por exemplo, isso aqui que
eu estou trazendo para você
Já vai trazer, puta, você vai
sair daqui
Com outro entendimento de Tolkien
Você pode ter certeza do que
eu estou te falando
Quando eu entrar na segunda parte
do treinamento aqui, que é agora
Prompt e Reg, você vai sair
daqui
Com outro pensamento do que é
Tolkien e tal, então assim
Entendi, dá um passo para trás
Respira, custo
Obviamente todas as empresas
Olham para o custo, custo é
tudo
Mas ao mesmo tempo você não
precisa ficar louco
Com algo que realmente não é
esse bicho
De sete cabeças para o nível
que a gente está hoje
Tá, não o nível que a
gente está hoje
Cara, se você parar para ver
O custo das APIs, na minha
humilde opinião
Elas são baratas, cara, para o
que você consegue fazer
O nível de inteligência que você
tem
Elas são baratas, agora, você sabe
dos
Por exemplo, você tem o agente
Que vai decidir routing
Como que você vai botar routing
Você vai meter um 4 -0
para fazer routing
Não, você vai usar, cara
Um modelo, talvez até treinado específico
Pequenininho para isso, beleza
Mas você vai, ou seja
É a capacidade de você entender,
interpretar
E saber tunar, eu acho que
vai ser muito
Nessa questão para você eliminar tudo
Isso depois
Tudo bem
Tudo bem
Minha pergunta é mais
Uma orientação de um iniciante
Estou me dando de carreira
Para mais
Para ter data de linha
Até a utiliza
Eu tenho
Uma secretária de agendamento
Acabei
Saindo de uma dor
De uma pessoa próxima
Até do meu sogro também
Estou elaborando outro
É bem baixo, viu Henrique
Seu áudio, tá
Tá bem baixo, tô tentando aqui
É, tá muito baixo
Se eu falar mais alto, você
melhora ou não?
Ah, melhora
Melhor
Então, eu estou recém -iniciando
Estou com os pais, com os
centros
Acabei caindo
Numa dor aí de um familiar
Para elaborar um robô de atendimento
Até tem um já rodando hoje
O quão
Isso é interessante
Eu segui nessa parte
Eu não vejo como é o
local
Eu não vejo muito interessante
Para mim, né
Principalmente se eu quero seguir a
parte
De data de engenharia, né
É válido eu seguir esse caminho
Ou ele é só o início
E tem muito mais a ser
aprofundado
E seguir outros caminhos
Mais interessantes
De forma consistente
Robusta
O que o Enel
Me agrega ou não
Ou há alternativas melhores
Eu acho que espera um pouco
Até o final que eu vou
mostrar
Uma multitude de coisas
E eu vou
Cara, para quem ficar até o
final
Assim, vocês estão vendo
A superfície do negócio
Vocês vão ver coisa aqui de
arrepiar
Então, assim, relaxa
No final você vai ter uma
visão
E uma auto -resposta disso aí
Do que você está falando
Mas a resposta é
Se você pode explorar essa área
Definitivamente
Olhe vagas no LinkedIn
Olhe vagas em todos os lugares
Que a galera está pedindo
E olhe por esse prisma
Porque, cara, a gente vai precisar
fazer isso
De novo, escolhe quando você vai
entrar
Se você vai entrar em um
Agora ou se você vai entrar
depois
Para mim, eu adoro surfar a
onda
Eu sou surfador de ondas
Então, eu prefiro surfar a onda
Agora que é muito mais fácil
Ao médio prazo
Porque, olha o tanto de conteúdo
A gente está na primeira parte
De cinco do treinamento
Então, relaxa
Vamos lá
Mais duas e a gente continua,
gente
Luciano e Rodrigo, vai lá
Rodrigo, apertei sem querer
Multiplar de novo
Mutei
Bom dia, bom dia
Bom dia, galera
Então, eu acho que já peguei
no gancho
Da pergunta do amigo aí
Que está começando
E aí eu acho que o
que ele tem ali
De projeto na mão
É algo menor, né
Então, eu acho que a dúvida
dele
Sobre o N8N
Eu acho que como a gente
vê aí
O Luan explicando
Que isso é uma fonte
Para almejar projetos maiores
Eu acho que no geral
Aí para o pequeno e médio
Tem bastante coisa que tem uso,
né
Nossa, demais
Então, porque você não vai ver
Você não vê quase nada disso
Nesse nítido, né
E tem bastante espaço
Para pesquisar, então
Já pegando lá o gancho
E na opinião do Luan, né
Falando que eu vejo a gente
Conversando muito sobre os grandes
Os grandes, os grandes
Mas a gente tem
Os pequenos e médios aí
Que estão inscritos no mercado
Eu acho que é grande
E riqueza está nos médios
Tá?
Eu realmente acredito
Porque assim
O poder que você consegue
Entregar para a galera média
Que vai escalar e virar grande
É muito absurdo
A gente está vendo como empresa
A quantidade de clientes
Que pedem isso hoje assim
E está assustador
Então, eu vejo muito sobre isso
Mas por que eu sempre falo
Dos grandes, ô Rodrigo?
Porque eu penso o seguinte
Cara, se você tem essa visão
Que você tem agora
E eu te mostro o grande
Você concorda que o pequeno
E o médio vai ser
Piece of cake
É isso
Eu estou mostrando
O que está acontecendo
Lá no Bleeding Edge
Então, se você foi inteligente
E entendeu o que está caindo
Você vai falar
Caralho, velho
Beleza
Isso aqui é arquitetura
Tipo top notch
Foda
Então, meu irmão
Eu posso fasear
Eu posso pensar isso aqui
Eu posso diminuir isso aqui
Custo e tal
Então, assim
Eu estou te dando
A visão mais complexa
Trazer para baixo
Vai ser muito mais fácil
Se eu te der uma visão
Muito narrow
Quando você acionar
Um cliente nível alto
Cara, não, cara
Vocês usam N8N?
Não, a gente não faz
Reg com N8N aqui, não
A gente usa
N8N
Lengshen
Lenggraph
Lengflow
Não, eu não conheço isso
Entendeu?
Então, eu vou começar
De cima para baixo
Mas a sua ideia está certa
É assim mesmo, tá?
Luciano
Eu só vou dar
Meus dez centavos
De apoio
Eu concordo plenamente
Porque faz alguns meses
Que eu estou estudando
O N8N
Mas, assim
Com foco no pequeno
Por exemplo
Sim
Assim, eu passo
Por exemplo
Estou caminhando na rua
Cara, aqui dá para dar
Um apoio
Para esse cara aqui
Uma venda automatizada
Uma classificação
De clientes
Enfim, até ontem
Eu estava falando
Com o entregador
De gás e água
Aqui da região
Porque eu tive uma ideia
O cara
Ele me atendeu
No telefone
E estava atendendo
Um cliente
Em loco lá
Eu penso assim
Cara, quantas oportunidades
Esses pequenos e médios
Estão perdendo
De lead
De confirmar
Um cliente
Por conta de um
Atendimento precário
Na verdade
Não é questão de culpa
Ou não culpa
Mas o atendimento
É limitado
Uma série de coisas
Então
E eu acho assim
Voltando mais lá atrás
E só corroborando
O que tu já falou
Que N8N
E outros
Grandes modelos
O cara tem que ter
O feeling
De onde implementar
Não é para qualquer um
Que serve
Não é
A dinâmica
Do N8N
Ela é
Absurda
De grande
De possibilidades
Enfim
Mas quando a gente
Fala de um grande
Por exemplo
O cara não vai botar
Um N8N
Ele não é escalável
Por exemplo
Eu tenho um N8N
Numa VPS
Ali
Pequena
Me atende
Mas
Para um cliente grande
De alta performance
De alta volumetria
Certamente não vai
Atender
Aí entra
O que eu estava até
Falando com o Matheus
No início
Cara, eu estou brincando
Com o Langchain
OpenAI
E Big Query
Por exemplo
Fiz um agentezinho aqui
Linguagem natural
Para
Ah, eu quero
Coca
A categoria
Dos produtos
Que mais vendeu
Olha o ranking
Cara
Isso daí
É o sentido
Eu acho que
Hoje
Mais
Alto nível
De
De tu
Prover
A democratização
De um dado
De uma informação
Para qualquer
Departamento
De um cliente
De uma empresa
Sim
É isso
Boa
Concordo com você
E tem muita
Área para explorar
Tem muita coisa legal
Acho que a gente está
Entrando num momento
Muito legal
Gente
215 pessoas
Vamos ver quem são
Os guerreiros
Que vão ficar até o final
Tem novidade no final
Novidade boa
Para vocês
Para mim
Para todo mundo
Principalmente um ebook
Exclusivo
Para quem ficar até o final
Sobre todas as minhas
Ferramentas
Disso
É uma jornada
De seis meses
Então assim
Eu não vou falar
Mas isso vale muito dinheiro
Para quem ficar até o final
E não vai estar
Na gravação
Não vai estar online
Não vai ter como pegar
É só para quem tiver
Até o final
Tá
Vai ser
Todas as ferramentas
Que a gente tem
De stack
A gente vai entregar
Para vocês
Qual é estudar
Qual é a sequência
Qual é tudo
Tá
Então
Fiquem aí
Que eu vou entregar
Isso lá no final
Beleza
Antes da gente
Antes da gente
Iniciar
Me dá aí
Uma
Uma
Uma chamada
No chat
Porque se for
Para eu ficar sozinho
Aqui no sábado
Sem estar animado
Fica até cinco horas
Quatro horas da manhã
Ontem
Terminando aqui
Os ajustes
Desse
Desse treinamento
Para você
Luan
O put
Caso de uso
Tá pronto
Quer que manda?
O quê?
O put
Caso de uso
Qual caso de uso
Que eles querem ver
Nos próximos
Treinamentos
Ah tá
Manda aí
Manda aí
Manda aí
Vamos ver
Que é bom
Porque também
A pessoa acorda
Vamos ver
Quem tá acordado
Vamos ver
O que vocês estão achando
Ó
Text
Ó
Interessante
Pouco
Inteligente
Do
Document Extraction
Text
O reg
Tá chegando
Tá legal
Ó
Que legal
Não
Mas olha o chat
Bora galera
Vamos
Todo mundo votar
Bora
211 né
Vamos votar
Pelo menos aqui
200
Para poder responder
Esse público
Vamos lá
Vamos lá
Pelo menos
Vamos
Vamos
Tem gente ainda
Sempre para os 150
Vamos lá
Bora
Bora
Participação galera
Sábado
Vocês já estão
Aqui
Vamos
Vamos aproveitar
Desliga isso lá
Foca
Ó
Minha esposa
Tá fora
Eu tô focado aqui
Eu sei
Eu tô entregando
Mas
Vamos lá
Foquem
Não
Os três
Um
Um outro
Gente
Reg
E text
Query
Intelligence
Foda hein
Os dois
Ó
Galera
72
75
Legal
Legal
44
42
Muito bom
Matheus
É porque eles ainda
Não viram
Reg
Claro
Eu também vou mostrar
O text
Ups
Não vou
Mas
Relaxa
Tem coisa boa
Beleza
Termina aí
Vamos lá
Galera
Me fala aí
Me bota aí
Já que tem 180
Eu quero ver 180
Estão gostando
Sim
Tá curtindo
Já deu um contexto
Legal
É
E aí
E aí
Muita coisa nova
Quem aqui
Primeira vez
Está fazendo treinamento
Comigo
O que vocês estão achando
Tenho muitas pessoas
Que eu conheço aqui
Tá legal
E isso que a gente
Só começou
Vamos
Vamos entrar agora
Na engenharia
No AI
Engineer
Vamos entrar na parada
Mais
Mais robusta
Então vamos lá
Vamos falar
De
Prompt Engineering
Tá
Então
Esse conteúdo
Muito legal
Pode ir lá
Matheus
Gosto muito
Dessa parte
Então
Aqui a gente tem
Nós temos
Três pedaços
O prompt
Onde nós
Que imputamos
Conseguimos de fato
Trabalhar
O LLM
Que vai variar
Obviamente
De acordo com o modelo
Que você vai escolher
E a resposta
É
Interamente
Que você vai receber
Então a gente tem
Essa interação hoje
Prompt
LLM faz o reasoning
Faz
Pega essa resposta
Proveu o próximo token
De cada
Palavra
E te dá uma resposta
Baseada
Nisso
Existem vários
Modelos de fundação
Aqui interessantes
Inclusive alguns
Open source
Muito bom
De psique
Lhama
E assim por diante
Só que beleza
Vamos pensar um pouquinho
No internals
Do prompt
Como isso acontece
Então ele é dividido
Em cinco partes
Realmente
Então por exemplo
Se eu pegar aqui
A parte do input
Como a gente já viu lá
Então eu estou
Pedindo aqui
Por exemplo
Extracting voices
Data from Uber
Eats recipe
Então
Receipt
Beleza
Então eu estou pedindo
Para extrair os invoices
Eu vou fazer
Tokenização
Dessas palavras
Lembra que o Open AI
Chat APT
Usa o TikToken
Então ele vai quebrar
Isso em tokens
Depois ele vai fazer
O embedding
Que é transformar
Esses tokens
Em vetores
Então internamente
Ele vai ter
Um embedding específico
Que ele vai fazer
Lá
Na parte numérica
Ele vai fazer
A inferência
Do próximo token
De fato
E vai entregar
Essa resposta
Para você
Então
Internamente
Isso aqui
É o que acontece
O que a gente pode fazer
Em relação
A esse processo
Bem
De novo
O prompt
O prompt
Tem muito a ver
Com quantidade
De tokens
Tem muito a ver
De fato
Com a inferência
Que ele vai fazer
E com a predição
Da próxima palavra
E de como esse dado
Vai ser retornado
Para você
Ou seja
Transformar
E trabalhar
Com isso
É algo que nós
Usuários
Temos controle
De fazer
Então
No final das contas
A gente teria ali
O próximo
O próximo
Output
Sendo gerado
Como o CNPJ
Por exemplo
Tem duas coisas
Muito importantes
Quando a gente
Está interagindo
Quando nós estamos
Interagindo
Com o
LLM
Que talvez
Na superfície
Você não entenda
Mas por debaixo
Da superfície
Você entenda
Muito bem
Eu vou mostrar agora
Nós temos duas características
User message
E o system prompt
Então
De novo
User message
É autoexplicatório
Explicativo
É o input
Que você está passando
Então o propósito
Ele tem que ser específico
Escopo
Interação
Atual
Persistência
Parte
Da conversação
Histórico
Prioridade imediata
Então você está mandando
Ele vai reagir
Imediatamente nisso
A não ser que tenha
Os casos do chat EPT
Eu não sei se vocês
Já viram isso
Que o bicho é muito
Muito malandro
Ele fala
Relaxa
Eu vou trabalhar aqui
E já retorno essa informação
Para você
Vocês já viram isso
Ele nunca retorna
Ah vou processar
Esses documentos
Para você
E já volto
E ele não faz
É maravilhoso
E casos né
E você tem o system prompt
E é aqui que
A coisa legal acontece
Olha só que interessante
Quando você
Faz um prompt
Você acha que é esse prompt
Que é passado
Para o LLM
Não né amigão
Porque nosso prompt
É meio merda né
Então na verdade
O que que todos
Esses caras fazem né
Eles tem um
Prompt augmentation
Ou um tuning ali
Do seu prompt
Então quando você escreve
Olha só o que que acontece
Por debaixo dos panos
E aí muita gente fala
Poxa
Por que que o token
Do cloud
Eu não sei se vocês já
perceberam
Se vocês usam cloud
Né
Vocês vão entender
Que o token dele aumentou
E muita gente acha
Que esse token
A gente utiliza
Boa parte dele
É
Mas uma outra boa parte
É utilizada dessa forma aqui
Por exemplo
Quando você manda
Essa informação
Para ele
Por debaixo dos panos lá
Clouds web interface
And mobile apps
Use a prompt
To provide up to date information
Such as current date
Blá blá blá blá blá blá
blá
Então ele usa isso aqui
Então na hora que você manda
Como contexto
O que que ele embute
Lá no seu prompt ó
Esse assistente é o cloud
Criado pelo Anthropic
A data atual é essa
Aqui tem algumas informações
Dos produtos
Olha só ele te passando
O contexto lá
Tá então ele vai passando aqui
ó
Se a pessoa perguntar
Cloud blá blá blá blá blá
blá
Ops tal
Você sentiu
Não tem outros produtos
E aqui ele vai especificando
Cadeias e explicação né
Ele não pode fazer tal coisa
É ó lá
É
If cloud cannot
Or will not help the human
With something
It does not say why
Or what
It could lead
Since it comes
Across as preachy
And annoying
Beleza
Ele vai passando características aqui
Vai dando tom
Vai dando características
Vai dando
É
Os
Os
Os contextos específicos
Para ele
Olha isso aqui
Que foda
É
Donald Trump
Is current president
Of the United States
And was inaugurated
On January
Blá blá blá
Ou seja
Isso aqui
São informações
Que foram injetadas ali
É
Nunca
Fala
Responde
Tal
Ideia
Observação
Ficou boa
Fascinante
É
Cloud agora está conectado
E aí você tem vários
Prompts aqui
Né
E você consegue ter aqui
Os prompts de cada um
Então no final das contas
Esse cara
Vai ser injetado
Na hora em que você faz
Então no final do dia
Tá
Ele vai estar por aqui
Você vai receber
Ele vai de fato
Né
Tokenizar
Receber o injection aqui
Passar pelo processo
E ter sua resposta
Então esse system prompt
É onde você dá
Por exemplo
Isso aqui vocês vão entender
Por quê
Você vai dar o propósito
Você vai definir
Quais são as características
Qual o comportamento dele
Você vai dar por exemplo
Informações como
Escopo
De conversas anteriores
Você vai dar por exemplo
Persistência
Você vai dar por exemplo
Você vai dar prioridade
Qual o caso de uso
Você vai sentar com a persona
Que vai fazer isso
Cara e persona funciona muito bem
Eu não sei se vocês já
viram isso
Cara persona é uma das coisas
Que mais funcionam
Quem
Quem nunca usou persona
Que diga eu
Ah por exemplo
Se comporte como especialista
De tal tal tal
Pra me responder isso
Vocês podem fazer um prompt
Sem persona
E com persona
Você vai ver que a tendência
Da persona é muito explícita
Funciona muito bem com persona
Tá
E aí
Beleza
O que que significa
Um ótimo prompt
Ou pelo menos
Trabalhar isso
De forma inteligente
Tá
Vamos lá
É
Contexto
Então
O que que você tem que
fazer
O que
Às vezes
Muita gente tem preguiça
Da contexto
Né
Então
Da contexto
Vai prover visibilidade
E clareza
Pra o LLM
Então por exemplo
Se eu tô trabalhando ali
No processo do Uber Eats
E tals
Você
É
Você está trabalhando
Você está processando
Invoices do restaurante
Do Uber Eats
Pra operações
De reconcilhamento
De recolhiação
De reconciliação
Então aqui eu tô passando
Contexto
Hum
Roll
Ah
Isso aqui é muito importante
Né
Ele dá uma identidade
Pro modelo
Pra que ele possa
Reduzir
Fazer um narrow
Das características
De acesso
Que ele vai fazer
Beleza
Então aqui ó
Assign an identity
To model
To frame
Its capabilities
Então
Haja como um documento
Um expert
De processamento
Em documento especializado
Financeiramente
Com invoices
E receipts
Então cara
O que ele faz
Quando você passa
Esse kill
Esse hint
Pra ele
Principalmente palavras
Como expert
Principalmente palavras
Como financial
Principalmente informações
Como invoices
Ele vai acessar
Especificamente
Essas informações
Então você vai conseguir
Fazer um ground
Muito melhor
Ação
Né
O que você vai definir
Que ele faça
Então por exemplo
Tem muita gente
Que escreve prompt
Que
Dá o contexto
Não dá role
E sinceramente
É o que mais a gente
faz
E eu vejo assim
Dá contexto
Não dá role
Dá uma ação
Tipo
Às vezes nem dá ação
Eu quero tal
É tipo assim
Faça isso
Isso
Isso
Me mostra isso
Isso
Isso
Me dê
Às vezes você só joga no
prompt
Né
E você não tem a ação
A ação é o que?
Cara
Identifica todas as extrações
Verifica as transações
Acha os campos
E assim por diante
E esse aqui também
Muito das vezes
A gente
Escreve muito prompt
Por causa que a gente não
passa
Output
Esperto
Me cria um artefato
De documento
Me cria um artefato
Em output
JSON
Me cria um artefato
E tal, tal, tal
A gente esquece isso
E isso faz a gente reinterar
Cada reinteração
Custa tempo
Então
Né
Se você puder escrever tudo isso
E reduzir o seu tempo
Muito melhor
Né
Então aqui eu estou especificando
O jazon
Para ele me retornar
O que seria um prompt legal
aqui?
Por exemplo
Isso aqui é um prompt
Muito legal
E eu vou mostrar para vocês
A realidade
Uma coisa muito foda
Tá
Então
Ó
You are a document processing expert
Specialized in financial invoices and receipts
Aqui é o que?
Aqui é exatamente
A role que eu especifiquei para
ele
Aqui eu estou dando o contexto
Aqui eu estou dando a ação
que eu quero
Aqui eu estou dando o output
que eu quero
E aqui embaixo amigão
Eu estou dando o guideline para
ele
Quem já brincou com guideline?
Cara
Isso aqui
Olha
Sem sacanagem
Quando eu descobri o guideline
E quando eu descobri o remember
Também
Em palavras garrafais
Isso mudou minha vida
Quando você tem um contexto muito
alto
E você manda um remember
Em garrafal
Você força o modelo
A fazer um reasoning em cima
de todo o contexto
Mais ainda
Então quando você quer lembrar de
alguma coisa
Que você já está muito na
thread
Você dá um remember
Esse cara
Dá uma hint
Para ele olhar
O que se perde no meio
Então
Isso é uma técnica muito legal
Isso está dentro de um
Um
Um
Ixi
Eu vou mostrar uma parada para
vocês aqui
Vocês vão ficar doidos
Tá
Calma aí
Então vamos lá
Técnicas avançadas
Tá
Então existem várias aqui
É
Inclusive
É
Umas bem legais
Mas na realidade
É
Você precisa estar muito bem especificado
Do que você quer fazer
Realmente
Tá
É
E existem mais do que essas
outras aqui
Características do que você pode fazer
Enfim
Mas existe uma parada
Existe um template de um bom
Prompt
E eu vou mostrar para vocês
Algumas características aqui
De como fazer isso
Na
Na vida real
Tá
É
Vocês vão ver uma coisa legal
Antes de ver uma coisa legal
Eu quero falar desse projeto
Quem já viu eu falando desse
projeto aqui?
Dan Ross
Projeto que
De fato
Eu fiz esse projeto
É
Beleza
Vocês provavelmente
Não escutaram o que eu tenho
para mostrar
As informações saíram
E eu vou falar sobre isso
E eu
Muito foda
Saiu semana passada
Tá
Esse cara foi publicado
Na Google Next
E foi publicado
No site da Pifian
E aí eu vou falar coisas
Para vocês aqui
Que vocês não vão acreditar
Mas é a realidade
Eu vou provar para vocês
Tá
É uma empresa
Que é subsidiária
Da Amazon .com
Então faz todas as entregas
De equipamentos
No Canadá
Nos Estados Unidos
Tem milhares de portos
Tem milhares de
De containers
Tem centenas de terminais
E assim por diante
Qual era o grande desafio deles?
E aí só para vocês achando
Que GNI
Não é usada fora do país
Então toma na cara aí
De vocês
Então o que a gente fez
Na verdade
Então
Basicamente
Os equipamentos
Os produtos da Amazon
Chegam no porto
Esses caminhões
Pegam essa carga
Vão para o terminal
O trucker
Entrega o BOL
Que é o Bill of Letting
Que é o documento
Que fala
Cara, dentro desse container
Tem esses conjuntos
De coisas aqui
E posteriormente
Esse cara
É o que?
Você tem CLECs
Que são mais de 3 .400
CLECs
Nos terminais
Que pegam esses documentos
E injetam essas informações lá
Beleza
E aí eles fizeram
Uma apresentação
Dentro do Google Next
E eu tive acesso
A os outcomes
Da minha implementação
Por que minha implementação?
Por que eu falo isso
Com tanto orgulho?
Porque foi o meu primeiro
Projeto de GNI
Foi o projeto
Que eu me desafiei
Foi um dos projetos
Que eu mais me desafiei
Na Pythian
Foi estimado
3 .000
Pessoas
Eu quis fazer sozinho
E eu consegui
Fazer sozinho
95 % do projeto
Foi desenhado por mim
Os códigos foram
Inteiramente escritos
Por LLMs
Eu não escrevi
Uma linha de código
E rodo mais de
30 .000 documentos
Por dia hoje
Então se você acha
Que Vibe Coding
Se você acha
E aí eu vou usar
A palavra Vibe Coding
Mais no sentido
Do que eu fazia
Que é
Não deixar
O cara
Guiar
O meu código
Mas eu guiar
O meu código
E eu aceitar
O que está sendo feito
E eu entender
O que está sendo feito
E eu editar
Em cima disso
Então eu ainda considero
Isso como uma característica
De Vibe Coding
Na minha opinião
Eu sei que essa palavra
Ela é muito
Muito ácida hoje
Falar que é Vibe Coding
Quer dizer que você é burro
Não
Eu acredito muito
Que é muito legal
Utilizar ferramentas
Para augmentar
Copilot
Windsurf
Tem muita coisa legal
O meu Vibe Coding
Não é tipo assim
Ah escreve
Eu vou ficar aqui
Jogando um videogame
Não
Muito pelo contrário
Ele vai augmentar
O meu trabalho
Talvez
Não chame de Vibe Coding
Para não ficar ruim
Mas qual é a diferença
Aqui desse problema
Né
Bem
O problema
Deles
E aí eu vou explicar
Esse caso de uso
Para vocês entenderem
A importância
De vocês entenderem
As coisas
Dos fundamentos
Que o Jorge falou
E eu falei
E várias outras pessoas
Aqui falaram
Tá
Luan
Você consegue
Construir um
Projeto de Terabyte Scale
Utilizando Vibe Coding
Ou escrevendo
Do jeito que hoje escreve
Para gerar
Para LLM gerar
Consegue
E eu vou te provar
Isso aqui
Né
Na verdade
Tá lá na Pifian
Tá no Google Next
Você pode acompanhar
Beleza
Então qual era o grande desafio
Cara
Uma vez que esse
Que esse caminhão
Né
Ele chegava lá
E colocava essa informação
O que que acontecia
Né
Ele
Ele tinha que
O usuário tinha que passar
Esses documentos manualmente
E daí
Entrava no sistema
E ia
Qual era o grande desafio
O que que eu tinha que
fazer
Né
Na primeira versão
Eu tinha que fazer o seguinte
Eu tinha que receber
Esses arquivos de TIF
Eu tinha que converter
Esses arquivos
Para PNG
É
Posteriormente
Eu tinha que converter
Esses arquivos
Para PNG
Eu tinha que
Passar uma série
De validações
Depois eu tinha que
Extrair
A primeiro escopo
De extração
Com Gemini
Depois eu tinha que
Formatar esse dado
E escrever
Dentro de uma API
Que é o Trackmate
Para poder escrever
Esse dado lá
Para os caras
Beleza
E aí
Obviamente
Que o projeto
Foi ganhando fases
Né
Então a primeira fase
Foi vendida por 170 mil
A segunda fase
Foi vendida por 340
A terceira fase
Foi vendida por mais 320
E aí
E agora a quarta fase
Que foi aprovada
Que eu vou trabalhar
Semana que vem
Foi vendida por
730 mil dólares
É
Que inclusive
Eu vou fazer sozinho
Como time
Tá
É
Então o que aconteceu
Aqui
É
Na atual
As coisas começaram
A ficar muito mais complexas
Porque a gente começou
A abrir para 35 terminais
Ao mesmo tempo
Né
Então o que aconteceu
A gente começou a receber
30 mil
Bills of Lettings
Por dia
Que possuem
Milhões de páginas
E o que eu precisava
Fazer com isso
Né
Eu precisava adicionar
Várias características
Ao longo do projeto
Eu precisava identificar
Quem é o shipper
Eu precisava ter
Prompt genéricos
Prompts vinculados a clientes
Eu precisava fazer
Tunning de prompt
Cara isso é caso real
Vejam isso
Colem o olho aqui
Porque isso aqui é caso real
Real, real, real
Tá
Não é eu falando
Mentir
Não
Eu fiz isso
Tá
Então o que eu usei
Para fazer isso
E eu vou te dar as
estatísticas
Que eu recebi do CTO
Do Jeff
Tá
Não estou mentindo
Daniel Ross
É
Google Next
Né
Então depois
Se vocês quiserem
Dar uma olhada
Do que o projeto é
Obviamente que a vida
De vocês é bem
Corrida
Mas
Se vocês quiserem ver
Um caso
Real
Realmente
Vida real
O que que a gente
Conseguiu fazer
Com isso aqui
E o que que a gente
Trouxe
Olha só
Olha só isso aqui
Olha o que que
Uma pessoa
Fez com
Wind Surf
Cloud
Gemini
E Chat EPT
Olha só isso aqui
O driver
Ele esperava
Tempo lá
Porque ele tinha que esperar
A galera fazer o que?
Porra
Passar os documentos
Tudinho
Manual
Para o sistema
Então
Vocês tem noção
Que o cara saiu
De uma hora e meia
Para dez minutos
Tem noção
O que que é isso?
Cara
Em vez de ficar lá
Uma hora e meia
Esperando
A galera fazer
Todo esse processo
Os caras estão fazendo
Isso em dez minutos
Depois
A criação do Freight Bill
Ela demorava
Quarenta e cinco minutos
Você tem noção
Que ela demora
Menos de cinco minutos?
Isso porque
O nosso lado
Faz em menos de dois
Menos de um minuto e meio
É o processo inteiro
Até a criação
A aprovação e tal
Agora gente
Desculpa a palavra
Puta que pariu
Olha isso aqui
A curace
Saiu de setenta e cinco
Para noventa e oito por cento
Com prompt
Eu vou mostrar o prompt aqui
Os prompts aqui
Isso é caso real
Não sou eu que estou falando
não
Se você assistir lá
O CTO está falando isso
Tá?
Não
O parágrafo
Parabéns não é isso
Não é pra mim o parabéns
O negócio que vocês têm que
entender
É que
Acho que vocês não entenderam
O ponto principal
Não é sobre mim
É sobre, cara
A realidade
O que você consegue fazer
O que nós
Com conhecimento foda
Com fundamento
Teoria
Você acha que eu sei
Todas as melhores práticas
De Python
De Thread
Executor
Você acha que eu sei
Todas as melhores práticas
De como chamar a GNI
De como fazer call
Meme
Como que eu atrelo
Uma parte de prompt
Com outra
Com vídeo embedado
Com imagem embedada
Você acha que eu sei
As melhores práticas
De como comprimir
Um TIFF
Para um PNG
Não, não sei
Não sei
Mas eu entendo
Muita coisa de arquitetura
Eu entendo de prática
Eu entendo de design pattern
Eu entendo como funciona
O escopo do código
O que é a função
Então
Como a gente tem experiência
Nos fundamentos
E entende
Você vai ler código
E você vai entender
Você vai pesquisar
Ou seja
É o que o Tomás falou
Vai acelerar o seu conhecimento
Vai melhorar
Você
Vai augmentar você
Então se você for
Desculpa
Inteligente
Você vai ser 10 vezes
Mais inteligente
Se você for
A tendência de fazer as coisas
Com preguiça
Você vai ficar cada vez mais
burro
Essa é a verdade
Então
Essa é a realidade
Então os caras foram
Para 98 % de acurácia
Tá
E agora
A gente está ferrado
Porque eles querem colocar
Mainstream
Para todos os outros
Ports
E eu começo
Segunda -feira
Minha jornada
A implementar esse cara
Bem
O que que eu fiz
Né
Eu trouxe
Algumas características
De prompt
Para mostrar
Para vocês aqui
Mas antes disso
Eu queria mostrar
Para vocês
Um prompt
Não
Eu vou mostrar depois
Vou abrir o cloud aqui
Eu vou mostrar
Para vocês aqui
O nosso repositório
Que eu vou compartilhar
Agora com vocês
Que vai ter tudo
Que a gente vai acompanhar
Aqui na aula de hoje
E a gente vai ver
Algumas coisinhas
Na prática aqui
Tá
Então eu vou abrir aqui
Eu deixei ele aberto
Por enquanto
Depois eu fecho
Então vocês podem acessar
Tá
Tá tudo bonitinho aqui
A gente tem várias coisas
A gente vai passar por ele
Agora
Primeiro lugar que eu vou
É prompt
Tá
Então aqui eu trouxe
Alguns
Tipos de prompt
Tá
Que você pode
De fato usar
E eu vou explicar
Um pouquinho
O que cada um é
E aí eu fiz uma coisa
legal
Que é
A gente pegou
Os invoices aqui
O Matheus criou
Vários tipos de invoices
Muito legais
Cara
Para vocês testarem
Por exemplo
Isso aqui é caso real
Esse caso
Que eu fiz
Eu não posso
Obviamente mostrar
Aqui para vocês
Mas segue a mesma ideia
É um bioflaring
Aqui é um arquivo
Do Uber Eats
Mas no final das coisas
Da conta
É a mesma coisa
Beleza
Então
Aqui eu tenho
Um modelo bem
Straightforward
No modelo 1
No modelo 2
Eu já tenho
Um modelo diferente
Tá vendo
Com layout diferente
E o modelo 3
O Matheus fez
Uma coisa muito legal
E o modelo 3
O Matheus meteu
Um handwritten aqui
Aí fica legal
O handwritten
Eu acho
Eu acho legal
O handwritten
Então
A gente tem
Nós temos 3 modelos
Lá no caso
De uso nosso
Nós temos 17 modelos
Né
E cara
Eu consegui fazer tudo
Com prompt
É
Acurácia
Absurda
E o principal
Com o modelo
Merda
Do Gemini
Eu estava usando
1 .5 na época
Ou seja
O nível de sofisticação
De prompt
Que eu tive
Que colocar
Para chegar
Nesse nível
Foi gigantesca
Tá
E outra coisa
Eu parei de depender
Dos cientistas de dados
De Berkeley
Ah Luan
Isso quer dizer
Que você estava
Mais inteligente
Do que ele
Não, não, não
Claro que não
Obviamente não
Mas por exemplo
Eu consegui
Criar metaprompt
Ou seja
Eu
Ingeria
Os modelos
Dentro
Do cloud
E eu pedia
Para que ele
Criasse um prompt
Que fizesse
A extração
Do dado
Ou seja
Um prompt
Em cima do prompt
Um metaprompt
Então
Quando eu descobri isso
Cara
Eu parei de chamar
Os cientistas de dados
Porque ele escrevia
Cada um desses
Eu falei
Cara
Eu vou ficar chamando ele
Eu vou testar aqui
Quais são os principais
A ação
Aí eu falava para ele
Olha o prompt
O metaprompt
Tem que ter a ação
O contexto
Tem que especificar tal coisa
Tem que fazer isso e isso
Cara
Eu não migrei ainda
Na verdade
Essa versão
Que eu vou colocar agora
Vai ser migrada
Mas eu sei que é melhor
Vai ser assim
Tipo
Absal
Absurda
Tá
Então
Então nós temos
Três modelos aqui
Então a gente vai brincar
Com eles agora
Então a gente tem
Nós temos aqui
O que?
O readme
Ah eu deixei
Um metaprompt generator
Para vocês
Como que eu uso
Características aqui
Pedir para esse cara gerar
O que é um metaprompt?
É um cara
Que vai fazer o que?
Você vai especificar
As características
Dentro desse cara aqui
E ele vai
Gerar
Um prompt
Para você
Específico
Para o que você quer
Então
O contexto
Que você vai passar
O sample
Dos documentos
E qual é
O output
Que você deseja
E ele vai criar
Um prompt
Extremamente otimizado
Para lidar
Com aquele tipo
De nuance
Que você quer
Então no meu caso
Ali por exemplo
Quando chegou
No nível de complexidade alto
Eu precisava
Identificar
Pelo shipper identification
Se esse cara
Teria que usar
Um prompt
Genérico
Ou se ele teria
Que utilizar
Um prompt
Customizado
Ou seja
Específico
Para o cliente
Baseado
No que?
No formato
Desse cara
Então uma das formas
Que eu fiz
Ao invés de ficar
Tentando ler
Formato de arquivo
Eu gastava
Dois LLMs
Eu gastava
Dois
Duas chamadas
O primeiro
Que eu chamava
De router
Que é o cara
Que identificava
E o outro cara
Que fazia
Esse cara aqui
Tá?
Então
Funcionou muito bem
A gente teve um custo
A mais no projeto
De três dólares
No mês
E a gente resolveu
Um problema extremamente
Complexo
Em vez de eu ter
Que ter OCR
Que eu treinar
Um modelo específico
Que eu ter que pensar
Em várias outras características
Para achar esse modelo
Então eu consegui fazer
Isso por prompt
Tá?
Beleza
Então o que a gente
Pode fazer aqui?
Olhar rapidamente
O readme
Tá?
Não vou gastar
Muito tempo com isso aqui
Porque a gente tem
Outras coisas para passar
E mostrar
Uma prática
Para vocês aqui
Então
O que a gente tem?
Eu trouxe sete
Né?
Eu trouxe zero shot
Né?
Que você vai dar
Uma inferência
Sem nenhum exemplo
Você vai lá
Tipo aqui
Estraia
Os detalhes
Desce em voice
Beleza
Vamos lá
Vamos lá
Dar uma olhada
Eu quero mostrar
Para vocês isso aqui
Eu peguei um projeto
Eu não estou de sacanagem
Que o cara fez isso
Num projeto de produção
Estraia as informações
Do invoice
Falei
Cara
Quem foi esse animal
De teta
Que fez isso aqui?
Aí eu falei
Cara
Por que você meteu
Isso aqui?
Porra
Porque o modelo é bom
Não é justo
O modelo é muito bom
Mas e esses detalhes
Aqui que você não capturou?
Né?
Então eu vou vir aqui
Na minha máquina
Eu vou pegar aquele
Aquele exemplo do 3
Que ele é mais complexo
Ali que ele tem
O handwritten
Né?
Então eu vou
Eu vou abrir ele aqui
Para vocês
Para a gente ver
Então eu vou pegar
Esse cara aqui
Modelo
Mod 3
Vamos ver se é o 0
É o 0
Tem 52 e 98
Aqui está vendo?
Então eu vou chegar aqui
Vou dar um
Zero shot
Vou dar um upload
E eu vou chamar
Esse cara
Aqui agora
Deixa eu ver
Onde ele está
Ele está daqui
Mod 3
0
Cara
É isso que eu estou
Estou pegando o cloud
E eu estou usando o sonet
Que assim
Já está falando
De um nível
Abissal
Então
Se ele fizer
Ou seja
Aqui
Não dei roleplay
Eu não dei role
Eu não dei contexto
Eu não dei
Cara
Eu não dei
Guideline
Eu não dei nada
Para esse cara
Beleza?
Então vamos ver
Se ele consegue entender
Pô
Ele já fez um trabalho
Excelente
Ele já dividiu
Por categorias
Aqui para mim
É
Aí por exemplo
O que isso quer dizer?
Isso quer dizer
Que cada modelo
Se comporta
De um jeito diferente
Ó
Deixa eu provar
Para vocês
Se eu chegar aqui agora
E pedir
Para esse cara
Fazer a mesma coisa
A resposta dele
Não vai ser igual
Porque os modelos
Foram treinados diferentes
Com embeds diferentes
Por
Características diferentes
Por
Searches diferentes
E assim por diante
Ó
Aqui ele já extraiu
Dessa forma
Né?
Se eu pegar o groc
E pedir para ele fazer isso
Ele vai fazer
De uma outra forma também
Né?
Então vamos dar uma olhada
Nesse cara
Isso aqui é interessante
Para vocês verem
Cara eu brinquei muito com isso
Eu tive que brincar muito com
isso
E a gente só tinha Gemini
Então
Só dava para fazer com Gemini
Na época
Infelizmente
Tá?
Então ó
Aqui ele já
Ele já deu uma
Total
Viajada
Tá vendo?
Ele foi contextual
Eu falei
Cara extraia
Ele falou
O que eu fiz
O que tem nos conteúdos
Análise
Ó
The document likes
Specific in voice details
Você tinha as document likes
Dates, amounts e tal
Ele fez uma análise
E daí
Ele cara
Ele falou
Se você tiver acesso
Beleza
Extract
Vendo que ele
Ele olhou o sumário do arquivo
Ao invés de
De fato de
Extrair esse cara
Então eu vou ter que ser
Mais explícito para ele
Aqui
Ele já entendeu
Esse cara
Ele criou uma tabela
De mudanças aqui
E
Aqui
Ele tirou
Agora o que eu quero saber
É o seguinte
52 reais
Então vamos lá
52
Não tá aqui
52
Não tá aqui
E muito menos aqui
Então
A porra do prompt
Faz diferença
Entendeu?
É isso
Perdeu a informação
Perdeu a informação
A porra do prompt
Faz diferença
Vamos ver se faz mesmo
Vamos lá
Então vamos voltar aqui
Vamos trabalhar agora
Uma característica
De você conseguir
Especificar
O output
Então
O que eu vou falar pra
ele?
Vou falar pelo seguinte
Por favor
Extraia
Né
Seguindo essas características
Em jazon
Né
Então vou falar aqui
Em jazon
Então
E vou falar aqui
Em jazon
Então vamos lá ver
Opa
Bem melhor hein
Ele já extraiu
E olha só que interessante
Quando eu chamo
Other items
Olha só que legal
Que ele colocou aqui pra mim
É
Deixa eu abrir
Presta atenção
Isso é prática real
Pra vocês entenderem
Que faz diferença
Porque muita gente fala
De prompt
Mas não mostra isso
Na vida real funcionando
Né
Ó
Itens do pedido
Ele colocou isso
Dentro de um array
Lindo né
Cara
Muito inteligente
Beleza
Pegou total amount
Colocou o payment
E assim por diante
Bem
Tem 52 reais aqui?
Não
Não tem
Bem
Tem 52 reais aqui?
Também não tem
Mas ele fez a mesma coisa
Beleza
Eu já fui explícito
Né
Sobre isso
E o
E o
E o grock
Tá aqui né
O bichão
O bichão da
Da bala louca
Tá aqui
Tá sofrendo
Pra poder entender
O que eu pedi pra ele
Mas beleza
Vamos lá
É
Vamos voltar aqui
Essa demo é bem interessante
Na minha humilde opinião
É
Few shots
Aí aqui
Eu vou passar alguns exemplos
Ó
Tá
É
E aí
Que que é legal
Isso aqui funciona muito bem
Tá
Ele já fez isso sem
Mas eu falo o seguinte
Olha
Você
É um invoice
Parse
Aqui está a estrutura
De output baseada
Então
Se ele não fez
Como ele já fez
O item ali
A gente ia ver
Que ele ia abrir
O item
Entendeu
Porque aqui ele já fez o
item
Mas
Em muitos outros
Ele não conseguiria
Abrir esse item
Só que ele conseguiu fazer
Olha só como ele estruturou
Pra mim
Ele já fez
No específico
Processo
Que eu pedi
Tá
Bem legal
Isso né
É
Beleza
Ó o grock
Grock mandou bem
Viu
Matheus
Grock mandou bem
Gostei
Bora mandar aqui
Pra ele agora
Um
Um few shots
Pra ver se ele
Se ele consegue
O grock
Depois dessa versão nova
Ele não tá meio
Igual o Claudio
Na versão 4
Meio que
Tentando se corrigir
O tempo inteiro
É
Ele tá meio doido
Sim
Eu também
É
O meu grock é pago
Respeita
Você acha que é
De graça
O meu grock é pago
Acho que eu troquei
Todas as minhas assinaturas
De streaming
Pra assinatura
De LLM
Nos últimos meses
É
Só tem
Só tem
Ele e LL
Tá meio grog
Boa
Ele tá meio grog
Ótimo
Mais o que
Tá
Chain of thought
Isso aqui a gente fazia
Muito em modelo
Que não pensava
Né
Todos esses modelos
Novos aqui
Que a gente tem
Ele pensa
Ele tem reasoning
Avançado
Então ele vai passar
Pedaço
Ele vai fazer
O think through
Entendeu
Principalmente se eu habilitar
Aqui o opus
Né
Que é o papai opus
Papai opus vai fazer
Um chain of thought
O que é o chain of
thought
Você tá falando o seguinte
Olha eu quero que você
Seja uma
Assistente
Tal tal tal
E que você segue
Essas
Siga essas seguintes regras
Identificar o pedido
Extrair tal tal tal
Comece analisando
O layout
E blá blá blá
Então eu vou
Eu vou sendo mais
Explícito em relação
Que ele vai fazendo
E ele vai fazendo
Pedaço a pedaço
Em cima desse cara aqui
Tá
Bem eu pedi pra ele ser
Explícito
Sobre isso
Mesmo assim
Ele não achou
Aquela informação
Tá
Ótimo
Vamos aqui
Agora
No
OCR focused
Né
Olha lá
Esse invoice
Contém texto
E mimics
Handwritten
Or cursived fonts
Opa calma aí
Vamos ver se isso aqui
Ajuda um pouquinho
Caralho
Ô Matheus
Eu fico excitado
Com essas coisas
Véi
Isso aqui é muito
Muito legal
Você ver
É
Quando eu tava fazendo
Um projeto
E eu comecei
Adicionando isso
Eu comecei a ver
Tipo
Isso se tornando luz
Hoje não
Porque hoje os modelos
São muito avançados
Então tipo
Você precisa fazer
Pouca coisa
Pra ter grandes
Resultados
Mas cara
Antes
Putz
Olha só
É
Eu vou analisar
OCR análises
Crystal potential
Start schemes
Blá blá blá
Item verification
Blá blá blá
E aí
Matheus
Fala foda
Pra mim
Por favor
Olha
Ele achou
152
Aí
Ou seja
E a foda
Olha o que ele falou
Totalmente
Ele fez um crossover
Então ele fez o total
E falou assim
Olha gente
Pega isso
Reduz
Pelo valor
Fica lá
Vocês estão dormindo
Vocês estão achando
Isso aqui foda não?
Eu acho
Eu acho
Isso aqui é muito
Desequilibrado
Renan
Como tudo na vida
Igual a gente
Imagina você
Tentando ler
Algo que você
Nem entende
Exatamente
Beleza
Aí nem a gente consegue
Então a gente
Já conseguiu
Aí tem
Algumas outras variações
Que você pode fazer
Agora
E esse aqui Matheus?
Esse aqui eu apelei né?
Toma
Aqui ó
Ó
Na verdade vamos ver
O que que é melhor
Ó
Você é um documento de identidade
Sua missão é extrair
Estrutural financial logística
E from beritos em voz
Documentos que estão em PDF
Estão dando hint aqui
O documento contém
Tipado e hint written
Alguns campos
E aí botei
Olha
Atenção
Isso funciona
Infelizmente
Se você não sabe disso
Funciona, tá?
Esse documento tem e -mail
Handwrite
Apply OCR strategies
Objetivo
Olha lá
Duas caixas altas
Como markdown
Funciona também, tá?
Extrai o dado estruturado
Escaneado
Estritamente
Não presente
Retornar nulo
Ou seja
Aqui eu não estou deixando
Nenhum campo
Estou retornando um mulo
O sistema deve
Somente
Olha essa hint aqui
Only
E daí você força ele
A fazer o narrowing
Retornar nulo
Retornar valores
Documentos que são tipados
Ou hint written
E nunca alucinar valores
Nunca
Em caixa alta
Retornar nulos
Aí
Entidade
Definição de entidade
Extrair os seguintes campos
Embaixo de entidade
Restaurante
O que que é
CNPJ
Ou seja
Especifiquei
Depois sumário de pedido
Depois sumário de detalhe
Tá?
E aí eu passo pra ele
Essas características
E esse nem está muito animal
Vou mostrar o que eu fiz
Mentira
O que eu fiz não, né?
Que
O metaprompt
Que tu fez pra mim
É
Ele ainda não está conseguindo
Amigão
Ô amigão
Estraia pra mim aí
Amigão
Ó
Eu pedi
Coisas específicas
Tá vendo?
Que ele não achou
Algumas coisas
Ele não achou
Algumas coisas
Ele não achou
Ó
Ele não achou
Algumas coisas
52
Então
Falta alguma coisa
Aqui
Pra que eu possa
De fato conseguir
Agora
Deixa eu mostrar pra vocês
Um prompt
Que aí eu vou te mostrar
Uma técnica
Que aí na verdade
Quem me ensinou
Foi o
O cientista de Berkeley
Que é o Paul Spighalter
E esse aqui
É o melhor prompt
Que existe
Eu vou mostrar pra vocês
Um aqui
Beleza
Bem tranquilo ele
Viu?
É
Onde eu mostro pra vocês?
É
É o web editor
Tem algum editor web aqui
Legal pra ver?
Eu queria só abrir um negócio
E ver se tem que compartilhar
a tela
Mas eu vou ter que compartilhar
a tela
Eu pauso em
20 minutos o almoço
Tá?
Olha lá
Olha só
Você é um documento
Sem registração
Sua tarefa é extrair
E tal
Até demais
Objetivo
Extrair regras
E tal
O JSON tem que ser seguido
Durante a extração
Retornar valores
Tem que ser incluídos
Tal, tal, tal
Um consignir por documento
Definição
Definir
Aqui é importante
Não se confunda
Com as características
E tal
Beleza
Só que
Quando
Agora
Dica de milhões
Tá?
É
Quando você tiver
Um prompt
Que ele é
Muito complexo
Que ele é muito grande
Nós vamos ter que
Dividir esse cara
Em dois
Luan
Por que eu divido esse cara
Em dois?
Eu divido em dois
No mesmo contexto
Em duas chamadas diferentes
Dentro da mesma sessão
Ou seja
Como que eu faço?
Eu mando o primeiro
Mando a imagem
E mando o segundo
Então o que ele faz?
Ele encapsula
Ele faz um bubble wrap
Desse cara
Só que aqui no segundo
Eu vou ser mais
Especialmente
Específico
Olha só
Especificações
Para esse tipo de cliente
Estrutura do documento
Nomes e padrões
Que ele utiliza
Patterns
Business entities
Endereço
Desambiguidade
Casos especiais
Hierarquia
Cara
Esse
Ó
Eu consegui 98 %
Com esse aqui
Tu bota fé, Matheus?
Esse foi os 98 %
Porque eu falei
Para ele
O que ele fez
O seguinte
Como a extraction pitfalls
Não extraia tal, tal, tal, tal,
tal, tal
Pensando assim
Não faça tal coisa
Pensando assim
Não separe tal, tal, tal
Dessa forma
E lembre
Que o objetivo
É definir com precisão
O recipiente
O shipment
Blá, blá, blá
Provedo
Tal, tal, tal
E o output
Aí isso aqui
Fez eu conseguir
98 % de acurácia
Ou seja
O que o Jorge tinha falado
lá
Aprender a escrever em inglês
Ou português
Que seja
Tá?
As contas
Teve da mesma
É
Exatamente
E aí?
O que vocês acharam?
Caso real
Assim
Tipo
Real, real, real
O que vocês acharam?
Legal?
Beleza
Ó
Agora está na hora
De a gente começar
Os nossos fundamentos
De reg
E começar a olhar
Estratégia
Chunking
Começar a preparar
Esse cara
E aí o Matheus
Vai vir com a demonstração
De Langsham
Pra gente
E a gente finaliza
Com o fluxo
De Langflow
Eu vou falar de Langfields
E vou falar de algumas
Outras coisas
E depois a gente tem lá
O ebook
E outras coisas
Pra ver
Vocês querem tirar
O almoço agora
Ou vocês querem
Tirar o almoço depois?
Se tirar o almoço depois
Esses fundamentos de reg
Vão precisar de mais
Uma hora
Mais ou menos
E aí
O que vocês acharam?
Vocês querem tirar o almoço
Vamos fazer um ebook
Eu acho que vai ser melhor
Tem aqui
O guideline
Readme
Aqui eu tenho
Explicação
De melhores práticas
E eu tenho
Metaprompting
Então o metaprompting
O que ele faz?
Ele vai criar
O prompt
Do que você quer
Então ele vai gerar
O prompt
Que você deseja
O que você vai fazer
Pra gerar esse prompt
Que ele deseja
Você vai passar o contexto
Vai explicar
Uma breve história
Pode ser um documento
Que você atacha
E aí
Você vai dar
Um sample
Vai descrever
Duas ou três exemplos
Desse cara
E você vai dar
O output
Você vai colocar
Esses caras aqui
No contexto
E vai passar
Esse prompt
Pra ele aqui
E ele vai gerar
Pra você
O prompt perfeito
Tá?
O depois
Do almoço
Você queria
Mais ou menos
Que horas
Só pra colocar
No book
Cara
Se a gente faz agora
Meio dia e quinze
A gente volta
Uma e meia
E se for depois
Você tá pensando
De fazer
Uma hora
É porque aí
É uma
Uma e vinte
Uma e quinze
Mais ou menos
Só
Vou botar
Uma e vinte
Vou fazer um pouco
Porque tá
Estar em mil
As opiniões
O Diego falou
Que tem criança chorando
Então
Se tem criança chorando
Já sabe, né?
Pra não ter briga
Vamos pra números
É, eu acho
Que a maioria
Vai
É melhor agora
Que tem filho
Tal
É
Eu vou ativar
O chat do Telegram
Pra vocês brincarem
Vocês me falam
Depois
O que vocês acham
De uma POC
Nesse nível
Que eu não dei roleplay
Eu assim
Você viu que meu prompt
Ali foi bem
Bem suave
Mas testem
Tá?
Me falem
O que vocês acham
Beleza
Matheus
É isso mesmo
Então a maioria
Da galera
Vai de fato
Querer almoçar
Agora
Tá?
Então vamos pegar
Almoço agora
A gente volta
Uma e meia
Né?
Uma e vinte
Uma e vinte e cinco
Uma e dez
Eu só medi dez agora
A gente volta
Que horas, Matheus?
Uma e vinte?
É
Uma e vinte, né gente?
Pode ser?
Não, vamos fazer o seguinte
A gente volta
Uma e quinze
E aí eu abro
Quinze minutos
De perguntas
E dúvidas
E aí uma e meia
Eu volto
E uma e meia
Começa efetivamente
Isso, é
Exatamente
Beleza?
Então já é
Então beleza
Vejo vocês
Pra nós seguirmos aqui
Ótimo
Então vamos
Vamos falar desse cara
Que
Sei lá o que que significa
De novo
Parece que eu tô dançando
É
Minha esposa perguntou
O que que eu ia fazer
hoje
Eu vou dar um
Workshop de reggae
Ela falou
Que porra é essa
Ficar dançando com a galera
Mas enfim
O que que é isso, né?
Antes de eu começar
Deixa eu saber o seguinte
Quem aqui
Quem
Quem não sabe
Quem não sabe
Quem não sabe
O que significa de fato
Reggae
Fala eu
Só pra eu ter uma ideia
Aqui coloca aí
Escreve aí eu
E tá tudo certo
Porque relativamente
Né?
Novo
Tá
Aí eu me pergunto
Cara
O nome do workshop
Era reggae
Por que que vocês compraram, velho?
Foi por causa do
Prompt engineering?
É
Tô zoando
Mas ações à parte
Tudo bem
Vocês tão aqui pra aprender
É
Lembrando do objetivo, né?
É
Qual vai ser
Que eu quero atacar
Essa parte agora
Então eu vou
Vamos lá
O que que é reggae?
Né?
Reggae nada mais é
Do que você
Ter uma fundação
A mais em cima
Do seu modelo
Né?
Sem entregar
Algo a mais
Em cima dele
Né?
Então basicamente
Se a gente for pensar
Se a gente fizer
Essa pergunta
Pra esses caras aqui
Ó
Você tem um prompt
De entrada
E se você fizer
Essa pergunta
Obviamente que eles
Vão ser muito espertos
Hoje
Se você perguntar
Pra esse LLM
Por exemplo
Quais são as features
Mais atuais
Anunciadas
No Apple
18 .5
Na release 18 .5
Essa informação
É de maio
Né?
12 de maio
De 2025
Né?
E aí o que você
Deveria escutar
De alguns LLMs
Né?
Não de todos
Eu vou explicar porquê
Mas há uns
Um ano atrás
Um ano e meio
Dois anos atrás
Quando você não tinha
Tools e capacidade
De você externamente
Buscar essa informação
Quando você não tinha
Reg
O que
Quando você não tinha
Information retrieval
De fato
O que que
Deveria ele falar
Né?
Ele falou
Cara
Pelo meu treinamento
Pelo meu cutoff
De treinamento
Que foi 2024
E o S18 .5
Não foi ainda
Anunciado
Então eu não posso
Te prover
Informações
Né?
Mas
Se a gente for
Perguntar isso agora
Por exemplo
Pro Grock
O Grock
Eu tentei
Perguntar pra ele
Né?
Qual é o cutoff date
Dele
Ele falou que
Não tem cutoff date
Não
Que ele é o bichão
Da atualização
Que ele
Contei Jorge
Pra tu ver
Qual é o seu cutoff date
Ele respondeu pra mim
Assim
Ah não sei exatamente
Não sei o que
Mas eu posso
Aprender
Automaticamente
Falei
Tá mas qual foi
Sua última data
De treinamento
Ele falou pra mim
Assim
Eu não sei
Falei
Caralho
Que filha da puta
Como assim
Você não sabe
Né?
Enfim
O que que acontece
Hoje
Se a gente for olhar
Nas evoluções
Eu vou pegar aqui
O chat GPT
Por exemplo
Né?
Quando eu venho aqui
Realmente em tools
Eu posso habilitar
O que a gente chama
De deep research
Ou web search
Né?
Então isso aqui
É uma forma
De ele fazer grounding
De ele cara
Externo
Além da base
De conhecimento
Que ele tem
Ou seja
O LLM
Ele foi treinado
Ele tem um cut off date
Que é importante
Pra vocês entenderem
Cara
Qual a última data
De corte
Do GPT
4 .0
Blá blá blá
Tanto é
Que se você for
Entrar dentro da API
Eu vou mostrar isso
Pra vocês
Que isso é verdade
Se você for
Entrar dentro da API
Cês falaram de custo lá
Né?
Eu deixei o agente ligado
Já faz sei lá
Um
Bastante tempo
E a gente já testou
Um bocado
Eu gastei
2 .54 dólares
Mas enfim
Se a gente vier aqui
Por exemplo
Em modelos
Deixa eu ver
Se eu consigo achar
Onde vai estar aqui
Mas
Você vai ver que
Vários modelos
Existem
Por exemplo
4 .0
4 .0
2025
Blá blá blá
As atualizações
Enfim
Então você consegue
Chamar isso
Na hora em que você
Tá programaticamente
Fazendo
Ou escolhendo
Um modelo
Mas é muito importante
Que você entenda
O modelo
E qual a data
Tanto é
Que muitas das vezes
Por exemplo
Se você for
Resolver
Tentar pegar
Vamos supor
Eu vou falar
Pra vocês
Dessa agno aqui
Pra quem
Depois eu falo
Mas por exemplo
Isso aqui é relativamente
Muito novo
E cara
Você não vai ter
Isso inserido
Dentro do contexto
Do LLM
Então
De novo
Ele vai ter que fazer
Grounding
Então o que você tem que
fazer
Você tem que instruir
Isso externo
Pra ele
Então se ele não vai ter
A resposta
No treinamento dele
Como que você
Prover contexto
Tá
Bem
Aí a gente vai entender
Que o reg
É exatamente
Isso
Ou seja
Quando eu preciso
De uma informação
Recente
Ou quando eu preciso
De uma informação
Especializada
O que seria isso
Poxa
Eu quero saber
A política
Da minha empresa
Né
Cara
Isso não tá treinado
No LLM
Mas o LLM
Ele é muito bom
Pra responder perguntas
Mas ele não tem
Essa informação
Né
Muitas das vezes
Se você perguntar
Pra alguns chats
Desse
Ele vai fazer
O que a gente chama
De alucinação
Né
Então muitos
Desses caras aqui
Eles alucinam
Muito
Né
Principalmente
Se você mexe
Na temperatura
Se você mexe
Algumas informações
Você tá abrindo espaço
Pra ele alucinar
Então alucinar
É basicamente
Cara
Ele inventar algo
Ele criar algo
Né
Ou de fato
Abrir espaço
Pra que ele possa
Sugerir algo
Ou que nem mesmo
Exista
Né
Já aconteceu
Muitas coisas
Ah
Quem nunca
Debugou o código
Pegar o erro
Colocar
O cara fala
Que aquilo
Na verdade não faz
Você fala
Cara
Isso aqui tá errado
Ah é verdade
Tá errado
Ou seja
Ele tava doidão
Seguindo
Uma linha de processo
Completamente diferente
Então
Por que que
Reg
É o grande
Queridinho do momento
E a gente acredita
Muito
Principalmente
Cara
Os grandes
Os grandes nomes aí
De GNI
De AI
Enfim
Que esse é um cara
Muito importante
Na verdade
Um grande diferencial
Porque não dá
Pra você ficar treinando
Esses modelos
Todo dia
Né
Não dá pra você
Treinar esses modelos
Toda semana
Não dá pra treinar
Esses modelos
A cada três meses
Também
Né
Geralmente
Eu acho que tem a tendência
Obviamente
A diminuir
Mas Reg
Sempre vai estar
Aí não só
Pela informação recente
Mas justamente
Pela
Informação de domínio
Quer ver um caso
De uso muito legal
Que a gente tá implementando
Aqui
A gente saiu lá
De dez
Quinze pessoas
Pra trinta
Né
E aí
Uma das coisas
Que você vê
Como empresa
E aí vai entendendo
Como que empresas
De pequeno
Médio
Grande
Pode se
Se comunicam
Também entendem isso
Cara
Você vai crescer
Sua equipe
O tempo que você gasta
Fazendo processo
De onboarding
Cara
Se conectando
Com as pessoas
Sabendo quais são
As políticas
O que fazer
Blá blá blá
Blá blá blá
E não somente
Pra isso
Mas cara
Você é um funcionário
Você quer saber
Cara
Um documento específico
Se falou tal coisa
Você quer pesquisar
Você acha que é mais
Eficiente você abrir
O Google Drive
Aqui
E ir lá
Nesses caras
E tentar achar
Ou seria melhor
Perguntar pra um agente
Ou perguntar ali
Pra um chatbot
Que tem todas
As informações
De conhecimentos
Específicas
Da área
Que você tá em questão
E ele pode responder
Isso em segundos
Né
Então obviamente
Esse é o futuro
Né
Por isso que a gente fala
De base de conhecimento
Sendo
Um valor vital
Pra empresa
Então você vai ver
Que todas as empresas
Aí eu posso falar pra você
Com muita propriedade
Todas as empresas
Que você conversa
Todas elas estão
Nesse momento
Criar uma base
De conhecimento
Especializada
Interna
Pra fazer diversas coisas
Pra fazer venda
Pra fazer
Cara
Políticas
Pra sumário
De informação
É
Pra manter o time
Sincronizado
Sobre o que tá acontecendo
Pra fazer sumário
De reunião
Interna
Cara
E aí vai
Os casos de uso
São endless
São infinitos
Tá
Então
Essa é a grande questão
E o REG
Ele justamente
Traz isso
Beleza
Então
Olhando
Né
Lá de cima
Como que funciona
Né
É simples
Obviamente né
É
A falta de conhecimento
A ignorância
É uma benção né
Quando a gente olha assim
Uma arquitetura simples
Dessa aqui
Fala pô
Beleza
Eu entendi
Que eu tenho um prompt
Aí
Esse prompt
Em algum momento
Ele vai pesquisar
Um document store
Ou ele vai procurar
Um lugar
Onde tem armazenado
Essas informações
Ou seja
Isso aqui
Tem informações armazenadas
Aqui
Beleza
Depois disso
Ele vai recuperar
Os documentos relevantes
Beleza
Ele recuperou
O documento relevante aqui
É
E aí
Eu vou inserir
Esse contexto
No LLM
E aí o LLM
Muito bom
Por sinal
Pra se trabalhar com linguagem
Ele vai me retornar
Uma resposta
É isso
Né
Então quando a gente fala
De information retrieval
O cara
Por exemplo
A gente pegou uma empresa
Que tinha um documento
Que o cara falou
Que ele fez
Reg
Olha o reg dele aí
Esse é o reg dele
Beleza
Aí tá aí
O cara
Atachou um bocado
De documento
No agente
E falou que esse cara
É reg
Beleza
Né
Palmas pra ele
Então quais são os benefícios
Que a gente tem que
Imediato
Né
Primeiro
Melhoria da acurácia
E relevância
Né
Primeiro
Segundo
Capacidade de customização
Pra treinar
No seu próprio
Não treinar
No seu próprio
Desculpa
Acessar o seu próprio
Dato
Tá
Acessar o dado proprietário
Reduzir
Reduzir a alucinação
Então isso aqui
É muito legal
Porque você consegue
Fazer um prompt augmentation
Que a gente vai ver aqui
Que fala o seguinte
Cara
Só me retorna
Baseado
Em informações
Dos documentos
Tal
Lembra lá
No chat
Que eu mostrei pra vocês
É
Que eu botei vocês
Pra testarem
Que quando ele responde
Ele fala
Tipo
É
Fonte
Doc
Não sei o que
Então o
O reg
Consegue trazer citação
Ele consegue trazer
Em qual local específico
Ele escreveu ali
Baseado de como você
Indexa esse cara
Tá
E aí você tem
De fato uma solução
Que vai funcionar
Muito bem
Pra informações
Atuais
Ou além disso
Né
Estender o escopo
Pra informações
Proprietárias
Que é o que
Muita gente queria
Qual era o sonho
Da maioria das empresas
Há dois anos atrás
Um ano atrás
Cara
Eu tenho um LLM
Maravilhoso
Adorei
Tô super feliz
Só que tem um problema
Eu não posso expor
Minhas políticas
Eu tenho que ter
Dentro da minha
Empresa
Tentar nos meus
Guardrails
Eu quero treinar ele
Com características
Do meu setor
Com a minha inteligência
Da minha forma
E tal
Como que eu faço isso
Cara
Se você for pro mundo
De machine learning
É
Vai ser muito mais complexo
Você desembolar isso
Então o reg
Traz essa solução
Então o reg
Cara
É o futuro
Demais
Pra quem tá construindo
Essas informações
E todos aqueles casos
De uso que eu mostrei
Pra vocês
Tá
Tudo vai vir daqui
Beleza
Então
Vamos olhar agora
Dentro
Vamos abrir a caixa dele
E vamos ver o que que
ele faz
Passo a passo aqui
Tá
Esse aqui é muito legal
Então o que que ele faz
aqui
Passo a passo
Beleza
Vamos entender
A dinâmica
Então a primeira coisa
Que vai acontecer
Obviamente eu vou
Ter um prompt de entrada
Tá no seu controle
Então o usuário
Faz a pergunta
Cara
É
Aonde a
Taylor Swift
Tá fazendo tour
Esse final de semana
Né
Então ele faz essa pergunta
Lembra que essa é uma pergunta
Teoricamente
Eu tô
Implicitando
Um current date
Algo extremamente
Treinado e atual
Que eu não vou ter isso
Dentro de um LLM
Básico
Por exemplo
Se você pegar
O sofisticado
Como a gente tá falando ali
De grok
E tal tal tal
E fizer essa pergunta
O que que ele vai fazer
Vamos ver o que que ele
vai fazer aqui
Pegar o chat
Vou pegar o cloud
Que faz isso também
Então vamos ver
Bom que dá pra testar na
hora
Eu acho legal isso
Lembra que a gente tem que
Reduzir os caracteres
Isso
É token
Gasto
Inclusive eu errei um ali
Ó
Ele tem reasoning
Né
Ó
Olha aí
Eu não tenho informação
Tal tal tal
Beleza
Ah ele tem
É aqui ó
Olha só
Olha só
Ó o reg
Citação ó
Tá vendo
Ele fez citação
Então aqui você já consegue
Entender que ele
Precisou a base dele de reg
Tá
Que não é do treinamento dele
Olha que foda
Tá
Não tá no treinamento específico dele
Foi em outras fontes
Ele saiu do escopo
Do LLM
Dele básico
Pra poder ir lá
E tentar raspar essa informação
Se eu perguntar pro cloud
Vamos ver se ele vai ser
mais inteligente
Sobre isso
Ou se ele vai falar alguma
coisa diferente
Ah olha aqui
Saiu do contexto dele
Ele não tem essa informação
O cutoff date lá
Não deu
Ele foi externo
Ele utilizou o search
Ele fez grounding
E aí ele vai te dar
essa informação
Muito bem
Melhor
Olha lá ó
E trouxe
O que?
Esses caras
Tá
Então
Isso é reg
Foda né
Isso é reg
Então como que ele fez aqui
Então ele foi lá
Né
E aí aqui
Lembra
Você tem que pensar o seguinte
Eu tô olhando pros modelos
Mais sofisticados do planeta
Obviamente que quando eu for
Operacionalizar modelos
Você vai provavelmente
Aqui no Hang Face
Que a gente vai falar dele
Daqui a pouco
E aqui você tem vários modelos
Com várias características
Você tem modelos open source
E assim por diante
Então obviamente
Você vai ter essas características
Cada vez mais
Instauradas e necessárias
Pra que você entenda
Que a gente tá trabalhando
Com os modelos
Mais sofisticados do planeta
Então eles já fazem isso
Automaticamente
Você fala
Pô isso aí
Beleza
Ele tem que fazer isso
Não
Ele não faz isso
Tá
É automagicamente
É porque isso tá abstraído
Na interface deles
Eles tem que vender
E ficar bilionário
É
E multibilionário
Beleza
Ótimo
Ótimo
Então aqui
Mandou o prompt
Aí o que vai acontecer
Ele vai olhar o prompt
E vai entender o seguinte
Se eu tenho
RAG configurado
Pra esse cara aqui
Ele vai fazer o que?
Ele vai chamar
Um cara chamado
Retriever
Tá
O que que o Retriever faz?
Esse cara
É o componente
Mais relevante
Em relação a
Como ele vai interagir
Com o seu prompt
Então ele recebeu
O seu prompt
Em vez dele chamar
O LLM direto
Ele entendeu
Que ele tem um sistema de
RAG
Ou seja
Simplesmente
Em grandes tempos
Idiotas falando
É isso aqui
Quando eu recebi
Essa pergunta
E ele olhou
O prompt dele
O metaprompt
Desculpa
O system prompt
Ele entendeu
Que no system prompt
Ele tem o que?
Você tem acesso
Aos seguintes documentos
Via ferramenta
Então o system prompt
Dele falou
Que ele tem
Coisas adicionais
Então como ele
Entendeu
Que naquele contexto
Ele não tem essa informação
Automaticamente
Ele vai fazer o que?
Ele vai lá
Na base de conhecimento dele
Ele vai acessar
O Retriever
O Retriever vai
Cara
Em vez de eu
Alucinar aqui
Eu vou ver
Se tem alguma informação
Nessa base
Que esse animal de teta
Falou
Então ele vai lá
No Knowledge Base
Recupera esses embeddings
Faz essa consulta
E daí
Ele vai listar
Esses documentos
Vai trazer os chunks
A gente vai falar sobre isso
E como que ele
Recupera os arquivos
Relevantes
Então ele traz
Esse dado
Aí a gente tem
Um prompt
Augmentado
Que é o que?
Obviamente
O prompt
Que você
Mandou
Mais os documentos
E as informações
Que estão aqui
Atachadas
Dentro
Desses documentos
Desse cara
E daí sim
Tá?
Agora que o LLM
Fez o grounding
Ou seja
Ele foi lá
E buscou
Essas informações
Com mais
Deep understanding
Ele tem um dado
Atual
Ele fala
Cara
Para responder
A sua pergunta
O Taylor Swift
Vai estar performando
No BC Play
Em 23 de tal
2005
E aí ele pode
Adicionar a citação
Como a gente viu aqui
Que é muito foda
Então você consegue
Lá e ver de fato
A citação
Ele não está alucinando
Ou seja
Você está reduzindo
A alucinação
Que é um dos grandes
Problemas hoje
Nível enterprise
De
De NLMs
Para que você tenha
Inclusive
Confiança em cima disso
Né?
Então
Você consegue injetar
Conhecimento
Então cara
Imagina
Os casos de uso
Que você consegue desbloquear
Entendendo o reg
Você consegue pegar
Um modelo
Relativamente menor
É um
Um modelo
Que não precisa
De tantos bilhões
De parâmetros
E você consegue
Dar para ele
Uma ferramenta de reg
Um reg system
Para ele
Para que ele possa
Utilizar para fazer grounding
E para o que você vai
fazer
Possivelmente
Ele pode
Resolver o seu problema
Então
Isso vai te ajudar
Você ter entendimento
De quando escolher
O que fazer
E assim por diante
Tá?
Beleza
Então ele vai reduzir
A alucinação
Ele vai manter
O seu modelo
O seu processo
Atualizado
Né?
Ele vai habilitar
A citação
E ele vai focar
Exatamente no que
Ele é muito bom
Em geração de dados
Porque ele vai ter
Um retrieval
Ele vai ter um outro componente
Que restaura essa informação
E aí?
O que vocês acharam?
Foda?
Deu para entender?
Básica
A gente vai navegar
Ainda mais
Mas
E aí?
O que vocês acharam?
Ficou claro?
Dá
Dá para você dar tools
Para esse cara
De muita coisa
Tá?
A gente vai falar um pouquinho
Sobre isso
Mas e aí?
Deu para ficar claro?
Me fala aí
Só cinco, seis, seis pessoas?
Boa
Ótimo
Então
Vamos entrando
Vamos adentrando ainda
Tá?
Então
Qual é a grande
Qual o grande rolê, né?
A gente chama esse cara
Né?
Esse processo de recuperação
De information retrieval
Tá?
Claro
Ele vai recuperar a informação
Então
In a nutshell
Detalhadamente
É
O que que eu tenho aqui?
Né?
Eu tenho nada mais
Nada mais, nada menos
Do que prompt
Eu tenho
O retriever
Que é o finder
E eu tenho um documento
Né?
E basicamente
Eu tenho essa resposta
Ótimo
Então
Eu tenho
Input
Finder
E formato
Ou seja
O objetivo do retriever
É entender o prompt
E recuperar os documentos
Como ele faz isso?
Aí agora
Quem me perguntou ali
Alguém me fez a pergunta
Lembra?
Ah, eu coloquei os e -mails
E não estou achando
Esses e -mails lá
Por quê?
Agora você vai entender
Por quê?
Olha que coisa linda
Entendeu o internals do negócio
Então
Quando a gente fala de retrievers
Os retrievers já estão aí
Há muito tempo
Inclusive em bancos
De dados relacionais
Em Elasticsearch
E assim por diante
Para você ter ideia
O Elasticsearch
Já está há muitos anos
Ele funciona muito bem
E ele é um keyword search
Ou seja
O que que ele faz?
Isso é despedidamente, né?
Quando a gente entende o internals
Que aí fica
Ridiculamente fácil
De entender
Tudo
Tudo
Na minha opinião
Você vai lá
Criar um índice
Você entende o que que é
Você vai lá receber o dado
Você entende o que que é
Você vai conectar um ponto
Com o outro
Você entende o porquê
Então o que que acontece?
Nós temos duas formas, tá?
Que estão de fato, né?
Massivamente ampliadas
E entregues aí no mercado
Que é
Você tem keyword search
E você tem semantic search
Semantic search é relativamente
Novo
Não é novo, tá?
Mas relativamente comparado
A keyword search
O que que é o keyword
search?
Bem
É uma pesquisa
Que vai
De fato
Acertar o que?
Nas palavras corretas
Então se eu tô querendo
Um documento
Que tenha escrito
Luan
Cara
Ele vai exatamente
Tentar bater
Essa pesquisa
Com o Luan
Tá?
E aí nós temos
Dois tipos
De indexadores
Ou de características
De como você pode
Fazer isso
Você pode ir fazendo
Utilizando isso
TF, DF
Que ele faz
Basicamente um score
Dos documentos
Que ele vai
Varrer
Ou você pode utilizar
Esse BM
20
5
Que é uma outra forma
De como você vai
Fazer isso aqui
Dá pra gente entrar aqui
No detalhe de cada um
E sem sacanagem
Passar 40, 50 minutos
Falando de como ele faz isso
Tá?
Isso eu vou deixar
Pra um outro momento
Eu vou falar sobre isso?
Vou num outro treinamento
Porque vocês precisam saber
Quando você vai escolher
Um ou outro
Tá?
Mas a gente vai ver também
Que muita coisa
O Vector Database
Já abstrai pra você
E é maravilhoso
Tá?
Então a gente vai ver isso
também
Então só pra vocês terem ideia
O algoritmo
Que é usado
Do Elasticsearch
Cara
Que é uma das engines
De pesquisa
Que utiliza Lucine
Por debaixo dos panos
Que cara
Enfim
Tá aí
Teve no Google
Durante muitos anos
Exatamente
É o BM
25
Agora
Do outro lado
Da fronteira
Nós temos
O Semantics Search
Que é documentos
O que?
Com significados
Similares
Então ele tem
Um entendimento
Semântico
Em cima disso
Porque tem um embedding
Porque tem um vetor 3D
Porque tem a similaridade
De onde as palavras
São colocadas
Dos pares opostos
Dos pares opostos
Dos pares
Que estão juntos
Tudo aquilo
Que a gente viu
Tá?
Então esse cara
Que de fato
Entende o processo
E aí
Eu tenho duas métricas
Importantes
Pra eles
Que a gente vai ver muito
Acho que você já deve
Ter ouvido falar
Em Reranker
E assim por diante
Vem justamente
Porque é uma pesquisa
Semântica
Tá?
Então você tem
O TopK
Que é baseado
Em score de similaridade
Então quando ele faz
Essa pesquisa
Quando o Retriever
Vai lá na base
De conhecimento
Pesquisar
Que nesse caso
Vai ser o nosso
Vector Database
Ele vai ter um algoritmo
Que ele vai calcular
A similaridade
E score
Em cada um dos documentos
Que ele faz
E basicamente nisso
Ele vai trazer
Esses documentos
Pra você
E você tem o MAP
Que é o
Mean Average Precision
Que valida
A efetividade
Dessa busca
Então durante
Algum tempo
Nós tínhamos
Uma opção
Ou outra opção
Né?
Ou eu habilitava
Um índice
Ou uma forma
Do Retriever
Buscar por
Keyword search
Ou eu habilitava
Esse cara
Pesquisar por
Semantic search
E obviamente
Que muita gente
Pensa
Cara Luan
Semantic search
Vai ser muito melhor
Do que Keyword
Porque é na semântica
E a resposta é
Você está
Redondamente enganado
Depende
Do seu caso de uso
Depende
Como você consulta
Depende
O significado
Depende
Do dado
Que está dentro
Do seu sistema
Então não tem como
Você ter um
One size fits all
Tá?
E isso é um problema
Tá?
É
E o que a Clarissa
Falou é verdade
Eu li isso mesmo
Existe aí
Uma dica
De diferença
De tempo
De Keyword search
Para busca semântica
Muito bom
A Keyword search
Ainda é mais rápida
Bem mais rápida
Mas tratando de imagem
Acredito que a semântica
Search ainda vai melhorar
Muito bom ponto
Clarice
De fato é
Semântica search
Ainda não é
Né?
Eu diria
E obviamente
Porque é um campo
Novo e muito mais complexo
Nem se compara
Mas sim
A Keyword já é muito mais
Estabelecida no mercado
Essa função
BM25
Cara
Ela é utilizada
Para tudo
Né?
Que é um campo novo
Existe muito para explorar
E o que é relativamente
Interessante
Para mim
É que os
Vector databases
Podem fazer melhor
Do que isso
Eles podem fazer híbrido
E eu acho isso
Muito foda
Tá?
Então
Alguns
Vector databases
Eles possibilitam
Você ter uma pesquisa
Híbrida
Tá?
Então
Isso é muito interessante
Tá?
Então
Basicamente
A sua base
De conhecimento agora
Com o retrieval
Conta com duas formas
Né?
Ou ele conta
Com Keyword search
Ou ele conta
Com Semantic search
Então imagina
Que ele recebeu
Essa consulta
Do lado
Do Keyword
Ele verificou
Que ele fez o grounding
De quatro documentos
Ele
Isso aqui
A gente não tem tempo
Para entrar aqui
Mas aqui é uma outra característica
Que vai fazer com que
O seu sistema
Por exemplo
Vocês perguntaram
Para mim de token e tal
Né?
Isso aqui
Poucas empresas
Utilizam
É a realidade
E isso aqui
Detém
30 a 40 %
Da velocidade geral
Da sua pesquisa
Utilizar
Metadata filter
Quando você escreve
Um documento
Dentro do seu
Vector store
Você pode
Dar
Yultros de metadado
Você vê isso funcionar
É tipo
De pirar a cabeça
E aí você
Instrui
Na verdade
Quando o retrieval
Vem buscar essa informação
Você instrui o metadado
E ele adiciona
Um outro nível
De metadado
Para você
E ele consegue
Fazer um grounding
Extremamente mais efetivo
Tá?
Então daqui
Ele entendeu
Antes de entregar
Que ele aplicou
O search
Ele aplicou
O filter
E ele limou
Dois dos documentos
De relevância
E trouxe aqui
Documento A
E documento B
Do outro lado
Por nível semântico
Por similaridade
E aí vem esse cara aqui
Que é o nível
De similaridade
Que é o algoritmo
Que você está utilizando
Para fazer
Cozine
Ou dots
Você vai ter que saber disso
Principalmente em certos tipos
De vectors
Ou de pesquisas
Que perguntam
Em qual tipo
De similaridade
Você quer fazer
Esse cara vai aplicar
Aqui
Um filtro novamente
E vai ver também
Que o B
O X
E o Y
De fato
Sabem
Tá?
E aí disso aqui
Ele vai falar
Cara a relevância
É o B
Então ele retorna
Para você
Esses dois
E aí ele utiliza
Um re -ranking
Né?
Para reordenar
E reorganizar
Esse dado
Se necessário
Nem sempre é necessário
Dependendo de como está
Eu habilito
O re -ranking
O que é re -ranking?
É re -rankear
Né?
Então como que eu re -rankeio
O negócio?
Cara
Eu aplico o re -ranking
Para que de fato
Eu consiga ter
Um outro passo
Um segundo passo
Para que eu possa
Ter um brushing
Né?
Para eu realmente falar
Cara
São realmente
Esses documentos
Muita gente habilita
Esse processo de re -ranking
Mas na real
Nem sempre
É necessário
Tá?
Não quer dizer
Que é necessário
É muito importante
Mas não necessário
E a gente vai descobrir
Tendo métricas
Alguém lá no começo
Perguntou hoje
Né?
Poxa
Como que eu sei
O que fazer
O que implementar
Quais são os próximos
Quais são os próximos pontos
Yara yara yara
Né?
E aí os próximos pontos
Que você vai ter
Que entender de fato
É exatamente
Quando aplicar
Uma técnica ou não
Baseado no seu entendimento
Você só vai conseguir
Fazer isso
Se você entender
Os internos
Se você entender
Os fundamentos
Se você entender
Como esse negócio
Funciona debaixo do capô
Aí você vai ter
Propriedade de fato
Para poder fazer isso
Tá?
Ótimo
Perguntas?
Certo?
Nenhuma pergunta?
Nenhuma pergunta?
Nenhuma pergunta?
Não aqui é internamente
Omar
Como ele vai fazer
Essa pesquisa
Você pediu lá
Para ele lá
Doelo Swift
Aqui é por debaixo
Dos panos
Como o Retriever
Vai receber
E aí vai olhar
Um mecanismo
De busca
Por que isso aqui
É legal?
Porque eu não estou
Falando de
Vecas
database?
Eu estou falando de
cara, eu estou falando
de genérico, né?
Então, exatamente
o Fernando falou, olha, esse hybrid
search já é abstraído nos vector
databases
ou é preciso implementar? Ótima
pergunta. Você pode
implementar na mão, você tem
algoritmos que fazem isso, ou você
tem, por exemplo, um vector database,
dependendo
do vector database, que já te
dá isso aí.
E aí você começa a balancear
a preço, custo, tempo e assim
por diante.
Então, tem vectors que já
de fato te entregam
isso, tá?
Boa. Então, beleza.
Na verdade, é o
oposto, né? Ele adiciona,
ele tira essa limitação
e te entrega. Obviamente que
se feito da forma correta, tá?
Aí, de fato, ele vai
te ajudar.
Beleza.
Muita coisa, né?
Muita coisa.
E eu sei que
é muita coisa para digerir, até
porque
quando eu estava trazendo esse conteúdo,
o meu problema é, cara, até
quanto a gente abre o
capu, até quanto a gente
não faz e assim por diante,
tá?
Então,
então,
é...
Aqui, eu acho que é o
número perfeito
para que você possa fazer isso.
Reg pode ser usado para já
gravar algo que já
classificamos, tipo, no sentido de reduzir
novas chamadas da LLM para algo
já anteriormente feito. Você pode
obviamente treinar certas
coisas e pesquisar nessa
base para que você reduza exatamente
repetições. Então, você consegue fazer sim.
Tá?
Beleza.
Agora, vectors, né?
Então, o vector
existem vários aí no mercado,
tá? Todos têm
os seus pontos positivos e negativos
e tá tudo certo.
Um cara muito bom é
o AVH. Ele é bem
falado.
Tem o Supabase,
que é bem falado
e bem usado também.
Tem o Quadrante, que é bem
usado.
Tem o Milvus também.
Tem o Chroma.
Existem vários legais
aqui, tá? O MongoDB
também implementa agora.
Esse cara aqui, ele é um
cara
bem avançado. Ele
é Postgres por debaixo dos panos.
Ele utiliza muita coisa
interessante. Ele tem já por
debaixo dos panos
Hybrid Search,
se eu não me engano.
Então,
, ele é um cara bem
usado, né?
usado, né?
Olha só que legal. O AVH,
sim, provê
capacidades off -the -shelf, é
open -source, tá? Então, você tem
busca semântica, com similaridade aqui
já trago pra você.
Tem Hybrid Search, que implementa
BM25, que é o mais, de
fato,
é...
interessante.
Super caso de uso pra
retrieval, tá? Pra trazer relevância
desse cara.
Exatamente o mundo agrônomo. A gente
tá no
meio da ciência de dados e
da
engenharia de dados. A gente tá
exatamente
nesse campo aí, no meio do
ML Ops Engineering. A gente tá
no meio de tudo.
No meio da engenharia de software.
Foda. É exatamente isso.
A gente tá navegando um pouquinho
aqui. A gente tem o Superbase.
Cara, eu adoro o Superbase.
O Superbase é postless também.
Ele trabalha tanto
o armazenamento como o Vector.
Então, ele tem umas coisas muito
legais. A UI
dele é muito fácil.
Esse aqui é usado muito
pra quem trabalha com automação,
N8n, integração, essas coisas. A galera
adora
utilizar ele, porque ele é fácil,
ele é rápido,
ele é intuitivo, ele tem uma
Web UI muito fácil de usar.
É muito interessante esse cara.
Eu gosto bastante dele. O quadrante,
acho que o Matheus vai mostrar
alguma coisa bem legal de quadrante.
Ele também é muito bom. Tem
umas características
super interessantes, tá?
O Miurus também, interessantíssimo.
Open Source
também.
E nós temos também o Chroma.
O Chroma
na verdade, me surpreendendo
bastante, ultimamente,
porque eles lançaram algumas características
legais que eu vou mostrar pra
vocês, que
pra mim, são muito disruptivas e
eu vou dar
um dado pra vocês aqui, que
vai valer muito mais que
mil reais. Sobre o que
não cometerem, que são as experiências
do Field, galera.
Tá construindo isso aqui. É Field.
Tá? Então, Chroma também
muito bom. Tem umas coisas muito
legais acontecendo
no ecossistema de Chroma.
Eu vou falar pra vocês o
que é.
Existem vários bancos de dados interessantes,
de vetores aí. Beleza?
Então, o que eu preciso pra
fazer esse cara
funcionar? Então, basicamente,
pensando ali
em simplificar esse processo, eu tenho
configuração do Database,
criar a coleção, de fato
configurar esse cara, tá?
Depois nós temos carregar
esse dado, indexar essa informação,
e por fim, performar
pesquisas. Ou seja, essas duas partes
são offline, que a gente chama
de
pré -processamento, no RAG.
E essa terceira parte
é a parte online, onde eu
vou
pesquisar essa informação.
Beleza? Olha que coisa linda
que eu vou trazer pra vocês
aqui, ó. Colinha.
Colinha, colinha. Eu vou trazer aqui,
ó.
Beleza. Então, como
funciona esse processo, né?
Ele acontece
em dois pedaços aqui.
Pedaço de feeding, de alimentação,
como que ele vai ser alimentado.
Você tem
os arquivos, você vai
fazer o chunking desses arquivos, e
é a primeira
vez que a gente tá falando,
de fato, de chunking.
E aí, Luan, o que que
é chunking?
Né? Chunking é quebrar
em blocos
menores. E a primeira pergunta
da galera é o seguinte, Luan,
por que que eu preciso
quebrar um documento
em blocos menores?
Resposta, vamos lá,
gente, no chat. Por que que
vocês acham que eu tenho que
quebrar um documento
muito boa, Rafael, em Minas?
Chunk é um pedacinho.
É isso aí mesmo.
Um pedacinho.
Processar em paralelo? É,
também, tá? Nem todo
documento é relevante pra busca? Muito
bom.
Indexar e agilizar a resposta? Também.
Pra não precisar ler tudo
e enviar. Perfeito. Aqui, o Márcio,
ó.
Pra não precisar ler tudo e
enviar um
pedaço gigante pra LLM ver o
que precisa.
Pra desonerar a fase.
Pra indexar. Agora, vamos ver se
a gente vai falar
foda. Nós estamos falando disso aqui
Quanto tempo a gente tá nesse
workshop, Matheus?
Quatro horas?
Cinco horas? Mais ou menos.
Beleza. Cês vão ter um insight
agora
muito foda, que é uma
coisa que é tipo
underlooked
total.
E cês vão entender. Bem,
vamos parar pra pensar o que
a gente acabou
de ver aqui. O que a
gente acabou de concluir.
Né? A Morgana
acabou de colocar melhorar o contexto
pra virar o vetor.
Token e tal. Bem, então,
se eu pego um documento
e eu não faço chunking
correto, ou
eu não dou importância
pra o chunking,
cara, cê concorda
que o grounding vai custar mais
e que o token, o LLM,
vai custar
mais? Vai. Por quê? Porque dependendo
de como você quebrou esses chunkings
aqui, você vai trazer
mais, menos ou correto
pra dentro do contexto.
E aí, eu vou dar um
dado pra vocês aqui, que eu
quero ver todo mundo
falar foda. Isso é um dado
real,
tá? De conversas, de
cabês, de artigos, de tudo.
Olha isso aqui.
A técnica
de chunking embedding correta pode reduzir
40 a 60 % de
custo de token.
Cara, isso é foda ou não
é foda?
Mas cê já pensou nisso ou
você só ficou
reclamando que o token tá caro?
É, isso é
real, tá?
E o grounding, melhorando de 2x
pra 4x, especialmente em
RAG. Ou seja, pesquisa mais
rápida, mais efetiva com menos token.
Puta que pariu.
É isso.
Ou seja, é simplesmente
fazer por fazer ou é entender
a mecânica?
Tá?
É entender a mecânica.
Tá?
Então, galera, tudo faz diferença.
Entender faz diferença.
Fundamento faz a diferença.
Porque agora, quando você for
começar os seus projetos, no dia
que isso for
necessário, você vai começar certo.
Você vai falar, opa, eu tenho
que fazer
chunking e eu tenho que fazer
embedding.
Tá? Então, calma aí.
Antes de eu pensar em várias
outras coisas
e eu vejo a galera falando,
cara,
custo de LLM é caro e
não sei o que. Beleza,
tem casos que realmente vai ser
caro, não vai ser viável.
Óbvio. Mas não é a maioria.
É a minoria, tá? Porque
tem outras coisas que você pode
olhar aqui
que vai te salvar muita coisa.
Então, tem um artigo muito
foda do próprio Chroma
que, cara, esse aqui, se você
quiser rasgar
no entendimento de chunking,
é só esse que você precisa
fazer.
É o research do Chroma. Eu
vou deixar ele aqui no link.
Tá?
Esse cara, cara, eles fizeram
um report extremamente
detalhado
das formas de chunking
que você tem e, além do
mais,
trouxeram novas formas de chunking
pro gaming. Tá? Então, aquele
ele compara, cara, todas as melhores.
Ele explica o processo de chunking.
Cara, Luan, você leu
tudo isso aqui? Cara, eu li
80%.
Tá? Então, eu passei muito
tempo lendo isso aqui, vendo o
que eu ia trazer
pra cá e por aí vai.
Então, ele explica
a métrica, explica, cara,
fórmula matemática. Eu sou burro demais
pra entender isso aqui. Mas aí
ele explica,
cara, quais foram as formas que
eles
acharam, como que eles otimizaram,
qual o tipo de token de
redução.
Olha isso aqui. Tá? Então, o
nível
de tokens que eles economizaram.
O algoritmo que eles trouxeram, por
exemplo.
Esse é um dos caras que
a galera não usa
muito, mas é um dos mais
utilizados
nível 2. Que é o Recursive
Character
Tax Splitter. Tá? Eles
trazem também esses caras novos aqui,
ó.
E trazem, por exemplo,
hoje dá pra fazer LLM Semantic
Chunker,
o que não dava pra fazer
antigamente
porque é muito caro. Hoje já
não é. Dá pra ter
um modelo pra poder fazer isso
pra você.
E aí ele explica aqui vários,
tá?
Foda. É um artigo muito
foda de ler.
Eu recomendo. Se você quer
realmente entender cada vez mais sobre
isso aqui, recomendo leitura.
E tem também um
outro artigo que eu acho muito
legal, que
explica as características de chunking
que é esse cara aqui no
Medium. E eu deixei aqui. Basicamente
ele
explica
as características, os mais
utilizados, teoricamente, de chunking
como você vai quebrar esses caras.
Então, se eu for pensar
em começar a construir um RAG
de fato, o eficiente, a primeira
coisa
que eu vou pensar é pensar
como
eu faço chunking e embedding, tá?
Então, qual é o embedding
model que eu
escolho e qual
é o chunking que eu escolho.
E obviamente que aqui
não tem uma size fits all.
Nem sempre somente um
tipo de chunking vai resolver pra
um tipo
basicamente pra tipos de arquivos diferentes
você vai ter que, muitas das
vezes, utilizar
métodos de chunking diferentes.
Por isso que a parte de
ingestão,
de tratamento de dados, de sanitização,
de tirar caracteres específicos
e assim por diante, é muito
importante
e por isso que cada vez
mais engenheiros
de dados vão continuar ganhando dinheiro.
Porque nós precisamos
tratar dados no pipeline de engenharia,
a gente precisa sanitizar essa informação,
a gente precisa
apropriar essa informação
dentro da característica de
engenharia pra alimentar o RAG.
Então, hoje na Databricks
você já faz isso. Então, hoje
se você pegar projetos que trabalham
com
Databricks, cara, obrigatoriamente o cara
utiliza a arquitetura medalhão pra poder
trabalhar o dado inteiro e jogar
dentro do Vector Database sanado. Então,
o engenheiro de dados não vai
longe.
Ele não vai sumir. Muito pelo
contrário,
ele vai começar a ter outras
funções
muito mais importantes
pra trazer um resultado muito mais
significativo
do que ele faz. E o
RAG
é uma delas.
Agentes é uma delas. Então isso
vai acabar
viva nós, caralho.
Então isso vai ajudar bastante.
Tá? Então,
nós temos aqui
os tipos diferentes. Existem vários
de novo. Vale a pena vocês
olharem.
E aqui
nós temos também os algoritmos de
Embedding.
E eu vou contar um caso
interessante
que a gente passou
lá na Pythian, que não fui
eu
que trabalhei nesse projeto,
mas foi uma das pessoas do
meu time.
E cara, a galera tava
usando esse ADA002
com Fixed Chunking.
Tudo bem que Fixed Chunking, se
você olhar o cara
você tem que dar uma voadora
no peito dele.
Mas assim,
e os caras trocaram o Embedding
pra Small e adicionou Semantic
Splitting e o cara cortou
43 % de token.
Você tem noção do que é
isso?
A gente tá falando de redução
de quase 50 % de token.
Assim, é muita coisa.
É muita coisa. Ou seja,
você olhou pra isso aqui primeiro?
Não.
Então, de novo, talvez seja um
lugar
pra você começar a pensar
trabalhando esses caras aqui.
Tá? E a gente vai
e nós iremos aqui olhar
rapidinho algumas estratégias de Chunking
e como isso funciona pra que
vocês
consigam entender.
De novo, existem várias
formas de fazer isso. O Jefferson
perguntou
quais são as características.
Cara, vale muito a pena ter
uma relação
do que cada um faz, então
eu trouxe aqui
umas coisinhas pra vocês.
Vocês conseguem ver minha tela aí?
Do Data Spell?
Tá?
Tudo isso tá no repositório.
Tudo isso tá no repositório, tá,
guys?
Tá tudo lá no repositório.
O documento que a gente vai
utilizar aqui,
nós temos um documento
chamado Doc Datasets que explica
toda a documentação e os datasets
do Uber Eats,
como eles são classificados e assim
por diante.
A gente vai usar esse cara
pra aplicar
as estratégias aqui e eles estão
aqui.
Beleza?
Manda o link.
Deixa eu mandar o link desses
dois caras aqui.
Os dois links.
Beleza.
Então aqui, o que que a
gente tem, né?
O que que nós temos?
Nós temos cinco.
Então nós temos o
Fixed Size.
E aí, cara,
esse cara aqui, ele é muito
usado, tá? Ele, na verdade,
é um dos mais usados. Se
você
perguntar pra esses caras aqui, ele
é
completamente under looked, essa
parte de chunking. A galera
simplesmente divide chunking
pelo Fixed Size e chunking
com overlapping, que a gente vai
ver
agora e tá tudo certo. Então
a primeira
coisa que você vai começar a
olhar dentro de um sistema
de RAG é o seguinte, cara,
o que que você tá usando
pra fazer chunking?
Né? Então o primeiro ponto pra
você
olhar é que, definitivamente.
A gente vai olhar
recursivo, semântico,
tá? Que tá muito na moda.
Language based, contextualware
e tem um outro cara muito
legal que é você
utilizar o próprio LLM pra fazer
isso.
Antigamente não dava pra fazer porque
fica muito caro,
obviamente, mas agora dá pra fazer
com outro
modelo específico, tá? Então aqui
eu tô lendo esse documento,
só pra gente ver aqui, né?
Esse documento tá aqui, né? Lê
os primeiros
atributos aqui. Fixed Size
Chunking, né? Então
essa estratégia permite que você
quebra o documento em pedaços
fixos, né? E opcionalmente
com overlapping, tá?
E aí por que que a
gente usa overlapping
aqui? Quem quer me falar por
que que a gente usa
overlapping dentro de
um chunk size? Então o que
que eu tô falando?
Eu tô fazendo, né? Eu tô
quebrando
o meu documento
em chunks de 500
caracteres, mas eu tô fazendo um
overlapping de 100. Por que que
eu faço
esse overlapping de 100?
Uhum!
Exatamente!
Porque você quer manter o contexto
do negócio, né? Então
cara, um dos problemas é que
você adiciona,
né? Qual o problema que resolve?
Né? Te ajuda a de
fato você relativamente
né?
Manter o contexto dentro daquele chunking
que você pesquisou sem ter que
ir pra um outro
chunking e evitar o
grounding, né? Mas
vai te repetir. Então muita
gente coloca, cara, um overlapping de
100,
um overlapping de 200, então
essa é uma das características mais
utilizadas
que você tem normalmente aí,
tá? Então você vai fazer o
overlapping
desse cara. Então, por exemplo, se
eu fizer um overlapping
de 10, que que a gente
vai
ver aqui?
Ops!
É que eu trouxe, é, menos.
Tá? Então eu posso ter um
overlapping de 100,
eu posso ter um
overlapping de 300,
tá? E aí você
decide como você vai fazer isso,
porque
é fixo, então não tem muito
o que fazer.
Esse é o tipo Vanilla, né?
É o sabor que, cara,
pegou, vou fazer chunking, beleza. Divide
pela quantidade de caracteres,
mete um overlapping e tá tudo
certo.
Então tomem cuidado,
não façam isso, evitem trabalhar com
isso,
a não ser que você queira
testar P ou C e assim
por diante, mas a ideia é
que você seja muito mais,
que você seja muito mais
cuidadoso
com o chunking, que a gente
viu que
isso aqui tá muito determinístico, não
somente
na gravação, mas na hora de
recuperar
esse cara, beleza? Bem, aí
a gente tem o recursivo, tá?
Que basicamente ele é
bem inteligente, tá? Ele tenta
fazer algumas coisas aqui, ó, ele
tenta
dividir esses caras por
tipos de separador, ele tenta ver
verificar se o chunking é grande
recursivamente, ele quebra em pedaços,
ele tenta preservar
o contexto, tá? Então ele é
um
cara que basicamente é um dos
mais
utilizados depois que a gente passa
desse cara de fixed size chunking,
tá? Ele é mais
complexo, né? Obviamente ele
vai resultar em tamanhos que
variam,
né? Que faz a divisão
disso, então variam, então o problema
aqui é que talvez se pode
acessar um
chunking maior ou um chunking menor,
então a gente
procurou achar um balanço sobre isso,
né? É difícil, mas definitivamente
ele vai ser mais eficiente do
que
o size chunking lá, tá? Então
aqui ele vai tentar quebrar esse
cara
de uma forma muito mais eficiente
e guardar isso dentro de um
chunk.
Aí a gente tem semântico,
né? Estratégia baseada
em sentença, então ele vai tentar
entender
ali, ele vai utilizar uma biblioteca
de
linguagem natural pra tentar entender isso
e de fato quebrar,
né? Então aqui a gente tá
utilizando um transformer,
por exemplo, ó, pequeno pra tentar
entender essas características pra fazer
isso e daí você tem uma
quebra
de informações mais eficiente.
Ele é bem mais demorado relativamente
obviamente pra fazer,
só que ele vai te trazer
uma,
um aproveitamento muito mais interessante.
Inclusive eu deixei uma lista depois
de comparação
entre eles, tá? Aí a gente
tem, é...
acho que eu repeti aqui, não,
semântico e chunking,
semântico e chunking, ah tá, não,
acho que eu repeti
esse cara aqui. Não, só não
é
chamado esse cara. Tá, aí aqui
eu
tenho
baseado em sentença e caractere,
tá? Então aqui ele utiliza um
outro
tokenizer, que é esse punkt sentence
tokenizer pra fazer isso,
preservar a estrutura semântica,
bom balanceamento de simplicidade e preservação
e assim por diante, né? Trabalha
bem com LLMs ou vector databases
e context aware,
né? Então esse cara aqui faz
tipo um slide
windowing pra tentar fazer.
Ah, Luan, qual que eu sei
que é o melhor pra mim?
Né? Então vai depender muito
do estilo do documento,
vai depender basicamente do
que você quer guardar, qual é
o seu
vector database, mas eu
começaria explorando esse cara
aqui, ó.
O recursive, tá?
Ele seria seu primeiro ponto de
início, né? Claro, sem ser o
primeiro
que é simplesmente dividir
por um caractere fixo, tá?
E daí você vai evoluindo caso
você entenda no seu sistema
que o seu chunking
está, não está otimizado
para os tipos de pesquisa que
você faz.
Luan, como que eu faço isso?
É aí que você traz observabilidade
para o negócio. Você tem que
ter observabilidade
para entender na hora em que
o seu chatbot ou na hora
em que
alguém consulta essa informação do seu
vector, você tem que entender, beleza,
aonde está o problema? Está no
retriever?
Não, não está.
Beleza, onde está o meu problema?
Está no grounding,
né? Não, não, não está. Meu
problema
está onde, especificamente? Então a ideia
é você
tentar identificar aonde
esse cara está. Tem como facilmente
fazer
isso? Cara, se você tiver observabilidade
tem como você fazer isso com
certeza,
tá? Então aqui são os
diferentes tipos de retrievers.
Ótimo, então a gente
falou sobre tipos de retrievers, de
embeddings, de técnicas e assim por
diante. Ótimo? Ótimo.
Agora, eu vou trazer
o nosso querido
Matheus para falar de como
você operacionaliza, agora
começa a parte do negócio doido,
né? Como
que você operacionaliza tudo isso, né?
Como que a gente na verdade
vai
unir essas soluções que a gente
está vendo aqui,
testando cada componente. Então a gente
viu ali uma solução mais
abstrata, depois a gente viu ali
técnicas de prompting, depois a gente
entendeu chunk embedding. Como que agora
a gente vai unir tudo isso
para criar uma
solução, tá? Então o Matheus
vai trazer o nosso maravilhoso
Langchain, tá? Ele vai explicar
para vocês e é um dos
caras que você
é tipo, na minha opinião,
tá? Spark para engenharia,
Langchain para GNI. Você tem
que entender esse cara porque tem
muita
coisa que é, né? Ponte
dele, né? Então é importante você
entender
esse cara e eu vou passar
aí
para o Matheusinho.
Eu vou falar de Agno depois
também.
Vai lá. Calma gente,
vocês estão muito lá na frente,
calma. O Agno é novo,
vamos falar do que está aí.
O Agno acho que foi ontem
que lançou, não estou brincando não,
mas ele é muito novo.
Vocês estão muito lá
na frente.
Deixa eu ver...
Bom gente,
vamos lá, vamos
contextualizar. Eu vou ser mais
direto com vocês, acho
que a ideia aqui é a
gente
focar em Prompting Reg, a gente
trouxe a
ideia do Langchain
como um Agindo para vocês verem
algumas coisas,
tá? E trocar e também
pensar, fazer o Reasoning, vamos começar
a usar os termos que a
gente está aprendendo agora, né?
O Reasoning é para a gente
tomar as decisões e também
tomar algumas, pensar
algumas criações que a gente pode
fazer eventualmente,
tá? Então o Langchain
foi criado como um framework
para desenvolvimento. Eu vou dizer que
ele voltou
muito para a gente, mas eu
diria o Diagnome em
geral, tá?
O Langchain, ele está no mercado
já há um certo tempo.
Algumas empresas gigantescas
já usam, tá?
É baseado em Python, então
nós de agilidade, nós temos uma
familiaridade
em trabalhar com esse cara, tá?
Existe ali prós e
cons, eu sempre falo que
existe, tudo depende do seu
caso de uso, mas esse cara
aqui,
eu considero como um must para
vocês entenderem.
Matheus, eu vou pegar o que
seja
mais UI,
mais gerenciado, mais pronto no meu
caso de uso,
tudo bem, mas entenda a necessidade
desse cara aqui, o que ele
pode te ajudar,
principalmente em questão de flexibilidade,
tá? Os desenvolvedores
adoram trabalhar com
desenvolvimento, linguagem, poder ter
flexibilidade, o que é isso que
te dá, né?
Eu posso, praticamente,
eu posso
estender o meu caso de uso
conforme a minha necessidade.
Gente, eu vou deixar o chat
aberto aqui,
senão eu fico perdido.
Então,
tá? A ideia do Langchain
é justamente
ele trazer numa camada mais
de alto nível, toda a parte
de abstração
que a gente tem, a parte
de como é que eu vou
invocar modelo, como é que eu
vou trabalhar
com histórico de mensagem,
tudo isso, pensa de novo, gente,
é Python.
Então ele foi feito pra
ser fácil, né? Ser um desenvolvimento
de nível mais fácil
pra gente poder trabalhar, pra gente
trabalhar com
a parte de reg, então eu
vou,
de novo, tá gente? A gente
vai entrar ali
num caso simples, eu vou entrar
em três níveis de movimento com
vocês ali e a gente vai
conversar sobre eles, tá? Eu vou
mostrar aqui
da prática. Um caso bem simples,
tá? Uma chamada
simples. Sempre começam,
isso é muito importante, sempre
comecem pelo simples,
não tentem fazer algo
muito avançado,
já com uma ocorrência que vocês
estão iniciando
agora. Ah, eu vou já
desenvolver Skynet. Não,
cara, vamos fazer primeiro ali, ó,
conversaçãozinha, vamos
familiarizar com o ambiente,
né? Tem muita coisa nova
no mercado, tem muito conceito
antigo encapsulado em coisa
nova, então tem muita coisa
que confunde, tá?
Então, de novo, vamos
com calma, sempre, vamos
na manha pra gente não
perder aqui o nosso processo.
E aqui a ideia, gente, de
novo, é eu
ser um pouco mais prático, tá?
Eu vou ser
mais direto
ao ponto com vocês, porque a
gente,
a ideia aqui é a gente
mostrar
o Language Training como uma solução
de desenvolvimento, então eu vou mostrar
rodando aqui, a gente viu algumas
coisas
acontecendo, né? Então eu vou reutilizar
o que o Luan tá fazendo,
justamente pra gente mostrar o ciclo,
né?
De vida desse processo.
Então deixa eu descompartilhar
aqui, porque eu queria ir,
eu vou direto ao assunto, eu
vou mostrar código,
né? Todo mundo, todo mundo, todo
treinamento
fica, eu quero ver código. Código,
código,
código. Então, peraí,
deixa eu só
aumentar minha fonte, que eu acho
que vocês não vão gostar da
atual fonte,
que tá bem pequenininha.
Compartilhar minha tela novamente.
Pessoal, o tamanho tá bom?
Como é que tá aí? Vocês
estão vendo
bem? Então,
ok, alright, alright,
alright, beleza.
199 pessoas, por favor,
não interajam comigo, não fiquem
tímidos,
tá? E eu vou por parte
a parte, igual eu comentei, beleza?
Primeiro, situação de biblioteca,
né? Com qualquer tipo de
sistema, qualquer tipo de desenvolvimento
em Python, né? Então a gente
vai trabalhar
aqui com, eu vou usar
o ChartCBT, tá?
OpenAI, pra eu poder consumir o
LDM deles, porque eu não vou
trabalhar com o LDM local,
vou consumir o deles,
tá? Eu tô usando o ChromaDB
aqui pra quando a gente entrar
na parte
de, ah, desculpa, tô usando o
Pinecone aqui pra na parte de
Regulator Base, quando a gente entra
na parte de Reg. Tudo simples,
tá?
A ideia aqui, eu trabalhei no
simples
aqui, a gente fez um código
simples pra vocês
poderem reproduzir com calma,
testar, de novo,
façam da maneira correta,
devagar,
pega um bloco, entende esse bloco,
pega outro bloco, entende outro bloco,
não tenta fazer,
não tenta imaginar a casa
pronta, não. Tenta imaginar
o meu tijolo. Depois,
como é que vai ficar o
piso?
Como é que vai ficar as
colunas?
Depois, paredes,
o teto. Vamos fazendo por parte
a parte, pra vocês também não
ficarem
perdidos. A gente vai ver muita
coisa aqui
que é interessante, pelo fato
de, hoje, eu tava até
discutindo isso com algumas pessoas,
já sabe, tem até
pessoas aqui da equipe, né?
A gente trabalha em projeto americano,
e uma
das discussões que a gente tem
é como a gente consegue
trazer valor com IA
pra processos internos, tá? Alguém perguntou
de bocas internas, então a gente
tá fazendo muito
desse processo, atualmente,
justamente pra o quê?
Pra ganhar valor internamente primeiro,
fazer a prova de conceito interna,
e aí
posteriormente isso pode virar um produto.
Então existe por trás de tudo,
tá gente?
O americano é um
excelente pra gente trabalhar em nível
negócio,
tá? Eu tô falando negócio
porque é o que eu tô
interagindo atualmente.
Justamente pro pensamento
business, como é que a gente
consegue transformar
coisas que normalmente a gente acha
que é simples
em algo que pode ser rentável.
Então é bem interessante isso,
e IA tá sendo algo que
todo mundo
tá olhando pra isso.
Explodir.
Então, vamos lá.
Aqui no nosso repositório
tem tudo, tá gente? Toda parte
de
documentação tá aqui,
a instalação toda
tá aqui, então deixa eu só
gente,
não vamos riscar o
vocês vão atrapalhar
o coleguinha.
Agora eu só não sei onde
que tira, que depois eu tiro.
Eu tô gravando.
Aqui, achei.
Vamos tomar cuidado na hora
de rabiscar aí, né?
Porque senão vocês atrapalham a explicação.
Então vamos lá.
Beleza. Toda parte do Readme aqui,
todas as instalações, todas as bibliotecas
que vão ser
usadas, tá? O link do repositório
vai ser entregue no final, tá?
A gente pode ficar tranquilo.
E dentro das gravações também vão
ter o link do repositório.
Então a ideia aqui é a
gente fazer toda a
instalação, de pacotes,
todo o ambiente local, não tô
usando
nada aqui de infraestrutura robusta.
De novo, a ideia aqui é
a gente
mover,
rodar, ver aquilo funcionando,
ver como é que eu aplico,
pensa em casos que a gente
pode estender, tá?
Então é bem interessante.
Outro ponto é, Diego, eu tô
vendo
rapidamente a sua
pergunta, a sua mensagem,
umas coisas que eu acho interessante,
tá?
Eu acho interessante também no
Langchain é a quantidade de
ferramentas dentro do ecossistema, tá?
A gente vai focar no Langchain
especificamente,
mas tem o Langgraph, tem o
Langfields,
tem o Langflow, tem o Langsmith.
Então tem várias coisas dentro do
ecossistema, tá? Do Langflow
que a gente pode estar olhando
e pensando
como solução que a gente pode
estar
usando para o nosso caso de
uso.
Vamos sempre pensar em casos de
uso, tá?
Não pense em algo tão
generalista. De novo, a gente tá
trabalhando em nível enterprise,
então muitas vezes vão seguir para
esse caminho, eles vão seguir para
o caminho
que eu vou mostrar, até que
eu não vou
mostrar o Langflow, cara, ele é
sensacional,
porque de novo, apesar que parece
uma UI, você
consegue fazer muita coisa de desenvolvimento
fácil.
Então ele vai um pouco além
disso,
mas grande parte das empresas, tá?
Eles vão para esse caminho aqui
por conta do quê?
Da flexibilidade, de como que
você consegue customizar suas soluções.
Então, beleza. Tem um Readme
aqui com tudo, tá? Bonitinho,
certinho, explicando todo o processo.
Dentro da pasta src script,
eu criei aqui uma pasta chamada
smodel, tá? Um modelo
só simples para a gente ver
como é que funciona.
Então aqui nas bibliotecas do
Langchain, tá? Então eu tô chamando
a OpenAI,
eu tô usando os prompts padrões
do
Langchain, tá? Porque aqui eu não
tô passando nenhum prompt, eu vou
mostrar
aqui especificamente um caso que a
gente
vai chamar o LNM,
ele vai ter um prompt específico
que vai trabalhar em cima
disso, né? Eu vou mostrar,
eu vou pegar o caso aqui
de extração de informação
de PDF, só que eu vou
um pouco mais além do que
a gente vê, eu vou criar
algumas informações a mais,
e aqui a gente tem como
vai ser
o tipo de execução dentro do
LNC,
tá? Então, por exemplo,
isso aqui é muito, muito, muito,
muito importante quando você tá trabalhando
com a pasta
de API. Aqui quando você tá
interagindo com
o LNM, ele te pede a
temperatura.
O que essa temperatura significa,
tá? Essa temperatura significa,
na verdade, é como é que
vai ser o comportamento
daquele LNM que eu vou trabalhar
com ele. Por exemplo, se eu
quiser trabalhar
com ele, o que eu quero
fazer, que é a extração
de invoice, que é algo determinístico,
é algo mais inflexível,
eu vou de 0
a 1, por exemplo, porque eu
quero
que ele não estenda
demais
o processo dele de reasoning. Então,
eu quero que, na verdade, ele
seja, ó, essa temperatura tem
que ser a mais baixa possível,
porque eu quero que
ele seja o mais previsível
possível. Em alguns casos, a gente
vai
trabalhar com temperaturas mais altas.
O que isso significa? Quando eu
quero fazer um brainstorm,
eu quero fazer ideia, eu quero
desenvolver alguma coisa, criar,
vamos supor, eu quero criar uma
aula
específica de Kafka, porque todo mundo
geralmente
me arremete Kafka quando
eu leio pra mim, né? Então,
é normal.
Eu quero fazer uma aula específica
de Kafka
sobre Kafka Streams, por exemplo,
ou sobre Kafka com Flink, ou
sobre Kafka
com Flink e Pinot. Eu quero
uma ideia de
ecossistema, como é que eu consigo,
na verdade,
criar um storytelling desse caso. Então,
eu vou colocar a temperatura mais
alta possível
aqui dentro, tá? Então, a ideia
é o quê?
É você colocar, a temperatura
vai ser de acordo com o
que você espera
daquele modelo.
Tá? Que eu estou colocando aqui,
usando o OpenAI.
Beleza? Então, aqui vai ser
simples, eu vou ter a temperatura
7,
que é o intermediário, eu quero
uma sumarização,
né? Entre a sumarização e entre
a
parte de criativo, eu coloco
ele somente assim, ó. Eu quero
que você,
você vai receber a pergunta,
né? Você é só um assistente,
uma
pessoa, como é que eu digo?
Eu esqueci a palavra em português
tem hora, tá?
Prestativo. Você é um assistente prestativo.
A ideia é essa.
Prompt simples,
uso o padrão, não quero que
você, eu não quero
estender muito desse caso.
Quanto que é 1 mais, quanto
que é 2 mais 2 e
por quê, tá?
Esse número. Então, aqui a gente
vai
só executar, detalhe importante,
tá? Eu estou com a minha
OpenAI
Key aqui, API Key aqui
configurado, beleza? Por isso que eu
estou fazendo
o load aqui, no datain,
justamente para eu poder autenticar
lá na OpenAI, fazer o processamento
por lá e fazer o resultado
por lá de cá.
Tá?
Antônio, não, a temperatura não é
questão
de custo, a temperatura é mais
a questão
como é que ele pode, às
vezes, se você quer
um tipo de situação específica,
tá?
Pode levar o quê? Você precisa
de coisa
mais previsível.
Então, como é que é o
comportamento dele
em comparação com o caso disso
que você quer trabalhar?
Eu quero uma coisa muito
criativa, temperatura mais alta. Eu quero
algo mais específico,
mais direcionado. Então, ele não vai
ter
criatividade na resposta.
Então, eu vou criar uma temperatura
mais baixa.
Então, é racionalidade versus
a criatividade ali, tá?
Determinístico e criativo,
perfeito. Beleza?
Pedir para ele um 2 mais
2,
2 mais 2 é 4. Por
quê? Porque a soma
é de 2 pares e 2
objetos,
tá?
É a matemática fundamental.
Beleza. Ótimo.
É isso que eu queria. De
novo, aqui a gente
só fez um simples, né?
Um travessado simples.
Mas, beleza. O que acontece
aqui em muitas vezes, tá?
Aqui eu vou cobrar
token que eu estou usando, né?
Eu estou mandando com o LLM
do OpenAI.
Tá? Vinícius
que perguntou. Se eu tivesse usando
LLM local, né? Se eu tivesse
com um LLM local, não compraria
token.
Eu estou processando a minha máquina.
Beleza. O Luno
falou comigo. Matheus, então, nós
temos aqui, eu estou trabalhando com
a parte de
nota fiscal e eu quero trabalhar
isso
de forma em código,
tá? Eu quero isso codificado,
eu não quero usar UI, eu
quero flexibilidade,
eu quero tratar algumas coisas.
Eu quero fazer, por exemplo, a
instalação de Schema,
né? Então,
eu vou abrir o principal,
depois a gente vai quebrar ele
para a gente poder entender
o que está acontecendo aqui, tá?
Então, vamos
agora, vamos aumentar um pouquinho o
nível, né?
Para ficar legal. Então,
primeiro, tá? Eu trago as minhas
informações
no meu Envy. Eu começo
aqui a trabalhar os meus
dentro do meu projeto, tá? Meu
path do meu
projeto. Eu tenho que processar algumas
pastas
internas aqui. Eu vou fazer movimentação,
tá? Eu tenho aqui agora duas
pastas, tá? Eu vou mover uma
nota fiscal
nova aqui para o raw, para
a gente
poder ver que ele está vazio.
Eu tenho
aqui umas processadas. Eu vou abrir
daqui a pouco, porque
tem mais coisas que eu quero
mostrar para vocês.
Mas a ideia é o quê?
Eu vou
usar agora um loader
específico, tá? De invoice. Então, eu
vou
especificar. Olha, aqui é como
eu quero carregar qual vai ser
o tipo
de nota que eu vou trabalhar.
Então, agora eu vou começar
a tornar isso cada
vez mais complexo,
tá, gente? Quanto mais detalhes a
gente
coloca numa solução, mais complexo
ela vai ser. Eu tenho que
tomar muito cuidado
da complexidade, tá? Como é
que vai ser o encadeamento de
ações
do meu pipeline aqui dentro? Eu
coloquei
dentro aqui um chain para a
gente poder ver como é
que vai ser a quantidade de
execuções
que ele vai realizar.
E aqui eu coloquei dentro do
meu
utils duas informações, tá?
Eu quero uma validação do meu
JSON no meu esquema,
tá? Porque eu vou extrair nessa
nota fiscal não só os
dados, mas eu quero esquema
desses dados, tá? E os dados
que eu vou extrair a partir
dela,
beleza? Então, aqui eu
trabalho com algumas informações,
eu faço algumas melhorias aqui no
topo
do JSON, tá? Vou processar
o arquivo e aqui vai ser
legal. O que que eu vou
fazer
aqui nesse processo como um todo?
Toda vez
que algum dado cair na raw,
eu vou executar,
eu vou fazer essa extração
de esquema, eu vou fazer
extração de dados, eu vou montar
um JSON
com isso, eu vou movimentar o
JSON
e o meu arquivo pra dentro
da minha
raw, tá? Pra dentro da minha
process, desculpa.
Da minha process.
Então, toda vez que tiver, eu
vou sempre olhar
a pasta inteira, tá?
Dentro de, e podia ser um
bucket, podia
ser o que for, a gente
começa, aí começa
a cabeça
vai de acordo com o que
você quer
entregar. Aqui como a gente vai
poder
mostrar localmente, aqui eu vou fazer
dentro do meu próprio
projeto, entra aqui, processa
aqui, tá? Então,
vou abrir aqui agora os arquivos
pra gente poder
ver, a gente entender o código.
Então, a parte de chain, né?
Como eu
comentei. Então, aqui olha que legal,
é
aqui a parte de chain, eu
vou fazer o que?
Vou na OpenAI, por quê? Porque
eu vou invocar aqui
dentro o meu prompt.
Vou abrir os meus prompts aqui.
Então, eu vou ter
uma sequência aqui
de execuções que eu quero fazer,
olha,
eu quero que você executa dessa
forma,
você vai primeiro pegar o modelo,
você vai fazer esquematizar, depois você
vai pegar o modelo,
pegar o prompt, desculpa, você vai
pegar o
prompt, pegar o modelo, executar
e fazer todo esse processo
num candiado aqui. Eu tô criando
tudo com código,
tá gente? Aqui é código, código
mesmo.
Poderia fazer pra uma orquestrada,
poderia, mas eu quero mostrar pra
vocês
o language como um todo, beleza?
Então, aqui eu quero, você vai
criar
pra mim o pipeline usando o
GPT -4, tá? Aqui eu vou
fazer
flutuação de até 1500, aqui a
temperatura
de 0 .0, de novo eu
quero ser uma
coisa mais direcionada,
mais determinística,
porque eu tô, eu quero ter
certeza que os dados
vão ser o que eu quero,
e eu vou ter uma
sequência de prompts aqui.
Aí os meus prompts ficaram tranquilos,
tá?
Eu tenho um prompt específico,
estão vendo bem, gente? Eu deixo
eu reduzir aqui, eu não sei
se vocês estão
tão preocupados aqui, se vocês estão
caladinhos,
não precisa de atenção, não, peraí.
Então, eu criei um prompt
específico pra esquina, tá?
Então, aqui você é um especialista
financeiro,
você vai extrair dados, os campos
específicos
de uma nota e gerar isso
dentro de um JSON, onde a
gente
vai trazer o nome do campo,
o tipo e a descrição daquele
campo,
tá? É, gente,
toma cuidado aí no, no,
de novo, né, no
desenho na tela não
porque fica aparecendo na tela aqui,
tá? Atrapalha o
pessoal. Então, aqui eu vou trabalhar
com o schema aqui, vou extrair
o schema, e aqui
eu vou extrair os dados como
é que eu quero.
Então, você é um invoice,
você é um sistema de invoice
de dados, de extração de dados,
tá?
Você vai receber um JSON,
um schema do JSON, você vai
montar pra mim
esses dados. Então, aqui eu faço
também
a extração, e aqui eu faço
o projeto.
Então, eu fiz três camadas aqui
de prompt. Então, a outra camada
é, eu venho aqui e falo,
olha,
se você quer uma AI assistente
colaborativa baseada
em language chain de um invoice,
você vai analisar esses dados. Top.
Quais são os objetivos do meu
projeto?
carregar invoice
que tá lá dentro do, do,
do ROM. Então,
eu já passei por essa informação.
Você vai
ler com zona NLM
o meu dado e vai extrair
pra mim os campos, né,
de toda invoice, você vai procurar
tudo
que é campo dentro da invoice,
tá?
E aqui eu deixei uma pegadinha
que no final eu vou falar,
tá? Na hora do treinamento eu
vou falar
umas coisas bem legais, vocês podem
testar em casa.
Você vai
validar e emitir um
JSON no final representando um invoice.
Então, eu quero a schema e
o dado
da invoice lá dentro, beleza?
Current state, né? Qual
que é o seu atual status,
né?
Os loaders estão dentro
da pasta, dentro do arquivo loaders
.py,
ou seja, como é que é
a forma pra mostrar, tá?
Eu tô usando a biblioteca de
PDF
pra poder justamente carregar e fazer
o load aqui
do meu PDF de maneira otimista,
organizada, tá? Você vai trabalhar
com esses dois prompts
aqui pra fazer a
execução do pipeline
estilo runnable dentro
do link chain, então não vai
ter interação,
eu vou somente executar o pipeline
como um todo, tá? Aqui não
tem
chat, não tem nada.
Todos os meus, também vai, meus
documentos
já estão configurados, tome cuidado
pra você não truncar essas informações
e aqui pra você trabalhar
a parte de transformações, eu criei
um útil .pars pra algum tipo,
um útil .py pra você trabalhar
a parte de validação
e limpeza do JSON, tá?
Uso o contexto dado
pra ter uma sumarização
concisa do workflow.
Quanto mais detalhe,
quanto mais informação,
mais completo fosse o prompt, né?
Mais, como a gente já viu,
mais fácil vai ser a sua
atividade
a ser realizada, máximo.
Então deixou aqui,
chain eu mostrei, loader, tá?
Então eu tô usando o PDF
Plumber, justamente
com o Pandas pra ler o
PDF
e transformar aqui no DataFrame
e ali eu vou trabalhar nele
em JSON.
Difícil, hein? Passou tudo em Python
por então. Massa.
Útils, tá?
Útils eu faço algumas
algumas melhorias
no topo do meu JSON, então
eu procuro parênteses,
eu faço todo o processo de
limpeza
do JSON aqui nesse útil
justamente via validação, tá?
Esse cara é um JSON mesmo,
ele tá com
a estrutura correta que eu espero
no meu JSON, então todo o
meu
processo de validação e limpeza
eu coloquei aqui dentro. Porra massa,
né?
Então vamos rodar pra ver?
Só que aqui tá vazio, né?
Eu vou pegar
uma aqui
vamos ver quem que não tá
aqui
pra gente poder não fazer igual.
Então eu vou pegar o 2.
Modelo 2
que vocês têm acesso
vão ter acesso lá no GitHub,
tá?
O GitHub vai ter os modelos
das invoices
que vocês poderiam brincar à vontade
que a ideia é experimentar, tá
gente?
É experimentar mesmo.
Deixa eu pegar aqui
papapá papapá
Cadê?
Mudou o do load? Tá errado.
Modelo 2
Então modelo 2
modelo 2
raw data
raw blah
Então
tá aqui meu invoice
deixa eu fazer uma coisa legal
aqui
pra vocês poderem ver
pra gente falar
pô, Madel, você tá mentindo, você
tá gostando de coisa
mas não dá pra ver
deixa eu pegar aqui
aqui na tela
aqui na tela
aqui na tela
aqui na tela
estão pegando até lá mesmo
deixa eu pegar aqui
aí
aqui o que eu vou fazer
eu vou abrir
vou pegar
2 aqui
que eu nunca queria
então beleza, essa é a invoice
que eu vou trabalhar, tá?
Olha como a invoice já é
bem
as coisas não estão legais
vou trabalhar
com essa invoice aqui
invoice 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
up
3
2
2
2
2
2
2
2
2
que ele está executando.
Deixa eu abrir meu chart aqui.
Se vocês tiverem alguma dúvida, eu...
É, sempre assim.
Beleza, então ele já trouxe para
mim.
Já escreveu e já movimentou.
Já não está aqui mais nada.
Então, vamos aqui nas duas novas.
Já movimentou a minha nota.
E aqui ele trouxe.
Schema, invoice time, title, invoice time,
delivery status.
Ele trouxe tudo e foi explicando.
Olha que legal.
Legal, detalhe importante, tá?
Ele não só pegou o schema.
Ele simplesmente pegou o schema,
pegou o tipo de dado,
entendeu o tipo de dado
e gerou para mim também o
quê?
Uma descrição para cada um dos
campos.
Legal, então isso aqui...
Aqui são os dados.
Vamos...
Será que o Matheus está mentindo?
Não sei.
Vamos ver.
Vamos ver se eu consigo mostrar.
Talvez bem...
Então, 7 de julho de 25,
bonitinho, 8 de 23.
Aí, ó, já pegou.
Aí tem que pegar qual é
o time zone que ele pegou
isso.
Olha que legal.
Ele pegou um time zone diferente.
Ele pegou um time zone aqui,
ó.
Não sei de onde que ele
tirou o 23.
Sabe como é que é interessante
a gente sempre comparar e ver?
A gente pode fazer uma série
de melhorias aqui.
Então, aqui pegou o número de
pedido.
Da Uber Eats, pedido completo.
Aqui ele pegou o número de
Ele pegou o invoice ID.
O invoice ID está aqui em
cima, ó.
Ele mesmo considera que isso é
o quê?
Eu passei pelo pedido.
O mais interessante é...
Eu não falei para ele que
isso aqui é um ID.
Eu simplesmente falei, leia e interprete.
Olha que interessante.
É uma leitura e interpretação.
Então, ele pegou aqui e trouxe
as informações.
Simplesmente que eu falei para ele
assim, vai lá e lê.
Ele tirou...
Aqui, ó, legal.
Massa, aqui é verdade.
Aqui é no header aqui.
Ele não pegou...
Ele pegou aqui internamente.
Por quê?
Porque eu não instruí ele.
Eu não falei para ele onde
é que lê.
Eu falei assim, ó, abre e
entende lá para mim.
Está vendo como é que tem
pontos de melhoria que são interessantes
para a gente poder fazer esse
tipo de processo?
Que isso é importante?
Então, a gente também tem que
tomar um cuidado, Lucas.
Aí é porque tem uma série
de processos que a gente pulou
aqui, tá?
Aqui é ambiente de teste, demonstração.
De novo.
Então, aqui ele trouxe as informações
todas que eu queria.
Ele pegou e montou o JSON
disso para mim.
Beleza?
Gente, ficou claro o que eu
queria mostrar aqui inicialmente?
Vocês viram?
Ficou massa?
Pegaram?
Como é que eu não passei
nada para ele?
Olha, até a explosão teve.
Até o efeito especial teve na
minha apresentação.
Por quê?
Por que é importante vocês entenderem
isso?
Porque aqui é programático.
Aqui você pode simplesmente desenvolver, você
pode plugar outras bibliotecas,
você pode trabalhar com outros tipos
de processos no Python
de maneira muito mais simples.
Você pode fazer a escalabilidade do
seu processo aqui interno muito,
mas muito rápido e muito fácil
de fazer.
Beleza.
Eu vou rapidamente aqui, porque o
Luan, eu vou mostrar aqui um
reg pequeno que eu fiz
para justamente você entrar com o
Langflow e você fazer assim, de
novo, meu aqui vai ser simples.
Eu quero mostrar o quanto a
gente tem que fazer e quanto
tempo leva para você desenvolver algo
nesse sentido, porque tem muito mais
arquivo.
A quantidade de arquivo que eu
tenho de processos que eu tenho
que trabalhar.
Tá?
Então, a parte de reg aqui,
eu também tenho aqui o meu,
o meu amigo Ritmi, todos os
meus documentos
vão estar dentro do Git, tá,
gente?
Podem ficar tranquilos que todos vão
estar aqui dentro.
Então, toda a parte de instalação,
a gente vai abrir aqui um
por um para a gente poder
entender, a gente faz a ingestão,
consegue, Rubens, consegue porque eu vou
dar acesso
aos PDFs, né?
Aí é tranquilo.
Só fazer a instalação e rodar
os PDFs, tudo local, tá?
Ele já constrói os PDFs lá,
Matheus.
Já tá todos os PDFs lá,
eu coloquei ontem à noite.
Beleza.
A única questão aqui é, Rubens,
é você vai precisar abrir, criar
uma chave do OpenAI
para você poder fazer.
Aí não, né?
Eu não posso passar a minha
chave não, né?
Não dá, não dá.
Diego, Matheus, aí a pergunta, quando
usar programação e quando usar um
non -code?
Perfeito, cara.
Exatamente isso aí, ó.
Qual é o nível de customização
que você precisa?
E qual é o seu time
to market?
Se você tem uma necessidade que
é muito customizável, é muito flexível
e muito aberta,
infelizmente você vai conseguir fazer isso
no no -code ou no -code.
Por quê?
Porque ele já vai estar, ele
vai te entregar caixas prontas.
Tem algum...
Você precisa fazer algum tipo, por
exemplo, o Coulomb mostrou o protótipo
que a gente
fez referente ao chat, né?
Que vocês estavam testando lá no
Telegram.
Cara, eu queria fazer um MVP,
eu queria fazer uma POC, eu
vou desenvolver, pode preocupar
a língua, eu vou subir, tal,
tal.
Não.
Eu vou simplesmente fazer um protótipo,
rodar aquilo ali aqui que me
atendeu, beleza.
Eu vou para o próximo estágio.
Então, em questão de desenvolvimento, tá?
Tem que tomar cuidado com isso.
Voltou?
Voltou?
O som voltou?
Tá falhando?
Tá.
Pegaram o que eu falei sobre...
Tá.
Tem que ter diferenças?
Tá.
Eu vou cortar.
Eu vou voltar então para todo
mundo pegar.
A ideia, gente, quando você fala
do no -code para desenvolvimento, eu
ir para código mesmo,
tudo depende o quê?
Tudo depende principalmente do qual o
seu caso de uso.
Ele é customizável.
Ah, eu vou ter interação com
várias bibliotecas, eu vou ter muito
código, eu vou ter muita
necessidade de estender as capacidades da
minha plataforma.
Eu vou para código, faz muito
mais sentido, né?
Aqui, querendo ou não querendo...
Ah, mas eu estou muito tempo.
Tomo um tempo considerável, mas mesmo
assim eu consigo estender, eu consigo
trabalhar,
eu consigo customizar, eu consigo interagir
com outras plataformas.
E a questão da escalabilidade...
Fala um pouquinho disso.
Tá?
Depende.
Hoje, a escalabilidade depende de qual
plataforma você tem e quanto você
quer pagar.
Então, não sei se dependendo se
a gente trabalhar...
É, por exemplo...
Boa pergunta.
N8n, por exemplo, tem outras plataformas
que vão escalar muito melhor, tá?
Aí, está vendo?
Tem o Airflow.
Por exemplo, quando a gente estava
no Airflow o ano passado, antes
de lançar o Airflow 3,
uma das grandes discussões é usar
o Airflow para orquestrar rags de
GNI.
Teve muitas palestras sobre isso.
É um grande tema e, sim,
é possível a gente fazer.
Eu só não...
Acho que a gente...
Não só como a gente vai
falar.
Não, aqui não.
Mas a gente vai, a gente
vai fazer, sim.
O do orquestrar.
A gente vai trazer coisa de
Airflow, podem ficar tranquilos, tá?
É...
Eu...
Diego, eu...
Reclamações eu prefiro testar e validar,
tá?
Para ver se realmente...
Às vezes, eu sempre falo que
uma reclamação pode ser um fundamento
não entendido.
Às vezes...
Ah, da Dona Data.
Achei que era do Airflow.
Estava até preocupado.
No caso do N8n, sim.
A questão...
Eu mesmo quebrei, nossa, umas três
vezes testando arquivos de 22 linhas
CSV.
Coisa bem simples.
Achei que era do Airflow.
Estava até preocupado agora.
É...
Ahn...
Tá.
É...
Alex.
Para esse caso, nota fiscal.
Que a sua opinião é treinar
um small, não?
Depende.
Depende, Alex.
Boa pergunta.
Se é treinar um small ou
para ter mais performance e otimização
de custo?
Vamos lá, gente.
Custo sobre esses processos, o Luan
começou a falar em uma discussão
que a gente teve
bem legal sobre isso, tá?
É muito novo.
Muita coisa vai ter que fazer.
Eu sou MVP.
Você vai ter que fazer o
seu protótipo, uma coisa menor, um
ambiente menor e testar
e ver se isso vai ser
escalável e ver o custo disso
meio que na praça.
Você vai aplicar as técnicas, mas
falar assim, isso vai ser muito
mais barato ou não.
Você vai ter que muito validar,
testar o seu caso de uso
para validar a questão de
custo, tá?
Tem que pensar um pouquinho.
Talvez um small LNM funcione muito
bem para você.
Perfeito.
Mas depende do tipo de...
Por exemplo, no caso do Luan,
que deu o exemplo da empresa
que trabalhou.
Eram vários tipos de notas diferentes.
Era uma quantidade de regras de
negócios considerável.
Cara, não dá para fazer isso
no SLM.
Então, depende muito...
O mesmo caso de uso pode
ser algo pequeno ou pode ser
algo gigantesco.
Depende muito quanto a isso, tá?
Vou voltar para o Ray aqui
rapidinho.
Então, eu criei aqui a parte
de configuração.
Lembra do embedding que o Luan
mostrou, né?
Eu estou usando o text embedding
A2002, tá?
Aqui eu estou usando o Pinecone.
Eu estou usando o Pinecone na
menor versão dele.
Um small de 1536 dimensões.
Aqui eu vou usar o índice
invoices, tá?
Que eu vou subir lá para
dentro.
E aqui eu estou usando o
chunk físico, porque eu sou Nutella.
Eu quero fazer uma coisa simples.
E de novo, gente.
Vamos começar simples, tá?
Aqui é POC, igual o Luan
comentou.
MVP não é nada.
Aqui eu vou levar a produção.
Porque aqui depois, de novo, a
gente tem que repensar o projeto,
como tudo.
Porque essa é a melhor saída.
Mas aqui eu estou usando o
chunk de 1200,
porque eu estou usando somente uma
nota que eu vou trabalhar com
ela.
Então, eu tenho aqui a parte
de ingestão.
Eu vou ler todos os meus
arquivos JSON.
Olha que legal.
Estou pegando agora especificamente os JSONs
que eu criei da primeira parte.
Agora eu vou pegar esses JSONs
e eu vou trabalhar com eles
que foram criados.
Tá bom?
Aqui eu estou rodando um Threshing
QA, tá?
Para poder simplesmente validar o meu
pipeline de...
De...
De reg.
E aqui começa a mágica.
O retriever é o meu principal
código aqui.
Aqui é onde eu vou simplesmente...
Foi tudo que o Luan falou.
Eu vou criar o meu retriever.
Eu vou chamar o embed.
Eu vou fazer o load dos
documentos que eu trouxe.
Eu vou fazer o chunking.
Todo o processo vai rodar aqui
dentro do meu retriever.
Então, o retriever eu vou rodar
para o quê?
Para executar o meu reg aqui
dentro.
E aqui tem a pasta última,
que é a limpeza que eu
comentei.
Aqui eu tenho um JSON de
text.
Para validar por quê?
Porque como eu estou trazendo dados
em JSON, vocês vão ver que
a forma como vocês formatam
o dado para um LLM, tá?
Processando da melhor forma, é diferente
do que a gente está acostumado
com a engenharia
de dados, né?
Trazendo dados tubular ou em níveis
de coluna, que a gente tem
realmente trazendo um contexto
mais amigável para o LLM.
De novo, não é para a
gente, né?
Beleza.
Deixa eu pegar aqui o nosso...
Deixa eu pegar aqui embaixo aqui,
ó.
Isso que eu quero.
O nosso amigo.
Lógico que eu já tive vários
erros antes de fazer isso funcionar,
evidentemente.
De novo, né?
Demo é demo.
Vou simplesmente rodar o comando, passar
aqui a especificação de onda que
eu estou
no meu retriever.
Vou falar para ele fazer o
build do meu retriever, que eu
não quero usar QA.
Ele vai simplesmente embedar, fazer o
chunking e subir isso dentro do
meu...
Pinecone.
Acabou.
Eu vou aqui no meu pinecone.
Deixa eu dar um refresh.
E...
A minha voz é de 25.
Hoje que eu tinha trabalhado com
ela.
Está aqui com o chunk, está
aqui no meu vetor para trabalhar
com esse dado agora vetorizado.
Beleza?
E olha que legal o conteúdo,
né?
Aqui eu falei.
A minha voz é de ID
e tal.
Então eu pego o meu dado.
Eu estou dentro de JSON.
Transformo de uma fórmula textual para
se faço de gestão do meu
LLM e trabalhar com
ele.
Luan.
Acho que agora é com os
seres, meu jovem.
Gente, dúvidas?
Querem perguntar alguma coisa enquanto o
Luan prepara a parte dele?
Deixa eu tirar aqui do...
Podástico, hein?
Simples, né?
Simples.
Muito simples.
Simples.
Simples.
É por isso que eu entendo
que o consumidor fala assim.
Ah, eu prefiro não trabalhar com
.
Eu prefiro trabalhar com código e
tal.
Porque ganha -se um tempo.
Só que a longa...
A gente sabe, né?
A cauda longa.
Isso aqui vira muito código.
Muito código.
Muito grande.
Muito grande.
E aí fica complexo de gerenciar.
Tem que tomar muito cuidado.
Tem que ter um trade -off,
né?
Isso depois foi feito.
O importante é...
Não.
É avaliado.
É o que eu quero falar.
Caso...
Eu acredito muito hoje, cada vez
mais, nessa parada de...
Cara, prototipa, tem uma coisa funcionando
rápida, depois você itera sobre isso.
Por que eu estou falando isso?
Porque o futuro de quem consegue
posicionar esse tipo de solução é
como ser humano,
velho, vai estar muito adiantado dos
outros.
Eu vejo...
A gente vê na nossa empresa
acontecendo isso.
A diferença de profissionais dentro da
empresa, como eles interagem conosco, que
está no
nível C.
É muito engraçado.
Tem gente que começou agora e
tem uma puta facilidade.
Já tem gente que já está
no mercado, já tem uma dificuldade.
Às vezes o cara quer mostrar
muita coisa e não consegue mostrar
nada.
Então, assim, é tudo como você
se posiciona.
Então, é importante entender que o
mundo é interativo.
Então, por exemplo, fora do país,
a gente não fecha projeto desse
jeito que fecha no
Brasil, que eu vejo no Brasil
a galera fechando.
Bora fazer tudo, né?
Bora.
Bora.
Não.
Você faz um pedacinho.
Depois você faz um pedacinho.
Depois você faz um pedacinho.
Depois você faz um pedacinho.
Só que o que acontece?
Quando você começa a desenhar o
primeiro pedacinho, na verdade, você já
fez o collection
gathering inteiro.
Você já entende ali o processo
como um todo.
E daí você constrói aquele pequeno
pedacinho, mas na hora em que
sua arquitetura vai...
Na hora em que o seu
projeto vai aumentando em complexidade, a
sua arquitetura te suporta
isso.
Então, essa é a grande diferença.
Então, você consegue crescer.
Esse meu projeto que eu fiz,
que teve quatro rounds de phases,
cara, que tá no quarto
round agora, eu não vou mudar
a minha arquitetura.
A minha arquitetura, ela é a
mesma.
Eu vou acrescentar em cima dessa
arquitetura.
O CSG é o mesmo e
tal.
Então, você tem que ter muito
pensamento de como você vai fazer
isso e qual a ferramenta
que vai melhorar.
De novo, você consegue fazer isso
de várias outras formas.
Quais são os pontos positivos e
negativos de cada approach que você
faz?
A gente tem muito esse problema.
E aí, é não pensar como
arquiteto de solução.
Esse pensamento de que, tipo, cara,
qual ferramenta que eu vou escolher
vai ser a melhor?
Não existe a melhor ferramenta, né?
E, assim, e também as pessoas
falam, existe a melhor ferramenta para
esse caso.
Depende de muita coisa também, entendeu?
Depende do ponto de vista.
Depende de quem tá fazendo.
Depende de um milhão de coisas,
tá?
Então, o que que é importante
validar?
É saber os pontos positivos e
os pontos negativos, né?
Então, é meio que saber exatamente
esses dois aí, tá?
Você pode, teoricamente, versionar no código,
né, Diego?
Mas eu nunca fiz nenhum CSD
disso, não, tá?
No Langflow, dá pra fazer tranquilamente
com o processo de CD e
CI.
Porque o código é lang cheio,
no final das contas.
Então, tranquilo em relação a fazer
isso.
Beleza?
Bem, pra quem ficar até o
final, temos quantas pessoas?
194, tá?
Então, eu tô terminando de gerar
aqui esse PDF.
Matheus, que PDF é esse?
Que documento é esse que a
gente vai dar pra eles?
Que eu falei que vou dar?
É a compilação de quantos meses
de trabalho que a gente tá
aí?
Tentando achar tudo de relevante, raspando...
Luan, eu vou te falar que...
Você falou cedo, ainda acho que
é mais, tá?
Eu acho que é mais agora.
Colocando...
Nós estamos no mês...
Final do mês 7?
É.
Seu projeto deve ter aí, no
mínimo, uns 9, 10 meses.
É brincadeira aí.
É.
Exatamente.
O trabalho que eu e Matheus,
a gente tá fazendo, né, é
justamente coletar com players,
com o mercado, quais são as
ferramentas que a galera utiliza, categorizar
essas ferramentas,
mostrar, tipo, em qual ela cai,
qual a comparação entre elas.
Cara, isso é um trabalho muito
grande.
Por exemplo, muita dúvida eu tinha.
Cara, qual a diferença de você
usar Langflow ou QRAI?
Quando que você usa só QRAI?
Por que que você precisa usar
Langflow?
Por que que é importante Langchain?
Ah, e se você for usar
criação de agente, por que não
usar DeFi?
Ah, não, mas aí por que
que não usa N8n?
Quando que usa?
Então, essa separação de categorias, esse
tipo de coisa, realmente, são muito
complexos
de poder ser falados e tal.
E, quando eu acabar essa parte
aqui, eu vou dar esse cara
pra vocês, e esse cara só
vai estar disponível no chat.
Ele não vai ser entregue pra
vocês fora do chat, eu vou
anexar ele no chat.
Então, quem não tiver, não vai
ter.
Não adianta pedir e por aí
vai, tá?
Não adianta pedir porque não vai
ser dado.
Inclusive, o nosso time não sabe
disso, tá?
Então, é bom que a gente
tá fazendo pra vocês acompanharem aí
até o final, porque
eu ainda não acabei.
Tem coisa pra mostrar aqui.
Primeiro, fazer a vida de vocês
melhor.
Olha esse projeto aqui, que show.
Tá?
Um projeto que eu tô brincando
com ele num projeto novo de
chunking, que eu tô brincando
com algumas otimizações no RAG, tá?
Então, deixa eu mostrar pra vocês
aqui.
Chunk.
Bem -vindo.
Tô gostando, adorando.
Extremamente fácil.
Três linhas de comando.
Você escolhe qual chunking, Matheus.
Olha só.
Chunking.
Recursive chunking.
Chunking.
Recursive chunking.
Bota o chunking lá dentro.
Acabou.
Você não precisa usar mais algoritmo.
Ele faz pra você.
Tá?
É...
Muito show.
Ele, inclusive, tem algumas coisas de
inteligência também que ele trabalha.
Então, bem -vindo.
Muito legal.
Ele oferece vários, já, tipos de
chunks.
Sentença, recursivo, semântico, os mais
famosos.
Code, neural.
Tá?
Então, esse cara aqui, tô meio
que apaixonado por ele, assim, sendo
honesto.
Top mesmo, hein?
Oi?
Muito top.
Não, muito, muito, muito, muito top.
Tá?
Beleza?
Então, dê uma olhada.
Tá?
Ele tem o Python, né?
Facinho de utilizar.
Boa.
Vamos falar agora de Langflow.
Bem, quem nunca viu Langflow, fala
eu.
Só pra ver o quanto de
gente vai ficar doida vendo isso.
Gente, a gente esqueceu de falar...
Cara, é muito legal.
Não, ele é.
É, a gente esqueceu de falar
que nos treinamentos da engenharia de
dados Academy, nós podemos
falar foda.
foda, e foda geralmente
porque tem muita gente nova aqui,
e foda é quando
você tem um momento de eureka,
que você acha muito
legal, enfim, então pra mim esse
é um dos
projetos que me possibilitaram muito, né
Matheus, da velocidade pra gente
então, é um projeto
inclusive, deixa eu mostrar pra vocês
uma coisa muito legal
com
91 mil estrelas
no
no GitHub
tá
o Langshan
e é base
basicamente de tudo que você for
conhecer na sua vida
aí
de facilidade, ele tem
112
e já tá muito mais tempo
do que o Langflow
tá, primeiro ponto
pode falar foda aí até o
final da aula, primeiro ponto
ele é open source
segundo ponto, ele é criado
por um brasileiro, tá
que inclusive eu tenho
uma notícia que nem o Matheus
sabe
mas
foi desenvolvido por um brasileiro
esse projeto
extremamente utilizado lá fora
muito foda
realmente, tá
e a gente tem, cara, constantemente
atualizações
é um projeto que tem
muita gente colaborando
a gente tem o Matioli
que colabora também
bastante nesse projeto
é um cara muito legal da
comunidade
a gente vai falar
vai trazer algumas coisas interessantes aqui
tá, tem muitos contributors hoje
é, e cara, é um projeto
que é apaixonante
inclusive lançaram recentemente
um, eu não sei se você
viu
Matheus, um installer pra
aplicativo de Mac
você instala ele no Mac agora
ele traz toda a parada, tá
beleza, então o que que ele
é?
ele é, preparados?
caixinha
olha que coisa sexy
de chorar
de paixão
podachain
cara, esse cara aqui
ele é gráfico
e ele utiliza o
langchain por debaixo dos panos
ou seja, é um
no -code, na verdade não é
um no -code
é um low -code, no -code
que a gente chama, só que
na verdade
ele é backed by langchain
ou seja, você pode fazer
tudo que você faz, basicamente
95 % do que você faz
em
langchain, você consegue fazer no
linkflow, tem algumas coisas específicas
que você não vai fazer, até
porque você vai ter que
escrever programaticamente pra fazer sentido
mas eu vou explicar pra vocês
isso aqui
tá, então
antes de vocês verem isso
tá, ele é uma interface gráfica
e inclusive linda pra cacete
na minha opinião, tá, aonde você
pode conectar, você tem playground
pra testar, você tem tudo isso
dentro
dele, de novo, ele é open
source
tá, é
é extremamente fácil de instalar, você
pode
instalar ele via pacote, você pode
instalar ele
docker, você tem agora o desktop
tá, que você pode instalar ele
é, então você consegue utilizar
fluxos visualmente num
canva e realmente
entregar isso, tá
é, então assim
isso já é muito fora do
da base, na minha humilde
opinião, tá
então você vai construir fluxos
muita gente
muita gente eu escuto falando assim
Matheus, poxa, mas esse cara não
dá pra fazer
em produção, cara
você pode, na verdade, habilitar ele
como um webhook, e você usa
ele no N8n
calma aí Luan, não entendi
não, agora você bugou
a minha cabeça, é amigão, se
você
aprende a usar as coisas da
forma correta
as coisas dão certo, por exemplo
se você faz todo o seu
fluxo de reg
com todas as características
pythonicas ali e tal, que eu
vou mostrar pra
vocês, e você quer chamar no
seu
fluxo de trigger, por exemplo, no
N8n
você consegue fazer? consegue
você vai basicamente habilitar
um endpoint pra esse cara
e você vai conseguir fazer isso
tá, então você pode
triggar ele via API
tá
aqui ó, você pode simplesmente chamar
esse cara, ele vai executar todo
o processo que tá ali, tá
pra chat, pra tudo que você
quiser fazer
via webhook, tá
é
é
, beleza, mais o que?
webhook
você tem agentes, tem módulo
você pode instalar
todas as dependências python, então você
pode
integrar isso aqui, eu tenho um
ambiente que integra
isso com o langfuse, por exemplo
você pode usar o langfuse, você
pode fazer
o deployment disso em kubernetes
ou seja, essa é a grande
coisa, se você quiser produtizar
esse cara realmente, você pode fazer
ele
dentro, você pode começar com o
docker, eventualmente
você pode colocar esse cara pra
dentro
né, de deployment em produção
agora, eu mostro pra
pra eles ou não, Matheus, aquilo
lá que eu te mostrei
semana passada, essa semana
se você quiser ter um ambiente
de produção
nível lang
lembra aquele cara que faz
isso vale, né
isso vale
no final, né
como é que vai ficar até
o final pra poder
beleza, beleza, vamos ver
tá, então a gente tem aqui
algumas formas de como fazer o
deployment
desse cara em produção também, beleza
então vamos dar uma olhada nele
tá, então, você só
sobe ele no docker, tá, então
aqui
ó, ele já tá pra você
disponível
aqui, dentro de build
bem simples, tá, build
você tem o readme, que é
realmente
ridículo, mas eu tenho um docker
compose aqui
o que esse docker compose vai
fazer
ele vai buscar a última versão
do langflow, ele vai
setar um ambiente interno, mas
aqui você pode, por exemplo, utilizar
um ambiente, você pode
conectar com um banco de dados,
post
você pode fazer várias configurações aqui
por exemplo, configurar o langfuse
você sabia, Matheus, que dá pra
fazer integração de
langfuse com ele? Todas as métricas
que é passadas pro
pro OpenAI, todas as métricas são
capturadas de observabilidade dentro
do escopo do langflow, então
você já tem tudo integrado, então
quando você faz uma requisição
de agente
ele já marca tudo isso
automaticamente lá no dashboard do langfuse
porque você integrou
no nível instância
absurdo, absurdo é
você tem toda a observabilidade dentro
do seu ambiente
tá, e tudo open source
então aqui ele vai setar
você só precisa dar um docker
compose up
e daí magicamente
você tem esse cara disponível
aqui, tá
beleza, então eu vou
criar um novo flow, ele vai
aparecer dessa forma
aqui, eu vou criar um novo
fluxo
ele tem aqui alguns
templates que eu posso utilizar já
pra ver, pra testar, ou eu
posso
começar um branco
né, e aí olha só que
legal isso aqui
eu tenho simplesmente
um painel em branco aqui com
milhões de coisas que eu posso
fazer também
no nível agente
no nível prototipação
no nível de NIA e assim
por diante
e aí a primeira pergunta é
Luan, qual é a diferença do
langflow pro
pro N8N
né, isso é uma pergunta que
a gente recebe
bastante, inclusive quando a gente vai
posicionar isso com
cliente, assim por diante, cara, qual
é a diferença?
bem, a diferença é o seguinte
o N8N, ele é um sistema
completo pra automação
não só de agente
mas não só de processos
quando a gente usa o langflow
olha só o que ele fala
aqui
o langflow
é pra que você possa construir
o que?
especializados em agentes
então ele é um cara que
vai no nível
muito mais a fundo
do que um agente aqui que
tem
algumas toolings, por quê?
porque cara, você tem blocos
de repetição, você tem
algoritmos de compressão
diferentes, você tem python que você
pode trazer
o chunking ali, por exemplo, pra
fazer o chunking
você tem coisas que você
vai estender
então, o que que acontece?
muita coisa que
de fato
não vai estar lá, aqui você
tem
um pedaço, ou seja
aqui você tem 5 % do
que é um LLM
do que é um agente, do
que
de fato um fleet de agentes
consegue fazer, e você consegue fazer
isso
facilmente lá
e escalável, né, então você pode
botar isso, por exemplo, você bota
isso no
python, você
você gera o application
então o que que você pode
fazer?
você escreve, você sabia disso, Matheus?
você escreve todo o código
em caixinha
você exporta ele pra application
e você manda pra dentro do
kubernetes
e ele aplica o código
e aí você abre
quantos
quantos nós você quiser, você abre
o que você quiser, tá?
então, na verdade
é de fato
você conseguir fazer isso
você consegue, tá?
beleza, então o que que a
gente
tem aqui? nós temos os inputs
né, então eu tenho input texto,
input chat
eu tenho outputs
chat texto
que vai receber, eu tenho
prompts, né, então
como que eu posso escrever prompts
aqui
que é a entrada e saída,
eu tenho data
e olha só que interessante, eu
tenho
request API, eu tenho
diretório, eu tenho
um arquivo, então por exemplo, cara,
eu tô com um diretório
e aí você tá olhando aqui,
poxa, mas aqui tem um
diretório interno, mas se, pensa
se você tiver um diretório que
tá
sincado ali no seu S3, por
exemplo
você vai ter acesso a consumir
qualquer arquivo desse, só que
não só isso, ele tem uma
inteligência muito
legal aqui dentro de conseguir já
fazer
por exemplo, a extração do dado
automaticamente
pra você, tá? Ah, Luan, eu
quero
e aí todo mundo já pergunta,
porra, já
no fluxo de big data
né
o nosso fluxo de analytics, de
events
driven, como que eu posso conectar
tudo isso
né, bem, agora imagina
se você consegue ter esse fluxo
totalmente orquestrado, né, você consegue
fazer isso também
é, beleza, data
então a gente tem toda a
parte de conexão
tá, eu tenho um SQL query
que eu consigo
fazer também, fazer uma query em
algum lugar
processamento, eu tenho as operações
desde mais básicas pra mais complexas,
então
eu tenho um router, cara, isso
que aí a gente usa bastante,
você roteia
lógica, você faz judge de
LNM, cara, você tem noção
a galera não tem noção do
que é isso não, Matheus, infelizmente
você constrói
é que não dá
pra em seis horas fazer o
que a gente vai
fazer aqui, que a gente vai
mostrar pra vocês posteriormente
mas dá pra comprar, você
a gente vai chegar no nível
de criar um agente
que reg, eu mostrei pra vocês,
gente
o traditional reg, são
três regs que a gente tem,
tá
e eu vou falar deles
mas é porque a gente tá
perdendo, a gente
viu o reg tradicional
que exatamente você tem o LNM,
o retriever
knowledge base, augmentation
entrega desse dado, beleza, funciona
muito bem, agora imagina, Matheus, o
seguinte, que você tem um reg
e você quer vários agentes, você
quer
vários retrievers, você quer um
agente que vende, você quer um
agente
que é base de conhecimento, você
quer
um agente que faz
booking, você quer um agente que
consome essa informação, e aí quando
você vai conversar com a galera
do N8N
que não tem um background de
dados que a gente tem
o cara vai criar um fluxo
pra cada, vai alimentar
um pedaço pra cada e assim
por diante
como que você é engenheiro de
dados AI
que engenheiro vai fazer, você vai
criar um
multi -retriever reg
tem noção do que é isso?
O que é um multi -retriever
reg? É um reg
que satisfaz de forma híbrida semântica
vários
retrievers
e depois você tem um agente
que reg
tá, que aí
é o supra -sumo, hoje
se você falar do supra -sumo
de reg, é o agente que
reg
é você ter um LLM que
vai fazer
o routing em cima disso, você
já tem
esse componente pronto aqui, tá
então, você tem parser, você tem
extrator, você tem várias coisas
interessantes, ah, Matheus, mas eu não
tenho
beleza, ah, eu não
consigo aqui, tem uma coisa que
eu não sei
que não tem disponível pra mim
beleza, amigão, faz o seguinte
clica no code
ali em cima, no code lá,
ele abre
joga isso, o code que você
quer
os gráficos que você quer aí,
brother
então, vocês estão
acho que, é, a galera tá
dormindo
já, Matheus, gente
não, e o foda disso, nem
quando eu testei
ficava brincadeira nele, ele até criou
até a caixa, toda bonitinha
assim, cara
talvez vocês não tenham
vocês não estão conseguindo ver
o quanto isso é foda, mas
beleza
então, você pode vir pro custom
aqui
e fazer, tá
mas o que a gente tem
preparado, né
modelos, olha só
eu tenho o language model padrão
que eu posso chamar aqui os
provedores, ó
mas
eu tenho já todos esses modelos
aqui
disponíveis, só pra você chamar, arrastar
e conectar, tá
OpenAI, Ulama, OpenRouter, Perflex
Vertex
tudo, tá
Azure OpenAI
outro
Vectors, olha o tanto de
Vectors já disponível
Clique, Astra, Couch
Milvus
OpenPG Vector
SuperBase
O EVH
tá, tá tudo aqui, tá
Embeddings
haha, olha os embeddings aqui
pra gente falar de otimizações, ó
olha o Correio Embeddings, tá
os tipos que eu tenho, ó,
multilingual
e assim por diante, ó
olha que legal
tenho aqui também, agentes
olha, eu tenho um agente preparado
e eu posso estender tool mode
meu irmão, aqui, aí é a
brincadeira
fica foda
você começa a adicionar ferramentas
em cima desse cara
de agente pra criar multi
capacidades em cima dele
outro, memória
a memória que você vai dar
dentro pra esse cara
outro, ferramentas
aí vem uma cacetada, cara, de
ferramenta
que você pode, e tem essa
aqui
Matheus, fala assim, eu não conheço
a Tavili
não conheço a Tavili
beleza
então toma
vamos ver se
alguém entende isso
vamos ver se alguém entende isso
pra quem fala assim
caralho, isso aqui é foda
gente, você dá capacidades
pro seu LLM consultar em tempo
real a web
com o seu reg, alimentar seu
reg
fazer o grounding de permissões
em cima disso, é como se
você tivesse
um reg
de web
não sei se dá pra fazer
com isso aqui, acho que não
e você chama numa API, Matheus
com um vector
e você pode
embedar isso ali automaticamente
e é
a gente tem uma demo aqui
não vai dar tempo, obviamente
mas a gente faz uma raspagem
bem legal com isso aqui
tá, tem outras coisas interessantes
que dá pra fazer também
tem QAI, então por exemplo, muita
gente pergunta
poxa amor, mas qual a diferença
do QAI
para
esse cara
é porque você pode orquestrar aqui
o QAI, Matheus
você pode utilizar os agentes
e aqui você integra no seu
pipeline
como um todo, então tem como
você chamar
esse cara aqui dentro
tá, outra, lógicas
básicas, né, notificação
por exemplo, pode simplesmente
mandar uma notificação, de novo, tudo
que eu não tiver
eu posso instalar com o pacote
Python
então isso quer dizer o que,
Luan?
isso quer dizer, irmão, que você
vem aqui no pacote
Python, no Docker Compose
adiciona no seu requirements
instala e você tem
um ambiente gráfico
que tem acesso a tudo, a
data
base, a snowflake, é o que
você
pensar na sua vida, tá
então aqui você tem tudo
e helpers também, tá
ou seja, por os
bundles aqui, que eu nem mostrei,
os bundles
são o que cada componente consegue
te oferecer, então você pode raspar
dado de Notion, você tem
toda a família LingXen, ó, olha
o tanto
de coisa que você tem, acessa
a data
base, em cima dele passando a
URL
e a configuração desse cara
cara, tem, ó
o recursal character text,
ó, pra dividir os chunkings
como a gente viu
openAI, natural
language, cara, tem
enfim, qualquer coisa que você pensar
vai tá aqui dentro, você vai
conseguir utilizar
então
vamos ver como é difícil, Matheus,
criar um
reg, vamos tentar fazer isso?
então vamos
pegar aqui, vou vir
em editar
vou vir sobre
tu tu tu tu
vou vir sobre
vamos falar
vamos fazer a primeira parte, que
é o
off
preprocessing
reg
docs
beleza, vejam que agora
não sei porque tá demorando aqui
mas vejam que agora, daqui a
pouco
quando esse cara limpar, eu não
sei porque
que ele travou aqui, deixou
hoje as demos tão querendo me
sacanear
mas enfim
então
deixa eu tentar
de novo, vamos de novo aqui
edit
off
preprocessing
reg
docs
beleza, ele não quer renomear aqui?
não tem problema não
ó, eu posso vir aqui, ó
e dar um publish, tá vendo?
e aí eu posso ter acesso
a API
pra chamar esse cara, tá vendo
Matheus?
Então eu posso chamar a minha
aplicação
quando eu quero interagir, por exemplo
com um bot, com chatbot, por
exemplo
eu integro esse cara aqui
simples assim
então vamos lá, a primeira coisa
que eu preciso
que a gente aprendeu, o que
a gente aprendeu
vamos puxar aqui
o meu arquivo file
vou puxar um arquivo aqui
é muito difícil, viu?
selecionar o arquivo, eu vou selecionar
aquele arquivo
de datasets
beleza?
então o que ele fez?
fez aqui? Eu simplesmente
anexei esse cara aqui
dentro. Se eu vier em código,
eu consigo saber o que ele
está fazendo
aqui, né? Obviamente
eu tenho validação,
eu consigo ter controles aqui em
cima
disso, então ele estende para mim
aqui o tipo
de separador se eu quiser, alguns
tipos de coisas mais básicas
e padrões em relação ao que
eu tenho
esse cara, tá?
Beleza.
Mas o que eu consigo fazer?
Eu consigo
aí isso aqui é muito legal
eu consigo para a maioria dos
meus
componentes, a gente vai ver, chamar
eles de tools
então ele vira um tool para
um
agente ou para alguma coisa.
Então se você quiser customizar alguma
coisa para o seu agente
você consegue fazer também, tá?
E isso é muito legal.
Beleza. O que a gente aprendeu
agora, gente? Qual o próximo passo
do reg? Aqui a gente
está preparando a base. O que
eu preciso
fazer nessa base? Eu preciso
fazer o que? Opa, beleza.
Preciso embedar, né?
E para embedar eu preciso fazer
o que? Eu preciso fazer o
chunking, não é o primeiro pedaço
que eu tenho que fazer
chunking? É isso mesmo? Boa.
Então
vamos lá.
Chunking. Posso digitar aqui, muito difícil.
Chunking.
Aí eu tenho algumas formas de
fazer isso.
Eu tenho o recursive
né? Que ele é muito mais
otimizado. Ou eu tenho o split
text
que a maioria das pessoas faz
é o split text.
A gente vai utilizar
o
recursive chunking.
Alô, eu quero outro chunking.
Beleza. Adicione o chunking aqui, por
exemplo,
e traz ele como biblioteca
separado. Eu tenho um outro ambiente
que tem isso.
Tá? E aí está tudo certo.
Então o que eu vou fazer?
Eu vou conectar
esse cara
com o meu processo.
Então eu vou conectar
o input. Matheus, é muito
difícil. Presta atenção aqui, por favor.
Eu conecto o dado desse
cara.
Aí você vem aqui, ó.
Pup.
Encaixa esse cara
no input dele. Beleza?
E aí eu falo, olha,
o chunking size é mil
duzentos
e o código
Java. Java.
Java é falha, tá?
Então, beleza.
Tá aqui.
Legal. Então ele vai pegar esse
arquivo.
Ele vai simplesmente
quebrar esse arquivo.
E aí,
eu preciso de fazer o quê?
Eu preciso
mandar esse cara para um
database. Na verdade, o que eu
tenho
aqui é o Astra configurado.
Né? Então o que que eu
vou
fazer? Eu vou pegar esse cara
aqui
e eu vou conectar aqui
no dado. Mas
o que eu faço
é... Automaticamente
ele pegou a credencial desse cara
pra mim.
E o que que eu acho
legal, Matheus?
Acho muito foda isso, velho.
É essa parte aqui agora.
Bem, a gente...
O que que a gente entendeu?
A gente entendeu que não dá
simplesmente pra você jogar o chunk
aqui dentro. Beleza?
Então, o que que eu tenho
que fazer aqui?
Hã? É.
Eu tenho que fazer o quê?
Eu tenho que mandar
o embedding, não é não?
Né? Então eu vou selecionar o
database
aqui.
Vou selecionar a coleção.
Eu vou criar uma nova coleção
já já.
E agora,
eu vou trazer esse cara pra
cá.
Tá vendo que eu posso trabalhar
o
tumold muito das vezes
como uma ferramenta? Ou seja,
esse cara ele vira uma ferramenta.
Depois eu vou dar uma olhada,
eu vou mostrar pra vocês
especificamente o que ele faz.
Tá? Então eu vou chamar aqui,
ó.
Embedding.
Por que que eu tô usando
o
Astra? Porque ele oferece gratuito.
Tá? Então só pra deixar
fácil pra gente.
Então eu vou pegar o OpenAI
Embeddings.
Tá?
E
eu vou...
conectá -lo
aqui.
Aqui eu teria que programar.
Deixa eu pegar o TextSplitter.
Pra gente mostrar o
modelo básico.
Tá? Então o Splitter
entra aqui.
Beleza.
200 mil.
Aí a gente tem o Embedding.
Esse cara se conecta.
O Chunking se conecta
aqui.
E
E
o que que você não tá
abrindo?
Ah, aí o que que a
gente vai fazer?
Aqui no Embedding, olha lá.
O Chunking
vai mudar no Ingest.
E aqui o Embedding,
se eu vier em Opções.
Olha só, eu tenho...
Tem esses caras aqui.
Por que ele não tá deixando
eu conectar?
Eu deveria conseguir conectar aqui esse
Embedding.
Vai no Options.
Vai no Options do Astra.
Aí, vai lá em cima.
É, então.
Options.
É, Controls.
É.
Desse tem o Embedding desabilitado.
É, ele tá.
Com certeza.
Cadê você?
Tá vendo que ele não tá
aparecendo aqui?
Tava testando isso agora.
Tava testando isso agora.
Aí ele apareceu.
Parece que tá debaixo do Keyspace
do...
No Controls.
No Controls do Astra.
Cadê?
Tem?
Keyspace.
Não sei o que é a
versão.
Embeddings.
Cadê?
É, não tá aparecendo o seu.
Ah, calma aí.
Será que eu...
Não.
Beleza.
Relaxa que pra tudo tem uma
solução na vida.
Aqui, ó.
Não.
Cadê?
Esse irmão lá é o nosso
ADB.
É, tá certo.
É esse cara aqui mesmo.
Porque ele não tá aparecendo Embeddings.
É uma boa...
Uma ótima pergunta.
Primeiro que ele não tá deixando
eu puxar esse cara aqui.
Ah.
Ele não tá aparecendo pra mim.
Tem alguma coisa aqui que a
gente não perdeu.
Mas não tem problema não.
Aqui, ó.
Eu tenho outro simples que faz
isso.
Então, a mesma coisa, ó.
Tá?
Não mudou absolutamente nada.
A única diferença é que ele
apareceu aqui Embedding pra mim.
Tá vendo?
Embed Models.
Então, eu recebo o arquivo.
Eu quebro ele em texto.
Essa parte em GERI.
Obviamente que pra ele fazer a
ingestão completa do arquivo, ele vai
ter que embedar.
Então, ele usa o Text Embedding
3 Mol.
Lembra, tá?
É por causa disso.
Se tá faltando a Collection, né?
Então, talvez seja assim.
Tá?
Então, deixa eu pegar aqui.
É, pode ser que não tenha.
Deixa eu pegar a Collection aqui
e selecionar pra ver se ela
vai.
Tá.
Coletion.
Aqui.
Aí.
É isso mesmo.
É.
Isso mesmo.
Beleza.
Então, vou clicar aqui e vou
puxar aqui e tá feito.
Tá?
E aí eu vou inserir esse
cara dentro desse fluxo.
É porque ele não tinha mostrado
antes.
Tem outras 30 vezes que mostram.
O Astro foi a primeira vez.
Se bem que eu não posso
falar, porque executa muito bem, tá?
Ah, outra coisa que eu esqueci
de falar pra vocês.
O Langflow foi adquirido pela Datastacks.
Não.
A IBM adquiriu a Datastacks.
Tá?
Que a Datastacks cuida também desse
cara.
Então, a gente vai ter algumas
novidades em relação a isso também.
Muita gente do time trabalha nisso.
Então, você viu aqui que eu
adicionei a Coleção e tal.
Luan, como que você adicionou essa
coleção?
Voltando aqui pro Datastacks.
Eu criei uma conta.
30 dias de graça.
Tá?
O Astra.
Você pode simplesmente criar um database.
E aí você tem Serverless.
Você tem gerenciado.
Você pode escolher a nuvem e
assim por diante.
E aí a primeira coisa que
a gente vai discutir aqui, por
exemplo, quando o que eu vou
é que eu crio...
Por exemplo, vocês não perguntaram isso.
Pelo menos eu não vi aqui.
Mas a pergunta é a seguinte.
Luan, se eu tiver...
Isso é uma dúvida, né, Matheus?
Se eu tiver múltiplos documentos, eu
armazeno no mesmo Vector Database ou
eu armazeno num
Vector Database diferente?
Outro.
Dentro da mesma Collection.
Então aqui eu tenho o quê?
O Vector Database.
Dentro da mesma Collection, eu armazeno
vários documentos diferentes ou eu armazeno
Collections
diferentes?
O que vocês acham?
Ah, essa pergunta é boa, né?
Porque, por exemplo, aqui a gente
está pensando o quê?
Um documento.
Certo?
Mas eu posso criar aqui e
trazer todos esses caras para aqui.
Beleza.
E aí qual é o melhor
caminho aqui?
O melhor caminho é talvez ter
uma Collection por mesmo Database.
Beleza.
Mas...
É...
Ter uma Collection por documento, por
exemplo, uma Collection instrutores, uma
Collection
casos,
uma Collection uso, uma Collection e
tal.
O que vocês acham?
Então, gente, manda aí.
Tem menos de 194 pessoas.
Opa, exatamente, alguém falou ali
a Aninete por domínio
então essa é a grande recomendação
por domínio
domínio de conhecimento
então a ideia é realmente
você tentar armazenar
isso em coisas que fazem sentido
e obviamente que
todos vão ter seus prós e
contras
você pode armazenar separado, você pode
armazenar
junto e assim por diante
no final das contas o melhor
é você
utilizar a parte
de
metadado para te ajudar
então a gente não vai ver
aqui
porque vai entrar mais complexo, mas
você armazena tudo num domínio, por
exemplo
se eu estou falando de formação
a gente vai armazenar tudo dentro
da mesma collection
só que a gente vai ter
metadado
para cada arquivo desse
então cada arquivo desse vai ter
um metadado
entregando informações, então na hora que
o agente
for consultar, ele vai consultar esses
caras
isso vai reduzir muito o rounding
e a acumulação de grounding que
você vai fazer
tá? Então é exatamente isso
isso também segue, Flávio, não só
para o agente, segue também como
você armazena
beleza?
Então eu fiz esse cara aqui
então vejam que nós temos
o vector
data explorer
eu tenho aqui a collection
chamada uberits docs
então eu tenho aqui dimensões
quantidade de registros, esses dados
estão ali armazenados
vejam uma coisa muito importante
que muita gente ignora
olha só
atributos, vector
que é de fato a representação
binária do vetor
e o metadado
tá vendo, Matheus, que aqui não
tem nenhum metadado
para você poder utilizar como cosine
ou similaridade
ou coisa desse tipo
então uma das grandes paradas fodas
é que
a maioria dos vector databases
já na hora da pesquisa
da pesquisa semântica
ou pesquisa híbrida
ele tenta filtrar pelo metadado
então o metadado ajuda pra caceta
então aqui é muito importante que
você
adicione metadado em cima disso
para que você tenha de fato
cada vez mais
um grounding muito melhor
tá? Então isso vai te ajudar
bastante
beleza? Aí voltando aqui
nós temos esse processo
então vamos alimentar esse caso
aqui, né
vou repetir inclusive
esse processo
se eu não fiz nada errado
tem essa nova collection
eu só por via de dúvidas
eu vou criar uma nova collection
aqui
pra gente ingerir
então eu vou criar aqui uma
nova collection
vou chamar de
ube -its -docs -b2
tá?
Ah, importante
aí aqui fala, vector database
collection é
embed generation
o que que eu coloco aqui?
eu boto nvidia ou eu boto
bring my own ou eu boto
um provider pra embedar
nenhum deles
por quê? Porque essa
collection tá sendo gerida pelo
langflow, então se ela tá
pelo langflow, eu vou trazer a
minha
própria aqui, que é de fato
o embeddings que eu já
tô fazendo aqui, tá vendo?
então eu já estou trazendo esse
embeddings aqui
e por que que esse embedding
tem que ser igual?
porque se não, de novo, a
criação
a plotagem do 3D
basicamente pra você fazer similaridade
vai ser uma caca, você vai
perder
similaridade, então toda vez
que você fizer isso, você tem
certeza que
o que eu estou de fato
fazendo embedding é o mesmo
escara que eu estou consumindo esse
embedding
então tá aqui
dimensões
tá? Aí ele me passa aqui
as informações
das dimensões que eu preciso saber
então a dimensão anterior,
eu me esqueci qual é
e é a mesma dimensão
Luan, como que eu sei a
dimensão?
ó, 1536
dimensões
aí a gente vai ver aqui
agora
Uber
Docks
Uber Docks
V2
por exemplo
Uber Eats
Docks V2
Bring My
On
dimensões 1536
Embeddings Ada 02
né? E aí se a gente
ver aqui
a gente tem esse cara aqui
Embeddings Ada 002
tá vendo que eu já ia
errar?
então é o mesmo daqui ó
Embeddings
ops
Small
tem que ser Embeddings Ada 002
tá vendo aqui?
então o mesmo que eu estou
fazendo caso eu perca a similaridade
então esses caras estão fazendo Ada
002
então tá certo
é isso mesmo?
é Ada 002
aqui você tem a forma como
você vai comparar
lembra que eu falei do COSINE
e do DOT?
olha lá
o COSINE ele pesquisa para similaridade
diferentes magnitudes, ou seja, para texto,
imagem, embeddings, preferência, recomendação
DOT é melhor do que o
COSINE
melhor versão do COSINE e preferida
somente quando os dados são normalizados
numa unidade correta de tamanho
tá? e você tem o Eclosidian
então isso aqui são formas de
como métricas de similaridade você tem
vamos continuar com o COSINE
vamos criar esse cara
esse cara vai criar essa coleção
e se a gente não fez
nada errado aqui
o que que vai acontecer?
nós iremos receber os arquivos
transformar eles num dataframe
quebrar eles em chunks
com overlapping
mandar esse overlapping para cá
pegar o Ada 002 que é
exatamente a cor da coleção
a quantidade de menções lá
e indexar esses caras aqui dentro
da nova coleção
que é a V2
que já já deve estar disponível
para mim aqui
tá criada? tá criada
e agora nós iremos simplesmente
apertar
aperta play
ok, play
beleza
show
se a gente não fez nada
errado
quando atualizar aqui nós teremos algumas
informações dentro deste cara
né? e temos de fato
temos 159 registros
tá aqui
os caras armazenados
e infelizmente
de fato nenhum metadado interessante
para gente que a gente poderia
apresentar
mas esses caras foram quebrados aqui
lindo né?
ótimo
agora
a gente vai
para
a parte mais interessante
como eu consulto essas informações
né?
ou seja, como que eu uso
a base
para fazer
perguntas semânticas em cima disso
então para a gente não passar
ali
vocês já entenderam a ideia aqui
é
olha só que
fera
que eu vou trazer para vocês
aqui
eu vou trazer esse fluxo para
a gente
ajeitar ele
para vocês entenderem o online
aqui a gente viu o offline
então o que o offline fez?
o offline
nesse caso o offline porque a
gente escreveu
nós escrevemos o dado para dentro
do vector
agora nós iremos consumir esse dado
do vector
então vamos lá
como que a gente consome esse
dado do vector?
primeiro ponto
input do chat
então eu vou receber um input
de chat aqui
né?
e aí esse input de chat
vai bater aonde?
vai bater agora
no v2
beleza
só que
para que esse cara bata aqui
eu preciso passar o embedding
olha só o embedding está errado
está vendo?
tem que ser o 002
beleza
então estou passando o embedding aqui
o embedding
e
a pesquisa
qual a search query?
é essa search query
ele vai embedar
para poder puxar a informação de
documento
aqui
dentro do v2
então v2
beleza
aí essa query
então o resultado desse
desse cara
a gente vai puxar o texto
dele
e vai enviar esse texto
para o prompt
só que
né?
para eu enviar esse cara
ou seja já está fazendo string
file
está tirando esse dado já em
data frame
já está convertendo esse resultado para
a gente
e aí eu tenho duas informações
no prompt aqui ó
que esse é importante você entender
primeiro
o input
que é a questão
do prompt
e
o contexto
que eu estou recebendo do parcer
que é o que?
o
prompt augmentation
então o que eu estou recebendo
aqui ó
contexto
deu o contexto anterior
recebendo as perguntas
tá
questão
resposta
aqui eu não estou adicionando nada
de grounding
de augmentation né
poderia fazer várias coisas aqui
beleza
aí agora
eu vou pegar esse resultado do
prompt
aqui ó
olha lá
que ele buscou
dos documentos
e eu vou passar esse cara
para o chat cpt
que vai de fato receber
ó
eu não estou passando nenhum system
message
eu não estou passando nada
tá cru
aqui eu poderia passar o que?
um contexto para ele sinistro
do que é esse cara
e aqui eu não estou nem
passando o contexto desse cara
estou setando uma certa temperatura
e estou mostrando o chat
então se eu chegar agora por
exemplo
nos meus documentos né
deixa eu salvar esse cara aqui
nos meus documentos
e eu simplesmente botar playground
e eu falar quais são os
atributos armazenados sobre os motoristas
vamos ver aqui o que ele
vai responder
se ele vai responder
se ele está afim de responder
ou não né
hum
os atributos armazenados sobre os motoristas
entregues são
driver id e assim por diante
olha só que legal
de onde vem os dados
de avaliação dos restaurantes?
os dados de avaliação dos restaurantes
vêm da fonte de ratings
não dei contexto nenhum para ele
e ele está completamente legal
não dei nenhum system context para
ele
e mesmo assim ele está sendo
bem sucinto
bem direto ao ponto
quais são os possíveis status de
um pedido no Uber Eats?
show isso aqui né gente
muito foda né
está buscando
fazendo toda parte que a gente
falou de similaridade
assim por diante olha só
bem bonitinho olha só
os possíveis status
cada transição de status é registrada
com timestamp emitida
outro
qual o tipo de dado que
está no MongoDB?
agora
vamos para perguntas mais complexas
que geram grounding de relacionamento de
entidades
cara a gente criou um reg
em quanto tempo?
como os dados de rota se
conectam os motoristas e pedidos?
os dados
de rota se conectam
os motoristas e pedidos
por meio dos atributos presentes e
tal
cara olha isso dessa forma
a rota
vocês estão vendo isso aqui né
a gente criou em sei lá
5 minutos
e isso que a gente não
fez nenhuma técnica
eu estou mostrando para vocês como
a gente vai interar em cima
disso
quais são os atributos disponíveis para
cada
a gente está falando do reg
tradicional
aqui a gente está esquecendo atributos
filtrem melhor prática de
de search e assim por diante
perguntas mais complexas aqui ó
só para vocês terem ideia de
como funciona
tá
quais os principais desafios ao trabalhar
com esses data sets?
olha lá
aí agora eu estou mudando
estou precisando do grounding do documento
mais o LLM
para ele poder realmente me responder
em relação a isso
porque eu preciso de muito mais
contexto
quais os desafios ao trabalhar
está lento ou está rápido Matheus?
você acha que está em resposta
de tempo boa?
o que vocês acham?
está excelente
os principais desafios
é o teu também
a gente tem muita coisa para
melhorar
eu não estou rápido não
você está
nível de acessibilidade
você fez isso lendo
documento sensível
sim
e outra coisa que eu fiz
não sei se vocês perceberam
o documento está em português
eu fiz o embed em inglês
olha só isso aqui
essa próxima
quais informações estão disponíveis para
análise
de tickets de suporte?
olha só esse aqui
as informações disponíveis para análise de
tickets de suporte inclui tal
e se eu
olha Matheus
desejar realizar um cancelamento
o cancelamento de um pedido?
imagina se você tivesse um agente
que pudesse fazer isso
você acha que dá para fazer
Matheus?
claro
tem como ir lá no banco
e desativar esse cara?
geralmente
pois o ciclo de vida do
pedido
o cancelamento geralmente está
imagina que na hora
gente vocês estão prestando atenção no
que eu vou falar?
olha só
aqui poderia entrar um agente
que ia entender
opa ele falou
ele deu um mini em cancelamento
agente cancelamento?
chega aí
ele vem aqui
faz uma query no database
lá no local
entende
verifica o contexto
e cancela para você automaticamente
dá para fazer isso Matheus?
perfeitamente
você acha que isso acontece no
Uber Eats?
lá por debaixo dos panos?
ou você acha que é um
bocado de macaco fazendo isso lá
atrás?
tem gente
o mundo está aí
tá?
então
o que a gente viu hoje
aqui?
é um pedaço da superfície
cara tem muito mais para ver
não somente isso
para que vocês pudessem ter um
nível de consciência
Matheus você consegue extrair o PDF
lá daquele documento que eu prometi
para eles do Delta EIS Tech?
e
e anexar aí
para
depois focar
ainda não
só anexa quando eu falar tá?
então tem muito mais
e o mercado está muito cheio
de
não só
existem três camadas
e a gente está finalizando aqui
eu tenho algumas coisas para falar
mas vai valer muito a pena
você ficar nesse final aqui
porque eu vou te dar
algumas dicas sobre como se posicionar
isso tá?
primeiro existem três camadas
a primeira camada cara é
se você tem o conhecimento e
entendimento do que dá para fazer
você consegue se posicionar como líder
nisso
tá?
para mim eu fiz isso na
pitching
então só para vocês terem uma
ideia
isso é fora do país
hoje eu sou
hoje eu sou um dos caras
que se encarregam em todas as
soluções de GNI
isso para mim eu estou vivendo
eu estou vivendo um momento
o Matheus sabe
muito, muito, muito único para mim
o que eu estou aprendendo com
cliente
o que eu estou aprendendo com
solução é
impagável
porque isso é o conteúdo do
momento
muita gente fala para eu falar
mas aqui você viu na prática
10 % da superfície
aqui a gente criou um reg
de um documento
a ideia é que você veja
um sistema
fim a fim de produção acontecendo
isso
e você tendo esse cara
você tem multiagente
você tem reg nível 2
e eventualmente agente para você poder
fazer isso
e daí você começa a ver
que agentes são muito possíveis
claro que não do jeito que
o pessoal fala
que vai mudar não sei o
que e tal
não, você vai conseguir fazer muita
coisa
só que o importante é você
saber se posicionar
outro
cara, não falar por falar
é realmente tem embasamento teórico e
técnico
a gente viu
cara, a gente ficou 5, 6
horas aqui
já viu estratégia de chunking
embed, melhores práticas
quais são os pitfalls
técnicas para poder fazer chunking
vimos ferramentas, cara
falamos de N8n
vimos de langchain
vimos de langflow
a gente nem tocou superfície da
quantidade de ferramentas
que nós temos aqui
que você vai ter acesso logo
no final
Mateus, eu não estou mentindo gente
olha só isso aqui
cara, isso aqui é um trabalho
de compilação
que vocês não tem noção
parece simples
mas essa lista vai ser entregue
para vocês
então vocês vão ter acesso a
essa lista
para que vocês possam de fato
consumi -la
entender o que cada um cai
as categorias de cada uma delas
tá
para vocês poderem começar a estudar
começar a ter um entendimento em
cima disso
agora, como tudo isso se encaixa
então realmente o grande desafio hoje
e a gente tem recebido isso
direto
cara, quando que vocês vão falar
de GNI?
eu lembro que
janeiro
janeiro, dezembro do ano passado
a galera, poxa Luana
quando que vocês vão trazer conteúdo
de GNI
para o Engenheiro de Dados Academy?
para quem conhece a gente
sabe que a gente não traz
conteúdo
só falar do que é falar
a gente precisa ter vivido
a gente precisa ter trago lá
do bleeding edge
lá de fora
o Matheus está trabalhando com um
cliente muito grande
que é a InfraSite
que é a parceira da formação
Matheus, eles falam sobre isso aqui
que a gente está falando
ou não?
por favor, fala a verdade de
coração
não só fala como ter uma
reunião dedicada
para o board da empresa
para discutirem estratégias de IA
o que eles querem fazer?
o que eles querem usar IA?
para que IA?
a ideia é
você tem uma empresa de pessoas
inteligentes
usando métodos inteligentes
é essa a frase que eles
mais usam lá dentro
e é assustador gente
e base de conhecimento?
reg?
nossa, é uma coisa que eles
estão desesperados
e alguém já entregou isso para
eles?
a gente está no processo de
entregar
então gente, isso é realidade
não sou eu vindo aqui
toda vez que eu vim
para quem já foi meu aluno
aqui gente
eu falo balela para vocês
parece louco aqui talvez para algumas
pessoas
mas isso aqui é realidade
agora, como tudo isso se encaixa
em um pipeline de engenharia?
como tudo isso de fato te
entrega um pipeline fim a fim?
como de fato você vai conseguir
ver tudo isso?
bem, e aí cara foram muito
tempo
muito tempo de casa
eu, Matheus
todo esse grounding
todo esse cara
para trazer para quem de fato
está interessado
ao primeiro Bootcamp de IA e
Data Engineer
do país
a gente vai trazer algo
que jamais foi visto
obviamente como sempre
quando a gente traz conteúdo
para quando você está interessado
em consumir um conteúdo diferente
obviamente a gente está aqui para
entregar
porque não tem nenhum lugar
e eu, de novo
pode ir na internet
caçar
quem entrega
prático
quem entrega realmente
production ready
escalabilidade
que mostra tudo isso junto
que se conecta, enfim
quando a gente traz conteúdo
a gente demora bastante
Matheus
para trazer conteúdo novo às vezes
mas é porque existe
um motivo de como você vai
operacionalizar
quais são as tecnologias
quais são as ferramentas
quais são tudo que você vai
utilizar
e por isso que a gente
trouxe
pela primeira vez
o nosso primeiro Bootcamp
tá?
e o Bootcamp
ele tem uma característica muito legal
porque ele é
pela primeira vez
um Bootcamp muito prático
atacando casos de uso
fim a fim
e a gente vai utilizar
as mesmas características Matheus
da nossa formação
de R $ 4 .987
ou seja
a gente tem metodologia prime
nós temos
formação focada em conceitos e teoria
e prática
nós temos ambiente para você testar
né
então a gente tem
tudo o que está acontecendo
no Bleeding Edge
trazendo para vocês
com ferramentas consolidadas
de mercado de fato
tá?
então vão ser seis dias
com seis módulos
calma aí Luan
como assim?
sim
você vai passar
20 horas
com a gente
Matheus
o que você aprendeu aqui?
me fala aqui
o que vocês aprenderam
em seis horas de conteúdo?
vocês aprenderam o que?
foi bom?
não foi?
fala aí
vamos lá gente
tem 190 pessoas em sala
tudo bom?
imagina 20 horas
fora esse conteúdo
que eu falei
ou seja
eu já vou começar
acima desse conteúdo
tá?
é
já estou falando
que para quem é da formação
quem é The Plumbers
quem é o que for nosso
esse é um conteúdo
completamente diferente
tá?
ele não tem vínculo com nada
tá?
ele é completamente novo
nosso Bootcamp
é completamente novo
é
então
o que que acontece?
o que que a gente vai
fazer
Matheus?
algo cara
louco
né?
primeiro
a gente vai mostrar
na segunda -feira
um Intelligent Document Extraction
funcionando
do nível
que eu mostrei
para vocês
lá fora
com multi prompting
só que
qual o problema
aqui Matheus?
é open source
vocês vão consumir
open source
aonde você vai conseguir
aplicar isso
vocês vão conseguir
aplicar isso
em qualquer ambiente
vocês vão utilizar
Airflow
LangFuse
ChatGPT
Gipsy
como
LLM as a Judge
vocês vão ter
uma cadeia
um chain
de event driven
de funções
que fazem
tudo isso
e vocês vão escrever
a galera não está vendo
aonde a gente vai chegar
mas calma
num banco de dados
relacional
ou seja
a gente vai estruturar
as informações
de documentos
lá
só que num ambiente real
com Airflow 3
com event driven
que se conecta
em nível de ação
dentro desse cara
inclusive Matheus
eu vou para Seattle
daqui 3 meses
apresentar sobre isso
é
eu também
eu vou fazer
um workshop
lá
de 3 horas
em Seattle
no Apache
no Airflow Summit
eu fui convidado
para poder falar
isso aqui
tá
então
isso aqui
vai ser a extração
com LLM
com todos
os wrappers
de Airflow
juntamente com
Python
justamente
função
event driven
conectando
com
esse novo
processo
para que você
tenha vários templates
versionamento
em cima disso
só isso aqui
já é uma parada
assim
caso real
real
só que a gente
vai muito além disso
então são 3 horas
de módulo
prático para você
vocês vão ter acesso
a gravação
dessa
desse cara
por um ano
vocês vão ter acesso
a um
é
a um cara
de fundamentos
de uma hora
antes disso
ou seja
calma aí Luana
vocês vão entregar
um conteúdo gravado
vocês vão ter um conteúdo
gravado de uma hora
de teoria básica
para vocês entenderem
para você entrar
totalmente aqui
em rendizom
ver o aplicando
e ter acesso
ao próximo dia
a esse conteúdo
no git
ver o aplicando
e ter acesso
em relação a isso
no git
isso gente
deixa eu falar para vocês
isso aqui
não é coisa legal
de fazer
tipo assim
isso aqui
que eu estou trazendo
para vocês
assim
nossa aprendi uma coisa
não isso aqui
você vai usar
você vai conseguir articular
se você cara
está passando
numa reunião
você escuta o cara
falando
você fala
não calma aí
tem como fazer isso
a gente pode utilizar
aqui um sistema
de inteligência
e você já vai ter
todo o template pronto
toda a lógica
de como fazer isso
com python
com lang fuse
com tudo open source
custando baratíssimo
em como trocar
esse modelo
para um small language
por exemplo
que você pode fazer
como utilizar
esses caras
eu vou ensinar técnicas
de como você pode fazer
inferência em outros lugares
quer ver
olha só o nível
together ai
talvez você nunca
ouviu falar
elastio
você está
a gente está indo
para um outro nível
galera
tá você vai poder
armazenar
sua plataforma inteira
por 40 50 reais por mês
aqui você tem
todas as api's
disponíveis
de todos esses caras
por api's
você conhece isso aqui
fala
fala se você conhece
isso aqui sim ou não
falei
beleza
então olha só
quanto isso aqui
vale para você
você tem uma api
transparente
em python
que você chama
áudio
visão
re -ranking
embeddings
coding
tudo
você paga tudo
por eles
ele faz a inferência
todinha para você
você pode trocar
de modelo
vocês estão entendendo
isso
a gente vai abstrair
a complexidade
entre chamar modelos
para verificar
custo em cima disso
então está falando
desse nível
de sofisticação
tá
outra
rag power
the chat bot
aqui eu mostrei
para vocês
a primeira parte
a segunda parte
a gente vai fazer
um fim a fim
um bot
de telegram
só que não daquilo
com multi inteligência
nível 2
a gente vai utilizar
um multi retriever
rag
vocês vão aprender
como fazer
um multi retriever
um cara
que vai ingerir
diferentes coisas
conversacionais
e vai ter comportamento
tom diferente
faq
resposta
citação
e aí melhores práticas
de embed
chunks
a gente vai ver
comparação entre eles
é tá
poxa
eu fiz assim
quanto que recuperou
vocês vão ter essa idéia
tá
a gente vai usar
essa mesma coisa
no nível
muito mais profundo
ótimo
só que aí mateus
que que a gente fez
estruturamos
o documento
né
no segundo dia
a gente criou
uma base de reg
no terceiro dia
que que a gente
vai fazer
a gente vai criar
uma solução
que entende
interpreta
tanto postgres
quanto vector
databases
faz interpretação
faz queries
faz processos
dentro ativos
e entrega
esse dado
final
text to query intelligence
na cara de vocês
vocês vão ter
acesso
a esse processo
como um todo
beleza
só que
são
eu falei
quantos dias
é
só que engraçado
você falou
os três casos de uso
que a gente só podia escolher
um
você entregou os três
num dia só né
não calma
não
mas tem outro problema
aqui
é que tem um quarto
tem um multi agent
que vai cancelar
tem um multi agent
que vai entender
que vai interpretar
a intenção
você vai ver
isso funcionando
e aí muita gente
vai falar cara
multi agent
não funciona
eu vou mostrar
para vocês
com crew ai
a gente vai ver
esse cara funcionando
integrado com
lang flow
são casos reais
beleza
vocês vão ter acesso
a esse cara também
tá
então a gente vai criar
agentes
a gente vai dar
Rose específicas
para ele
a gente vai de fato
trabalhar com
esses caras
integrar eles
no pipeline
de dados
inteiro
com DNA
transformando esse cara
no e aí
de engineer
na sexta -feira
eu estou tentando
trazer o criador
do lang flow
eu vou trazer
o Samuel Mattioli
que é um dos criadores
os caras que estão
mais convidados
aí
dentro desse esquema
do mundo de lang flow
o Fulvio
que é um super cara
de NI
Samuel
um outro
mostro
para vocês escutarem
deles
e tem outros convidados
que eu estou tentando
trazer
tá num podcast
aqui um Enei de 3 horas
que vocês vão escutar
dos caras
eu vou tentar trazer
o Rodrigo
o máximo possível
ele é o criador
do lang flow
para vocês escutarem
os projetos
se realmente
a galera está usando
e assim por diante
a mesma coisa
que eu trouxe
o Dan Lindstedt
eu vou trazer
uma galera chave
aqui são pessoas confirmadas
eu tenho outras
por debaixo
que eu não posso
anunciar ainda
mas Matheus
não calma aí
mas são seis
são
seis dias
são seis módulos
tá faltando um aí
tá faltando um
no sábado
são cinco horas
porque aqui
você vai ver algo
que
eu duvido
que isso aqui
não vai deixar todo mundo
louco
a gente vai falar de
detecção e fraude
em streaming com o Enei
Matheus
o que isso significa
gente lembra da história
da engenharia
Enei
e nós
estamos falando
da união
pronto
vamos fazer algo inteligente
para poder
realmente
trazer valor
em um nível
de detecção
de fraude
em tempo real
usando coisas
como Spark
como Kafka
como ByteWax
como LLM
entre outras
coisas
mas
você vai conseguir
entender
a classificação
de fraudes
via texto
e conseguir tomar ação
em cima disso
né
porque que a gente
demorou para trazer isso
porque a gente
foi fazer estudo
de mercado
e a primeira formação
e a única formação
que conseguiria trazer
algo tão produtivo
eu não sei
se vocês conhecem
essa formação
é a formação lendária
é uma formação
muito legal
eu tenho muito feedback
bom de quem já fez
essa formação
tem grandes players
aqui
e a gente se baseou
muito olhando
nesse ecossistema
de como a gente
traria
esse treinamento
tá
e eles entregam
muitas coisas legais
práticas aqui
são alguns módulos
então tem
QRI
tem automação
tem N8n
tem ETL
tem DeFi
que a gente também
vai mostrar para vocês
então tem muita coisa
então a gente
uniu tudo isso aqui
para pensar um pouquinho
de como seria
né
essa entrega
hoje eles oferecem
isso que para mim
é um supra sumo
de formação
de treinamento
de GNI
hoje
que trabalhem com isso
e que mostrem
pedaços disso
não no contexto
de engineering
né
esse cara
custa 12 .888 reais
e eu já tive pessoas
que fizeram
tá
e vale muito legal
para esse momento
essa já é uma formação
um pouco mais estratégica
você tem
cara o João
por exemplo
que é um dos caras
que é o fundador
do QRI
você tem uma galera
muito legal aqui
tem uma proposta diferente
mas qual é
o grande gancho
é de fato
uma formação
mais completa
eu não conheço
na verdade
se alguém tiver aqui
tem 185 pessoas
aqui
vocês conhecem
alguma formação
que de fato
mostra
engenharia
não só engenharia
mas realmente
GNI
conceitos de GNI
como operacionalizar
como tirar
realmente
em cima disso
então esse é
o grande problema
e como que a gente
faz isso né
porque a gente tem
grandes processos
grandes
grandes coisas
acontecendo
nesse ecossistema
ecossistema
que demorou demais
para ser validado
tem muita coisa
acontecendo
então vocês vão ter acesso
a repositório
dataset
certificação
acesso a plataforma
e assim por diante
e a ideia
é que vocês consigam
começar a entrar
no mercado
para entender o seguinte
cara
eu saí desse bootcamp
mano
eu sei
o que está rolando
eu tenho uma ideia
do que fazer
eu posso
fazer
eu sei
implementar
eu entendo
o que é
se alguém falar
de inteligência
se alguém
fizer distração
se alguém falar
dos casos
mais utilizados
e reg
abre para outros pontos
né
então você vai entender
tudo isso aqui
de fato
só que isso
vem com valor
é complexo
a gente conseguir
vender isso
de uma forma
em que seja agradável
que seja agradável
para todo mundo
porque esse é um trabalho
extremamente complexo
a gente trabalha
nisso aqui
nos últimos seis meses
eu estou dividindo
o meu tempo
entre isso
mais a formação
para poder trazer
esse conteúdo
que é um conteúdo
que muita gente
pedia aqui
tá
então infelizmente
esse cara
não é para todo mundo
tá
é um cara
que tem um custo
muito alto
e de fato
não vai conseguir
fazer todo mundo
esse treinamento
é um treinamento
totalmente exclusivo
completamente
nichado para vocês
e é uma pena né
Matheus
porque cara
é um conteúdo
hoje que nego
está pagando
muito dinheiro
para ter acesso
muito
eu vou pegar
o exemplo
meu e seu
por exemplo
quando a gente
estava pensando
nisso tudo aqui
é cara
além da gente
ter gasto
com vários treinamentos
que assim
foram
bem decepcionantes
para mim
eu não consegui
pegar nada
rendison
fala cara beleza
eu quero ver a teoria
como eu adoro
ver a teoria
entender
mas eu quero entrar
tá então
eu fui cara
eu fui
na Coursera
eu fui no DataCamp
eu fui no
eu fui no Prompt
é
Learn Prompting
eu fui
cara
e não desmerecendo
os conteúdos
obviamente
né
é
mas
não tinha conexão
eu não via
o fluxo
tá então
quando a gente teve
cara
Matheus
vamos inverter
um pouco
em vez da gente fazer
muita teoria
vamos dar uma hora
de teoria
do que todo mundo
precisa ter o ground
e a gente entra
nas demonstrações
porque posteriormente
esse cara vai ter
todos os processos
para estudar
ali dentro da plataforma
ele vai conseguir evoluir
porque ele sabe
o que ele tem que fazer
né
ele vai ver essa plataforma
usando
quais são os pontos
positivos e negativos
de cada implementação
e a gente está justamente
nesse processo
de entregar isso
a gente vai lançar
esse produto
esse vai ser um produto
nosso exclusivo de lançamento
é um produto
que vai entrar
e vai estar no perpétuo
com a gente tá
é um produto em que
já foi validado
anteriormente
é
e
cara
infelizmente
são poucas pessoas
que vão ter acesso a isso
porque um treinamento
desse
vai de fato
custar bastante
né
não tem como ser diferente disso
até pelo timing
que esse conteúdo é
né
até pelo timing
que todo mundo precisa
Matheus
isso é triste
né Matheus
sim
sim porque
gente
a gente tá nessa jornada aí
tem um tempo
né
essa brincadeira
não é
agora
a gente tá com acesso
a empresas
bem legais
para fora do país
justamente
para trazer algo
que
honestamente falando
te coloca
numa série
numa
numa
numa posição
muito privilegiada
como profissional
e assim
é o que a gente
quer melhor
para o que a gente
consegue alcançar
né
são as pessoas
que estão com a gente
são os nossos alunos
como vocês estão aqui
né
é
né
e
a gente até fica
meio triste
de mudar
o posicionamento
porque a gente sempre tenta fazer
algo
que seja muito
custo -efetivo
né
mas realmente dessa
dessa vez
não deu inclusive
a gente já validou
com algumas pessoas
que já fizeram a aquisição
desse produto
e eu falei cara
como que a gente pode fazer
algo que ajude
de certa forma
a galera que realmente
quer dar espaço
que realmente acha interessante
de novo
isso aqui cara
é o que vocês vão
consumir como base
para o que vai vir
daqui para frente
nesse Spectrum
e se isso faz sentido
para vocês
é um investimento alto
mas eu realmente
acredito
o nosso movimento
como empresa
é esse
tá
o meu movimento
como na Pifian
é esse
é
o movimento
como os meus
peers da empresa
estão se movimentando
é
Matheus anota aí
anota aí
o que eu vou falar
eu vou trazer para vocês
dentro
de quem tiver acesso
a esse Bootcamp
eu vou trazer para vocês
a entrevista
com o meu gerente
o Biba Dash
o Hit
tá
o cara tem 20 anos
de TI
23 anos de TI
pode anotar aí Matheus
que eu vou chamar ele
e ele vai fazer
eu quero que ele fale
para o cara
que trabalhou
nas maiores empresas
que você pensar
eu quero que você escute
diante
da boca dele
falando
o que está acontecendo
no mercado
no movimento
na movimentação
de consultoria
se a galera
está pedindo AI
se ele está forçando
o time dele a aprender
AI
se é uma coisa importante
ou não
para não falar
porque quando eu falo
e eu venho e trago
um treinamento desse
a galera fala
o cara quer vender
não
eu estou fazendo
como eu sempre fiz
nos últimos 15 anos
da minha vida
eu estou mostrando
para vocês
eu estou dando o caminho
para vocês
eu estou abrindo
o caminho para vocês
vocês vão ter
um ano de acesso
desse conteúdo
mas de novo
não é um conteúdo
para todo mundo
porque é um conteúdo
que tem um valor
muito mais alto
beleza
e não existe desconto
para ninguém
ninguém
cara
formação
poxa
o cara comprou
academy
o cara comprou
60 workshops
não
é um produto
completamente separado
completamente diferente
e a gente sabe
que não é para todos
não é Matheus
porque o valor
é muito alto
e realmente
é um investimento
que cara
vai matar vocês
porque
essas pessoas
que tiveram esse conteúdo
vai ter uma vantagem
muito grande no mercado
tá
porque
pensando nesse valor
e pensando em como
a gente consegue trazer
isso para vocês
a gente está anunciando
esse bootcamp
ai eu fico até desconfortável
de falar Matheus
que
né
enfim
a gente está anunciando
ele
por 3 .997 reais
tá
esse é o valor
do nosso bootcamp
cara
eu garanto
que esse conteúdo aqui
inclusive nós contratamos
duas pessoas
para o nosso time
para quem
estava estudando
sobre esse tipo
de coisa
tá
então cara
é um produto
mais high ticket
com certeza
é um produto
de qualidade extrema
que a gente vai trazer
com todos esses podcasts
com todo esse conteúdo
e assim por diante
então eu espero
que vocês apreciem
quando foi lançado
tá
é
e
espero que vocês
se divirtam
né Matheus
quando cara
a gente vai começar
as aulas começam
mês que vem
tá
uma semana de conteúdo
de fritação
e é isso
temos mais alguma coisa
para adicionar Matheus
não gente
para quem
é de novo
como a gente comentou
é
para quem
vai estar com a gente
prepare -se
porque vai ser
muita coisa
para absorver
muito
muito conteúdo
muita informação
tá
e aí
eu espero
visual
acho que
eu vou estar lá
também com o Duan
espero vocês
sim
é
e
e
e cara
é
pode
calma aí Matheus
já pode
é
colocar
o anexo
só um minutinho
antes para anunciar
eu
eu tava brincando
com vocês
a gente
a gente aqui cara
pensa
primeiro em transformar
como todo mundo conhece
é
a gente
entrega valor
e esse workshop
não seria
esse bootcamp
não seria diferente disso
tá
ele vai acontecer
de 7 às 10 às 11
da noite
vai ser uma semana inteira
então de segunda
a sábado
um sábado vai ser de 9
a 1 da tarde
cara
e
esse workshop
esse bootcamp
na verdade
quando a gente fez modulação
de produto
as pessoas queriam me matar aqui
mas esse produto
ele vai sair no valor
de 1497
tá
é
só que
é
a gente
acredita
muito
no que a gente
está fazendo
então
eu vou dar um link
para quem quiser
participar da apresentação
da pré -venda
agora
que vão ser
997
e esse valor
só é vendido
hoje
em 24 horas
a Valkyria tá aqui
é
eu não sei se a Valkyria
tá aqui
mas se ela tiver
como a gente falou
Val
são
de fato
24 horas
a partir do dia de hoje
é sábado e domingo
tá
depois disso
a gente não vai
vender por esse preço
eu realmente acredito
que esse preço
é um preço cara
que
é banana
para o que você vai aprender
você aprendeu em três horas
aqui
cinco seis horas
aqui
né
já sabe que
não foi isso
e vocês vão ter acesso
a participar
desse cara aqui
e ter acesso às entrevistas
e tudo isso
enfim
você já consegue entender
o valor que tudo isso faz
tá
então
nós já temos data
também
para esse cara
Matheus
qual é a data
25
de agosto
agora
até o dia 30
isso
então
aqui é a pré -venda
quando vocês verem
qualquer coisa fora disso
vai custar
1 .497 reais
esse bootcamp
tá então
quem quiser tomar a decisão
agora e fazer
faça
quem não quiser
vai pagar 1 .497
não adianta falar com o time
não existe condição
de desconto
em cima disso
o módulo
de entrega
desse modelo
era de 3 .987
o time de modelagem
falou cara
vende por esse preço
porque é um conteúdo
que tá hypado
então obviamente
que a galera vai comprar
só que a gente pensa
na cauda longa
tá
então realmente
eu acho que
para quem quer
consumir esse conteúdo
para quem realmente
dar esse boom
na carreira
e aprender isso aqui
realmente de verdade
cara
eu realmente acredito
que para a engenharia
para a IA
para tudo isso
acho que você vai aprender
muita coisa aqui
Matheus
mais alguma coisa
vamos deixar aqui
as datas
então ó
é do dia 25
do 8
a dia 30
do 8
é
de
20
, de 19
não, de 8 horas
né
às 11
e sábado
vai ser
de 9
às 2
tá
aí por exemplo
se tiver
esse caso
não adianta pedir desconto
para o financeiro
tá
se forem pedir
para alguém
esse é o valor
do valor
tá
esse desconto
foi dado aqui
para quem tiver
dentro do workshop
e quiser adquirir
beleza
esquecemos mais
alguma coisa
Matheus
não
só isso tudo
então beleza
gente
então vou abrir agora
para vocês tirarem dúvida
né
acho que
dá para tirar
uns 20 minutinhos aí
para quem tiver dúvida
quiser
falar alguma coisa
fique à vontade
deixa eu
colocar todo mundo
para poder abrir
quando quiser
só
só devagar
tá gente
quando eu liberar
o áudio
para
ficar
não atropelar o outro
deixa eu já deixar
de uma vez mais fácil
bom
vou te mutar aqui
porque vai entrar
aquele negócio
mas você desmuta o seu
tá
então pode desmutar aí
Márcio
pode tirar um boot
e depois
eu vou chamando
e vocês vão
só tirando um boot
tá
tá bom
boa tarde pessoal
primeiro obrigado
pelo curso
a minha dúvida
sobre
esse bootcamp
qual o nível
e qual o prequisito
vocês acham ideal
para que a pessoa
consiga compartilhar
ali
aprender e acompanhar
o que você achou
do treinamento
de hoje
do nível
eu nunca tinha tocado
né
eu tinha visto
algumas teorias
acho que deu para entender bem
eu já tinha visto
algumas partes
do código também
mas algumas coisas
eu senti assim
um pouco solto
por exemplo
eu não tenho
vivência com Python
por exemplo
só com SQL
e aí eu não sei
o quanto a vivência
com Python
vai me atrapalhar
durante o curso
entendeu
boa
boa pergunta
lembra que a gente
vai ter um módulo anterior
de gravado
justamente por causa disso
porque tem gente
que vai estar
em diferentes níveis
como a gente tem
na formação ali
diferentes pessoas
com diferentes níveis
então o que vai ter
nesse treinamento
é um subtreinamento
gravado
então duas semanas
antes da gente
entrar realmente
ali no online
e fazer esse
esse evento
de cinco dias
de seis dias
você vai ter acesso
a um treinamento
gravado com os básicos
então você vai ter
básico de SQL
básico de Python
e o básico de GNA
inteiro
para você ter o ground
perfeito
para poder entrar ali
no tempo
e entender tudo
que eu estou falando
perfeito
obrigado
Rodrigo
Fala Luan
tudo bom?
é não
minha dúvida
é mais com relação
às ferramentas
aí né
eu estou na formação
ainda estou patinando
um pouco
aprendendo aí
as diversas
milhões de ferramentas
que tem aí né
e eu estou
mais ou menos
do cara aí
com vivência
na SQL mesmo
SQL serve
mas estou aí
conhecendo
Docker
Kubernetes
estou ali
ainda me ambientando
e aí eu vi
que aí
a formação
essa é o
o simul de campanha
da GNA
gostei muito
do conteúdo
interessante
e aí
eu vou ver aqui
se eu consigo
também
participar
desse
desse boot aí
mas a dúvida
seria isso
você vai também
passar mais
algum
fundamento
aí das
ferramentas
que vão ser
utilizadas né
exatamente
porque assim
você vai pegar
a lista aí
das coisas
que o Matheus
compartilhou
você vai ver
que tem uma
cacetada
então cada dia
ali do dia
a gente vai explorar
várias ferramentas
então a ideia é que
cada dia que a gente
for entrar
a gente tem as ferramentas
a gente vai falar
sobre essas ferramentas
a gente vai explicar
sobre elas
e daí a gente
entra no conteúdo
explicando
todo o processo
e tudo o que é
background
que você precisa
entender
antes de navegar
vai ser explicado
na gravação
você vai ter ali
uma hora
uma hora e meia
de conteúdo
mais rápido
teórico
para você ter acesso
ao que você precisa
de fato saber
para entrar lá
e não ficar perdido
e depois conseguir
aplicar
entendeu?
Pronto
Opa
me escutam bem?
Me escuto bem
E aí
bom final de semana
a todos
tem uma pergunta
ela é um pouquinho
mais técnica
eu já participei
de um projeto
muito legal
de
era um projeto
focado em transcrição
de vídeo
fazendo um chatbot
de AI
ali para fazer
uma análise
de sentimento
e
só que a gente
não utilizou
Langflow
ali para fazer
a parte
da orquestração
do
do agente
nós utilizamos
Flowize
não sei se tu já
chegou a trabalhar
com Flowize
eu achei ele fraco
eu achei ele fraco
mas
eu acho ele
meio limitado
mas dá para fazer
muita coisa
eu ia te perguntar
assim
eu já vi
Langflow por cima
nunca participei
de um projeto
de produção
né
e eu queria saber
se é na mesma linha
do Flowize ali
se eles fazem
tipo as mesmas coisas
que a gente falou
tu acha o Langflow
um pouco mais robusto
e eu vi que tem muita
coisa
é
ele é tipo assim
Flowize é tipo
a criança de
sei lá
três dias
e o Langflow
ou o Langchain
é o adulto
dos 17 anos
ou já
adolescente
para adulto
é basicamente isso
diferença muito grande
o que é legal
se você pensar
para comparar assim
esquece Flowize
ele é muito fraco
para o que a gente
precisa fazer
para essas coisas
de extensibilidade
de quantidade de coisas
e assim por diante
aí o que é legal
você olhar talvez
é o DeFi
o DeFi já é bem
legal
tá
o DeFi já é uma coisa
que eu acho bem
bem bem legal
porque ele é
production ready
então você consegue
fazer muita coisa
no topo dele
assim ó
entendeu?
é eu não conheço
DeFi
minha primeira contato
é aí
é então
aí já ganhou dinheiro aqui
porque você já economizou
dor de cabeça
esse cara aqui
esquece Flowize
tá
não vai entregar
o que a gente precisa não
não é legal
era mais para entender
se era nessa linha mesmo
sim
vou dar uma olhada
muito obrigado
viu
Valora
Andrago
pode ir
para a mochile
opa
é
a minha dúvida é o seguinte
eu trabalho num banco aqui
que tem muita burocracia
e ao mesmo tempo
eles querem usar
inteligência artificial
agentes e tal
tem algum momento no curso
Luan
que você vai dar algum insight
de como implementar
essas ferramentas
em empresas que são extremamente
fechadas em termos de
segurança né
e onde você
não pode usar essas ferramentas
de qualquer forma
tem algum momento
que você vai
não
não vou
não
não
não
não
não tem nada no conteúdo
pensando nesse tipo de
de escopo
mais fechado assim não
mas
assim
dá para a gente falar
e é simples
porque acho que a única coisa
complexa aqui é
se você pega a stack de
dados
ela é simples né
é Flow
você consegue colocar on -premises
tudo você consegue
a grande dificuldade
é como você operacionaliza
a inferência de modelo
dentro
de on -premises
acho que essa é a grande
dificuldade
tipo
a máquina
tem que ter GPU
ou se você não tiver
operacionalizando como GPU
você pode usar um NX
que é por exemplo
um layer
então dá para a gente
conversar sobre isso
mas eu não vou ter
nenhuma demonstração
que eu trago o cara
para on -premises
fácil entendeu
mas a gente pode falar sobre
isso
e tem formas de fazer isso
também
tem muito banco grande
aqui no Canadá
que quer implementar
mas está
preso na sua
burocracia de
sim
sim
então está sendo
um muro assim
mas
é
e tá
No cara eu acho que
não eu acho não
eu já te digo
de certeza que eu tô te
falando
pode pode só loucura
esse muro ele vai cair
ele é insustentável
assim como o muro da Cláudia
era na época
louco
eu não vou para Cláudia
você tá doido que eu vou
e a galera
obviamente que vai demorar um pouco
porque hoje nos Estados Unidos
no Canadá
o que você falou né
é muita coisa
tipo assim
então falando de AI ethics
tem muita coisa ainda
nesse espectro
que é muito arriscado
então a galera
principalmente banco velho
tá
fazendo deployment da solução
das ações dela
dentro de housing mesmo
então isso acontece
tá mas eu acho o seguinte
sinceramente falando
se você aprender as arquiteturas
como um todo
é pegar os pedaços
que não podem ser expostos
e você trazer esses pedaços
para dentro da empresa
você consegue montar uma solução
com esse conhecimento aí
com certeza
eu vou falar para você
pessoal aí
quando eu levo essas ideias
que eu aprendo com você
aqui para diretoria nossa lá
é o pessoal fica maluco
porque
é esse nível de técnico
que você traz aqui
que vai dar a tranquilidade
para os diretores
começarem a adotar
essas tecnologias assim
então é muito valoroso
obrigado pelo rigor técnico aí
porque está ajudando
para caramba aqui
obrigado
eu só espero que você
cada vez mais cresça
sua felicidade
é minha felicidade
com certeza
acho muito foda isso
valeu Loh
brigadão
valeu
tamo junto
e o projeto lá
será que um dia eu vou
para lá
eu tô
eu tô
eu tô fascinado
o do Texas
o do Texas?
cara
eu só fico imaginando
a gente lá
com aquelas caminhonete
orelhuda assim
nossa ia ser muito foda
ia ser muito foda
é fazendo brisket lá
e tocando os projetos de IA
lá
queria muito isso viu
mas cara
que pena que eu não tenho
que pena que eu não tenho
QI suficiente
para estar ali
com aqueles caras velho
porque puta
eu discordo totalmente
eu acho que a sua cabeça
vai ser disputada
em algum momento ali
acho que
eu não sei
com certeza
com certeza
com certeza
ali é uma parada
sobrenatural
ali é
que você me mostrou
eu assisti aquele vídeo
já duas vezes
se você tem ideia
você já viu aquele dado
que só o PIB do Texas
é maior do que do Brasil?
cara
que eu não vi
é verdade?
tá
cara
só o PIB do Texas
é maior do que nosso país
inteiro
cara
ali
tem muita água
para a gente beber galera
vocês não estão entendendo
e assim
o rigor técnico
que vocês tem aqui
é o que tá precisando lá
pra toda essa coisa de ir
a rodar
vai precisar de gente que nem
vocês
vai, mas a galera não acredita
quando a gente fala essas coisas,
sabia?
tem gente que tá aqui escutando
e não acredita
mas cara, é exatamente isso que
eu acredito
eu fico agoniado
muita gente vai me entender aqui
muito do seu público, óbvio, Alva
é pai de família
eu tenho duas filhas
então eu queria ter 36 horas
no dia
pra poder assimilar tudo
porque é muito conteúdo bom
eu fico agoniado porque eu preciso
aprender as coisas
pra colocar nos projetos e tudo
inclusive eu vou aplicar
pra um visto dos Estados Unidos
olha só como você muda a
vida das pessoas
você, o Matheus, o time aí
eu vou aplicar pra um visto
nos Estados Unidos e uma das
minhas propostas
vai ser usando essas técnicas que
você ensina aqui
então você não usa só no
trabalho
você usa pra projeto de vida
pra muitas coisas importantes
depois tu pode trocar uma ideia
sobre isso também
sim, podemos
com certeza, bom demais
você não tá fazendo um trabalho
só
pra implementar
pra ter que você tá entregando
conteúdo
pra projetos de visto também
então a gente pode conversar sobre
isso depois
bom demais
obrigado demais, fico feliz
e só me ajuda, só vai
me falando depois
bom demais
com certeza, com certeza
e é
o próximo
pode ir
pode tirar um monte, viu?
ou
tá me ouvindo?
boa, boa
tá me ouvindo?
boa tarde
acho que era minha vez, desculpe
tá me ouvindo, alô?
tá
é a vez de quem?
é porque eu acho que eu
tava no
ele falou meu nome, eu acho
que eu tava no boot aqui
é a vez de quem?
a minha?
eu acho que a minha
eu vou até ficar ali a
vez
que o pessoal tá me esperando
aqui pra sair
é, o seguinte
o Lohan, você tá usando o
termo
bootcamp, né?
eu fiz até um bootcamp da
Cognizant
pra engenharia de dados
e, enfim, quando você fala em
bootcamp
é porque você vai participar de
algum projeto
senão seria um curso
eu queria perguntar isso, se os
melhores alunos vão participar
eu até não tenho muito tempo,
mas gostaria muito
de participar de um projeto, né?
realmente, que vai ser colocado no
ar
que vai ser utilizado
essa é a minha pergunta
é, a ideia é a gente
construir uma solução em cima disso
mas não somente isso, que a
gente usa essa base de talento
pra poder contratar
e pra poder utilizar em projetos
boa
tô dentro
valeu, tô precisando de sair
abraço, pessoal, parabéns aí
excelente conteúdo, valeu
valeu, obrigado
Marcelo
Marcelo, aí pede Marcelo
porque tem dois aqui, não sei
se são a mesma pessoa
beleza, sou eu
boa tarde aí, Luan
a todo mundo aí
Luan, me tira uma dúvida, cara
sobre esse
conteúdo que, pô, foi muito
muito top
como é que você recomenda
que a gente faça esse estudo
aí de forma eficiente
do material
e a segunda
pergunta é como a gente pode
criar produtos aqui
personalizados pra vender futuramente como
consultoria, por exemplo
valeu, valeu Luan, valeu Matheus
Valora, primeiro ponto
é como digerir tudo isso, né
eu acho que seguir o storytelling
realmente
de como eu construí, vocês vão
ter acesso
a todos os escala e draw,
já vai te ajudar
porque eu sempre construo os treinamentos
em storytelling
então você vai ter sempre a
sequência
e o segundo ponto, cara
esse aí é um ponto muito
importante
tem muito espaço pra criar solução
em cima disso, muito
principalmente se você usar, se você
pra mim, não sei se
vocês também veem dessa forma
mas pra mim, cara, se resume
do
seguinte, tudo é
utilizar a ferramenta certa no contexto
certo, o problema é saber como
utilizar hoje, porque tem muita coisa
então assim, dá pra fazer muita
coisa foda hoje
muita coisa foda
e às vezes eu vejo pessoas
pegando
caminhos muito mais complexos
que não precisam ser pegos
naquele momento, eu vejo muita gente
rodando nisso, cara, o que você
tá fazendo
isso? Ah, tá, mas
por quê?
tá, mas por quê? Então assim,
é importante
ter um arsenal de coisas justamente
pra você conseguir saber o que
você pode
usar, então acho que esse é
um grande
diferencial, sabe?
João?
Beleza, valeu, show de bola
João?
Pode tirar do mute aí, viu?
Me ouve?
Sim?
Beleza, aqui a minha dúvida é
mais na questão de
função, de atuação
desse papel, né?
Fiquei com várias dúvidas aqui no
momento que você falou
acho que eu entendi, né? Depois
eu acho que
não tava entendendo nada
eu acho que você falou que
não é ser um especialista em
IA
de fato, criando um modelo e
tal
essas coisas, eu acho que não
entra nesse nível
mas também eu fiquei com dúvida
se você continua com a atuação
de engenheiro de dados
mas só que com essas ferramentas
agora de
de GNI pra atiação
acelerar
e a outra dúvida foi sobre
essas entregas
essas entregas são específicas de IA
são entregas específicas de engenheiro de
dados
vai ter alguém de IA depois
eu fiquei bem confuso
nessas divisões de papéis
é um novo papel? É tudo?
Sei lá
Calma aí, vamos devagar
não dá pra você soltar 60
questões na minha cabeça
e sair não, calma, relaxa
pensa primeiro nas perguntas
calma, vamos lá, por partes
é uma área nova
assim como era a ciência de
dados
por exemplo
mas não relativamente nova
porque a ciência de dados
o cientista de dados, ele nasceu
de uma forma
eu diria, muito mais abrupta
do que o IA de engenheiro
ou esquece o nome IA de
engenheiro
é o engenheiro com skills de
IA
então segue o mesmo
fluxo, cara, você tinha estatístico
você tinha matemática
você tinha vários PHDs
só que os caras não tinham
um background de operacionalizar
um modelo na vida real mesmo
realmente com estatística
então ir pra esse mundo de
sistemas distribuídos
aprender como fazer todo esse processo
trouxe o cientista de dados ao
patamar
de conseguir pegar tudo de estatística
junto com todas as melhorias
da tecnologia e dos avanços
e hoje realmente criar modelos, tunar
modelo
fazer treinamento supervisionado
não supervisionado e assim por diante
só que essa área, ela expandiu
também
porque essa área evoluiu
muito mais do que as outras
muita coisa se automatizou
a gente foi, saiu de
inteligência artificial
pra NLP, depois pra redes neurais
complexas, depois a gente entrou
na era de GNI
então muita coisa que o cientista
de dados fazia
ele não faz mais, ele não
vai ficar fazendo
tuning de negócio pra maioria dos
clientes
ele não vai fazer milhões de
coisas
que ele fazia anteriormente, por quê?
porque a GNI vai fazer isso
por ele
ele deixa de existir?
claro que não, ele vai ter
casos mais específicos
ele vai deixar de ganhar bem?
claro que não, ele vai ter
casos mais específicos
quanto mais específico você é?
mais você ganha
mas o mainstream, a metade
70, 80, 90 % do bolo
já tá feito
então, por exemplo, esse é um
caso
se você pegar documentos, extração de
documentos
teria que treinar
se você fosse fazer isso nível
enterprise
você tem que treinar ali, trabalhar
com CR
treinar um modelo de NLP pra
poder
um transformer, pra poder entender
hoje você faz com prompt
e daí eu poderia citar várias
outras coisas
só que aí, começou a ficar
muito mais complexa
essa operação com o mundo como
um todo
como um todo, nasceu o MLM
como um todo, nasceu o MLOps
em GNI
então esse cara faz
não, cientista de dados
você não sabe operacionalizar o modelo
você cria, tuna, beleza
e eu vou operacionalizar o ciclo
de vida dele
versionamento, verificar se tá servindo
se como essa plataforma acessa
quais são as ferramentas e assim
por diante
só que ainda faltava uma grande
coisa
porque você tem um engenheiro fazendo
pipe
você tem um cientista trabalhando
com bleeding edge fazendo modelos
você tem um MLOps em GNI
operacionando
só que agora vem o GNI
e aí com a vinda de
GNI
você tem um engenheiro fazendo pipe
a engenharia de dados fica muito
clara em relação ao que ela
deve
fazer, por quê? Porque ela precisa
utilizar as características de IA
e como que você usa as
características de IA? Utilizando
DNA e trazendo isso pra dentro
do
forno, então você vai ter muita
coisa
que você vai fazer, as outras
áreas vão deixar
de existir? Claro que não, cada
um vai ter
sua peculiaridade, então é uma área
nova
eu não ainda vejo como uma
área nova, eu vejo como
uma extensão do tool set
de coisas que você faz
e sim vai ter coisas novas
que vão
cair na sua cabeça pra você
fazer
sem dúvida alguma vai ter
tá, então isso não descarta os
outros
mas te traz uma set de
características
novas pra você fazer
coisas que você fazia antigamente que
você não
precisava usar DNAI e coisas
novas que você vai fazer que
você vai conseguir
também utilizar essas caras, então eu
vejo os dois
cenários
aí por exemplo, se você for
se você for hoje no LinkedIn
pesquisar lá
sobre Data Engineer
AI, AI Data Engineer
Data AI Engineer, você já vai
ver vários
cargos que já falam esse nome
explicitamente, mas toda vez que você
digitar
isso por pesquisa semântica
você vai ver tipo assim, o
cara pede
Spark, não sei o que, entendimento
de Foundation
Model, entendimento de LLM
Ops, não sei, Langchain
você já vai ver que essas
coisas já estão acontecendo
de fato
ótimo
Matheus
Matheus
Matheus
Boa tarde
Boa tarde
Sim
Seguinte, Luan, Matheus
também
Recentemente eu criei uma POC
dentro da AWS
no Bedrock
pra que eu pudesse habilitar ali
na empresa uma análise
de
de algumas tabelas, de alguns relatórios
que a gente tem dentro da
AWS
pros nossos analistas, mas
algo bem simples, tá? Então basicamente
ali eu construí um
um agente pra fazer text
to carry e depois
eu fiz um segundo agente que
pegava o resultado
segundo agente não, pegava o resultado
desse primeiro agente
jogava pro Lambda
o Lambda chamava, atina, rodava
processava e devolvia pro
segundo agente que fazia uma análise
e retornava o insight sobre isso
só que um dos
problemas que
eu tô quebrando bastante a cabeça
é em relação à
questão de contexto
porque nesse caso eu tive que
hardcodar
a estrutura da minha tabela
imaginei, imaginei
a estrutura da minha tabela
pra poder
ter um contexto pro outro agente
pegar
isso, só que daí o que
que acontece?
quando a gente tá falando de
data lake
tá falando de ambiente big data
de um catálogo unificado
você tem uma porrada de tabela
então pra você poder
deixar isso menos
inflexível, né?
você tem que hardcodar sempre
a estrutura da tabela?
você vai ter que treinar o
seu modelo
você vai ter que treinar o
seu modelo
no seu dataset inteiro e embedar
isso
em questão vetorizada
e aí fazer com que os
agentes
pesquisam vetorizado lá dentro
não, perfeito
aí sabe qual que é o
segundo problema
que eu fiquei pensando depois?
em relação à questão de governança
porque tem gente que não tem
acesso a algumas
informações ou algumas tabelas
se eu deixar isso disponível
tá, beleza
aí você vai ter que ter
um outro agente
que vai fazer o guard rail
vai ser a forma mais fácil
de você resolver agora
entendi, faz sentido
faz sentido
você vai ter que ter o
cara que antes de entregar
ele vai ter que validar se
aquele cara
pertence a isso, aí você pode
ter uma tabela
de controle por LLM
que ele vai conseguir fazer isso
simples, por exemplo
entendeu? ou busca semântica
o que ele pode mostrar ou
não pode mostrar
dá pra você fazer
porque assim
claro, essa é a minha resposta
sem pensar, obviamente, né
mas se pensar, dá pra achar
uma coisa muito mais efetiva
e muito mais inteligente, porque aí
a gente vai gastar token
vai ter um outro contexto
mas tem como resolver isso aí
mas o primeiro passo é você
vai ter que fazer um RAG
eu entendi o que você tá
falando
era um problema que o outro
cara tinha
um amigo meu, ele tem
só pra você ter ideia como
isso aí, eles recebem
nota fiscal do país inteiro
e aí ele tinha dois agentes,
um agente
fazia essa pesquisa textual
em cima dos tipos, e era
um XML extremamente
complexo, e o outro
guiava essa solução ali
de query para o usuário final
então ele perdia exatamente esse contexto
entendeu? tipo, cara, qual é o
tipo
em qual tipo de nota eu
tô
navegando, qual é a empresa
ele não tinha características, então passar
isso num
contexto toda hora, era extremamente
extremamente caro pra ele
então o que que ele fez?
base de RAG
show de bola
aí só, poder só
fazer mais uma última pergunta, na
verdade
mais uma curiosidade, se chegaram
já
a dar uma olhadinha no
S3 Vectors?
Vector? S3 Vector?
não, é bem recente assim
eu também não cheguei a olhar
da AWS?
isso, a AWS S3 Vector
legal, deixa eu colocar aqui
ele saiu faz o que? uma
semana, algo assim
e eu achei bem interessante
porque ele vem, diz a AWS
que ele vem pra reduzir em
90 % o custo
com
caraca, pô legal hein
cara, a
a Amazon tá começando a me
impressionar
positivamente
sobre algumas coisas que ela tá
trazendo
legal, vou dar uma olhadinha depois
interessante, 90 %
muito obrigado, Luan
nada
nada
fechamos
gente, agradecer demais
todo mundo que ficou aí, foi
um prazer
é
se vocês quiserem
né, contem comigo, distribui
isso lá no LinkedIn, agita
pra gente compartilhar, é legal
e se vocês
tiverem qualquer dúvida, vocês sabem
Jorge, você me deve um
churrasco na sua casa, só me
chamar
e
e valeu aí todo mundo, brigadão
gente
valeu Mateuzinho, valeu todo mundo
abraço, bom final de sábado
aí, curtam bastante
tchau, tchau
valeu aí pessoal, prazer, Ivo
